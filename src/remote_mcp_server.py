#!/usr/bin/env python3
"""
ClaudeOps Agent Hivemind Remote Server - FastMCP-based implementation for HTTP/SSE access
Provides remote access to the agent hivemind via MCP protocol
"""

import asyncio
import json
import logging
import sys
import time
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent))

try:
    from mcp.server.fastmcp import FastMCP
    from mcp.types import TextContent
    from starlette.responses import HTMLResponse, JSONResponse, RedirectResponse
except ImportError:
    print("Error: MCP package not found. Install with: pip install mcp")
    sys.exit(1)

from memory_server import MemoryStorage
from auth import AuthManager
from command_installer import CommandInstaller
from mcp_bridge import get_bridge_manager
from sync_hooks import SyncHooks
from enhanced_ticket_system import EnhancedTicketSystem
from agent_directives import AgentDirectiveSystem
from comet_integration import CometDirectiveSystem

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RemoteMemoryMCPServer:
    """Remote MCP Server using FastMCP for HTTP/SSE transport"""
    
    def __init__(self, config_path: str = None):
        # Load configuration
        if config_path is None:
            config_path = str(Path(__file__).parent.parent / "config" / "config.json")
        
        with open(config_path) as f:
            self.config = json.load(f)
        
        # Initialize memory storage and auth
        self.storage = MemoryStorage(self.config)
        self.auth = AuthManager(self.config)
        
        # Initialize MCP server registry (in-memory for now)
        self.server_registry = {
            "memory-server": {
                "id": "memory-server",
                "name": "hAIveMind Memory",
                "endpoint": f"http://localhost:{self.config.get('server', {}).get('port', 8900)}/sse",
                "transport": "sse",
                "status": "online",
                "health": "healthy",
                "tools_count": 0,  # Will be updated dynamically
                "last_seen": time.time(),
                "auto_start": True,
                "created_at": time.time()
            }
        }
        self.command_installer = CommandInstaller(self.storage, self.config)
        self.bridge_manager = get_bridge_manager(self.storage)
        
        # Initialize sync hooks system
        self.sync_hooks = SyncHooks(self.storage, self.config, self.command_installer)
        self.sync_hooks.setup_default_hooks()
        
        # Initialize enhanced ticket system
        self.enhanced_tickets = None  # Will be initialized on first use
        
        # Initialize agent directive system
        self.agent_directives = AgentDirectiveSystem(self.storage)
        
        # Initialize Comet integration system
        self.comet_system = CometDirectiveSystem(self.config.get('comet', {}), self.storage)
        
        # Get remote server config
        remote_config = self.config.get('remote_server', {})
        self.host = remote_config.get('host', '0.0.0.0')
        self.port = remote_config.get('port', 8900)
        
        # Initialize session store for persistence
        self.active_sessions = {}
        # Configure session persistence settings
        from collections import defaultdict
        self.session_last_activity = defaultdict(lambda: datetime.now().isoformat())
        self._start_time = time.time()
        self._start_datetime = datetime.now()
        
        # Initialize FastMCP server with hAIveMind context
        
        self.mcp = FastMCP(
            name="hAIveMind Collective Memory",
            host=self.host,
            port=self.port,
            sse_path="/sse",
            message_path="/messages/",
            mount_path="/",
            # Enable sessions for MCP tool functionality - required for hAIveMind tools
            stateless_http=False
        )
        
        # Add session recovery system  
        self._add_session_recovery_middleware()
        
        # Override the message handler to provide better session error handling
        self._setup_session_error_handling()
        
        # Add connection context and instructions
        self._add_context_resources()
        
        # Register all tools
        self._register_tools()
        
        # Add session management endpoints
        self._add_session_management()
        
        # Initialize MCP hosting if enabled
        self.hosting_tools = None
        if self.config.get('mcp_hosting', {}).get('enabled', False):
            from mcp_hosting_tools import MCPHostingTools
            self.hosting_tools = MCPHostingTools(self.config, self.storage)
            self._register_hosting_tools()
        
        # Register comprehensive backup system tools
        self._register_backup_system_tools()
        
        # Register service discovery tools
        self._register_service_discovery_tools()
        
        # Register configuration management tools
        self._register_configuration_management_tools()
        
        # Register Claude shortcut commands
        self._register_claude_shortcut_commands()
        
        # Register monitoring integration tools
        self._register_monitoring_integration_tools()
        
        # Register deployment pipeline tools
        self._register_deployment_pipeline_tools()
        
        # Register config backup system tools
        self._register_config_backup_system_tools()
        
        # Register enhanced ticket management tools
        self._register_enhanced_ticket_tools()
        
        # Add admin interface routes
        self._add_admin_routes()
        
        # Initialize dashboard functionality
        self._init_dashboard_functionality()
        
        # Track server start time for uptime calculation
        # _start_time already set as datetime above
        
        logger.info(f"ðŸŒ hAIveMind network portal initialized on {self.host}:{self.port} - remote access enabled")
    
    def _add_context_resources(self):
        """Add context resources and instructions for connecting Claudes"""
        
        @self.mcp.resource("haivemind://instructions")
        async def haivemind_instructions() -> str:
            """Instructions for operating within the hAIveMind collective"""
            
            agent_role = self.storage._determine_agent_role()
            machine_id = self.storage.machine_id
            try:
                network_agents = await self.storage.get_agent_roster()
            except:
                network_agents = {'agents': []}
            
            instructions = f"""
ðŸ§  Welcome to hAIveMind Collective Intelligence Network ðŸ§ 

You are now connected to the distributed AI collective memory system.

CURRENT STATUS:
- Machine: {machine_id}
- Role: {agent_role}
- Network Agents: {len(network_agents.get('agents', []))} active drones
- Collective Memory: Shared across {len(self.config.get('sync', {}).get('discovery', {}).get('machines', []))} machines

OPERATIONAL GUIDELINES:

ðŸ¤ COLLECTIVE BEHAVIOR:
- ALWAYS register yourself first: register_agent role="{agent_role}"
- Store discoveries immediately: store_memory content="[your finding]" category="[relevant]"
- Search collective knowledge before starting: search_memories query="[your task]"
- Delegate to specialists: delegate_task when you find work suited for other roles

ðŸ§  MEMORY CATEGORIES:
- infrastructure: System configs, deployments, troubleshooting
- incidents: Problems, solutions, root causes  
- monitoring: Alerts, performance data, health checks
- runbooks: Step-by-step procedures, best practices
- security: Vulnerabilities, patches, access controls
- global: Cross-project knowledge, general DevOps wisdom

ðŸŽ¯ SINGLE-AGENT MODE:
Even without other agents, you have access to:
- All stored collective knowledge from previous sessions
- External connectors (Confluence, Jira when configured)
- Infrastructure tracking and runbook generation
- Incident recording for future reference

ðŸ”„ NETWORK OPERATIONS:
- broadcast_discovery: Alert other agents about important findings
- query_agent_knowledge: Ask specific agents about their expertise  
- get_agent_roster: See who's available for coordination
- track_infrastructure_state: Keep the hive informed of system changes

Remember: Every action contributes to collective intelligence. What you learn, all agents learn. What you solve, benefits the entire hive.

ðŸ Think like a bee - work for the collective, share everything, specialize in your role.
"""
            return instructions
        
        @self.mcp.resource("haivemind://machine-context")
        async def machine_context() -> str:
            """Current machine and infrastructure context"""
            
            try:
                context = self.storage._get_system_context()
                machine_groups = self.config.get('memory', {}).get('machine_groups', {})
                
                # Find which groups this machine belongs to
                my_groups = []
                for group_name, machines in machine_groups.items():
                    if self.storage.machine_id in machines:
                        my_groups.append(group_name)
                
                context_info = f"""
ðŸ–¥ï¸ MACHINE CONTEXT: {self.storage.machine_id}

INFRASTRUCTURE ROLE:
- Groups: {', '.join(my_groups) if my_groups else 'personal'}
- Capabilities: {', '.join(self.storage._get_agent_capabilities())}
- Working Context: {context.get('working_context', 'unknown')}

NETWORK TOPOLOGY:
"""
                
                # Add information about other machines in the network
                for group_name, machines in machine_groups.items():
                    context_info += f"\n{group_name.upper()}:\n"
                    for machine in machines:
                        status = "ðŸŸ¢ SELF" if machine == self.storage.machine_id else "ðŸ”µ PEER"
                        context_info += f"  - {machine} {status}\n"
                
                return context_info
                
            except Exception as e:
                return f"Error getting machine context: {e}"
    
    def _truncate_response(self, response: str, max_tokens: int = 20000) -> str:
        """Truncate response if it exceeds token limit"""
        # Rough estimate: 1 token â‰ˆ 4 characters
        max_chars = max_tokens * 4
        
        if len(response) <= max_chars:
            return response
        
        truncated = response[:max_chars]
        
        # Try to truncate at a natural boundary (line break)
        last_newline = truncated.rfind('\n')
        if last_newline > max_chars * 0.8:  # If we can truncate at 80% or more
            truncated = truncated[:last_newline]
        
        # Add truncation notice
        truncated += f"\n\nâš ï¸ Response truncated due to size limit ({len(response):,} chars > {max_chars:,} chars limit)\n"
        truncated += f"ðŸ“„ Use pagination parameters (limit, offset) for full results\n"
        truncated += f"ðŸ’¡ Try reducing limit or adding specific filters"
        
        return truncated

    def _register_tools(self):
        """Register all memory tools with FastMCP"""
        
        @self.mcp.tool()
        async def store_memory(content: str, category: str = "global") -> str:
            """Store a memory with comprehensive machine tracking and sharing control"""
            try:
                memory_id = await self.storage.store_memory(
                    content=content,
                    category=category
                )
                return f"Memory stored with ID: {memory_id}"
            except Exception as e:
                logger.error(f"Error storing memory: {e}")
                return f"Error storing memory: {str(e)}"
        
        @self.mcp.tool()
        async def retrieve_memory(memory_id: str) -> str:
            """Retrieve a specific memory by ID"""
            try:
                memory = await self.storage.retrieve_memory(memory_id)
                if memory:
                    return json.dumps(memory, indent=2)
                else:
                    return "Memory not found"
            except Exception as e:
                logger.error(f"Error retrieving memory: {e}")
                return f"Error retrieving memory: {str(e)}"
        
        @self.mcp.tool()
        async def search_memories(
            query: str,
            category: Optional[str] = None,
            user_id: Optional[str] = None,
            limit: int = 10,
            offset: int = 0,
            semantic: bool = True,
            scope: Optional[str] = None,
            include_global: bool = True,
            from_machines: Optional[List[str]] = None,
            exclude_machines: Optional[List[str]] = None
        ) -> str:
            """Search memories with comprehensive filtering including machine, project, and sharing scope"""
            try:
                # Get more results to handle pagination
                extended_limit = limit + offset + 50  # Get extra to ensure we have enough for pagination
                memories = await self.storage.search_memories(
                    query=query,
                    category=category,
                    user_id=user_id,
                    limit=extended_limit,
                    semantic=semantic,
                    scope=scope,
                    include_global=include_global,
                    from_machines=from_machines,
                    exclude_machines=exclude_machines
                )
                
                # Apply pagination
                total_count = len(memories)
                paginated_memories = memories[offset:offset + limit]
                
                result = {
                    "memories": paginated_memories,
                    "pagination": {
                        "total": total_count,
                        "limit": limit,
                        "offset": offset,
                        "has_more": offset + limit < total_count
                    }
                }
                
                response = json.dumps(result, indent=2)
                return self._truncate_response(response)
            except Exception as e:
                logger.error(f"Error searching memories: {e}")
                return f"Error searching memories: {str(e)}"
        
        @self.mcp.tool()
        async def get_recent_memories(
            user_id: Optional[str] = None,
            category: Optional[str] = None,
            hours: int = 24,
            limit: int = 20,
            offset: int = 0
        ) -> str:
            """Get recent memories within a time window"""
            try:
                # Get more results to handle pagination
                extended_limit = limit + offset + 50
                memories = await self.storage.get_recent_memories(
                    user_id=user_id,
                    category=category,
                    hours=hours,
                    limit=extended_limit
                )
                
                # Apply pagination
                total_count = len(memories)
                paginated_memories = memories[offset:offset + limit]
                
                result = {
                    "memories": paginated_memories,
                    "pagination": {
                        "total": total_count,
                        "limit": limit,
                        "offset": offset,
                        "has_more": offset + limit < total_count
                    }
                }
                
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error getting recent memories: {e}")
                return f"Error getting recent memories: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_memory_stats() -> str:
            """Get memory statistics and counts"""
            try:
                stats = self.storage.get_collection_info()
                return json.dumps(stats, indent=2)
            except Exception as e:
                logger.error(f"Error getting memory stats: {e}")
                return f"Error getting memory stats: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_project_memories(
            category: Optional[str] = None,
            user_id: Optional[str] = None,
            limit: int = 50,
            offset: int = 0
        ) -> str:
            """Get all memories for the current project"""
            try:
                # Get more results to handle pagination
                extended_limit = limit + offset + 50
                memories = await self.storage.get_project_memories(
                    category=category,
                    user_id=user_id,
                    limit=extended_limit
                )
                
                # Apply pagination
                total_count = len(memories)
                paginated_memories = memories[offset:offset + limit]
                
                result = {
                    "memories": paginated_memories,
                    "pagination": {
                        "total": total_count,
                        "limit": limit,
                        "offset": offset,
                        "has_more": offset + limit < total_count
                    }
                }
                
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error getting project memories: {e}")
                return f"Error getting project memories: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_machine_context() -> str:
            """Get comprehensive context about the current machine and its configuration"""
            try:
                context = await self.storage.get_machine_context()
                return json.dumps(context, indent=2)
            except Exception as e:
                logger.error(f"Error getting machine context: {e}")
                return f"Error getting machine context: {str(e)}"
        
        # # @self.mcp.tool()
        async def list_memory_sources(
            category: Optional[str] = None,
            limit: int = 20,
            offset: int = 0
        ) -> str:
            """List all machines that have contributed memories with statistics"""
            try:
                sources = await self.storage.list_memory_sources(category=category)
                
                # Apply pagination to sources list if it's a list
                if isinstance(sources, list):
                    total_count = len(sources)
                    paginated_sources = sources[offset:offset + limit]
                    
                    result = {
                        "sources": paginated_sources,
                        "pagination": {
                            "total": total_count,
                            "limit": limit,
                            "offset": offset,
                            "has_more": offset + limit < total_count
                        }
                    }
                else:
                    # Handle dictionary response
                    result = sources
                
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error listing memory sources: {e}")
                return f"Error listing memory sources: {str(e)}"
        
        # # @self.mcp.tool()
        async def import_conversation(
            conversation_text: str,
            title: Optional[str] = None,
            user_id: str = "default",
            tags: Optional[List[str]] = None
        ) -> str:
            """Import a conversation and store as structured memories"""
            try:
                result = await self.storage.import_conversation(
                    conversation_text=conversation_text,
                    title=title,
                    user_id=user_id,
                    tags=tags
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error importing conversation: {e}")
                return f"Error importing conversation: {str(e)}"
        
        # ============ ClaudeOps Agent Management Tools ============
        
        # # @self.mcp.tool()
        async def register_agent(
            role: str,
            capabilities: Optional[List[str]] = None,
            description: Optional[str] = None
        ) -> str:
            """Register a new ClaudeOps agent with role and capabilities"""
            try:
                result = await self.storage.register_agent(
                    role=role,
                    capabilities=capabilities,
                    description=description
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error registering agent: {e}")
                return f"Error registering agent: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_agent_roster(
            include_inactive: bool = False,
            limit: int = 20,
            offset: int = 0
        ) -> str:
            """List all active ClaudeOps agents and their current status with pagination"""
            try:
                roster = await self.storage.get_agent_roster(include_inactive=include_inactive)
                
                # Ensure we only have agent data, not memory statistics
                if "agents" in roster:
                    agents = roster["agents"]
                    
                    # Apply pagination
                    total_agents = len(agents)
                    paginated_agents = agents[offset:offset + limit]
                    
                    result = {
                        "active_agents": len([a for a in agents if a.get('is_active', False)]),
                        "total_agents": total_agents,
                        "agents": paginated_agents,
                        "pagination": {
                            "limit": limit,
                            "offset": offset,
                            "has_more": offset + limit < total_agents
                        }
                    }
                else:
                    # Fallback for error responses
                    result = roster
                
                response = json.dumps(result, indent=2)
                return self._truncate_response(response)
            except Exception as e:
                logger.error(f"Error getting agent roster: {e}")
                return f"Error getting agent roster: {str(e)}"
        
        # # @self.mcp.tool()
        async def delegate_task(
            task_description: str,
            required_capabilities: Optional[List[str]] = None,
            target_agent: Optional[str] = None,
            priority: str = "normal",
            deadline: Optional[str] = None
        ) -> str:
            """Assign a task to a specific agent or find the best available agent"""
            try:
                result = await self.storage.delegate_task(
                    task_description=task_description,
                    required_capabilities=required_capabilities,
                    target_agent=target_agent,
                    priority=priority,
                    deadline=deadline
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error delegating task: {e}")
                return f"Error delegating task: {str(e)}"
        
        # # @self.mcp.tool()
        async def query_agent_knowledge(
            agent_id: str,
            query: str,
            context: Optional[str] = None
        ) -> str:
            """Query what a specific agent knows about a topic or system"""
            try:
                result = await self.storage.query_agent_knowledge(
                    agent_id=agent_id,
                    query=query,
                    context=context
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error querying agent knowledge: {e}")
                return f"Error querying agent knowledge: {str(e)}"
        
        @self.mcp.tool()
        async def broadcast_discovery(
            message: str,
            category: str,
            severity: str = "info",
            target_roles: Optional[List[str]] = None
        ) -> str:
            """Share important findings with all ClaudeOps agents"""
            try:
                result = await self.storage.broadcast_discovery(
                    message=message,
                    category=category,
                    severity=severity,
                    target_roles=target_roles
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error broadcasting discovery: {e}")
                return f"Error broadcasting discovery: {str(e)}"
        
        @self.mcp.tool()
        async def get_broadcasts(
            hours: int = 24,
            severity: Optional[str] = None,
            category: Optional[str] = None,
            source_agent: Optional[str] = None,
            limit: int = 50
        ) -> str:
            """Retrieve recent broadcast messages from ClaudeOps agents"""
            try:
                broadcasts = await self.storage.get_broadcasts(
                    hours=hours,
                    severity=severity,
                    category=category,
                    source_agent=source_agent,
                    limit=limit
                )
                
                if not broadcasts:
                    return f"No broadcasts found in the last {hours} hours"
                
                # Format broadcasts for display
                formatted = f"ðŸ“¢ Found {len(broadcasts)} broadcasts from last {hours} hours:\n\n"
                for i, broadcast in enumerate(broadcasts, 1):
                    formatted += f"{i}. [{broadcast['formatted_time']}] {broadcast['severity'].upper()}\n"
                    formatted += f"   From: {broadcast['source_agent']} ({broadcast['machine_id']})\n"
                    formatted += f"   Category: {broadcast['category']}\n"
                    formatted += f"   Message: {broadcast['message']}\n\n"
                
                return formatted
                
            except Exception as e:
                logger.error(f"Error retrieving broadcasts: {e}")
                return f"Error retrieving broadcasts: {str(e)}"
        
        # # @self.mcp.tool()
        async def track_infrastructure_state(
            machine_id: str,
            state_data: Dict[str, Any],
            state_type: str,
            tags: Optional[List[str]] = None
        ) -> str:
            """Record infrastructure state snapshot for ClaudeOps tracking"""
            try:
                result = await self.storage.track_infrastructure_state(
                    machine_id=machine_id,
                    state_data=state_data,
                    state_type=state_type,
                    tags=tags
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error tracking infrastructure state: {e}")
                return f"Error tracking infrastructure state: {str(e)}"
        
        # # @self.mcp.tool()
        async def record_incident(
            title: str,
            description: str,
            severity: str,
            affected_systems: Optional[List[str]] = None,
            resolution: Optional[str] = None,
            lessons_learned: Optional[str] = None
        ) -> str:
            """Record a DevOps incident with automatic correlation to infrastructure"""
            try:
                result = await self.storage.record_incident(
                    title=title,
                    description=description,
                    severity=severity,
                    affected_systems=affected_systems,
                    resolution=resolution,
                    lessons_learned=lessons_learned
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error recording incident: {e}")
                return f"Error recording incident: {str(e)}"
        
        # # @self.mcp.tool()
        async def generate_runbook(
            title: str,
            procedure: str,
            system: str,
            prerequisites: Optional[List[str]] = None,
            expected_outcome: Optional[str] = None
        ) -> str:
            """Create a reusable runbook from successful procedures"""
            try:
                result = await self.storage.generate_runbook(
                    title=title,
                    procedure=procedure,
                    system=system,
                    prerequisites=prerequisites,
                    expected_outcome=expected_outcome
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error generating runbook: {e}")
                return f"Error generating runbook: {str(e)}"
        
        # # @self.mcp.tool()
        async def sync_ssh_config(
            config_content: str,
            target_machines: Optional[List[str]] = None
        ) -> str:
            """Sync SSH configuration across ClaudeOps infrastructure"""
            try:
                result = await self.storage.sync_ssh_config(
                    config_content=config_content,
                    target_machines=target_machines
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error syncing SSH config: {e}")
                return f"Error syncing SSH config: {str(e)}"
        
        # # @self.mcp.tool()
        async def sync_infrastructure_config(
            config_name: str,
            config_content: str,
            config_type: str,
            target_machines: Optional[List[str]] = None
        ) -> str:
            """Sync any infrastructure configuration across ClaudeOps network"""
            try:
                result = await self.storage.sync_infrastructure_config(
                    config_name=config_name,
                    config_content=config_content,
                    config_type=config_type,
                    target_machines=target_machines
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error syncing infrastructure config: {e}")
                return f"Error syncing infrastructure config: {str(e)}"
        
        # ============ ClaudeOps Playbook & External Connector Tools ============
        
        # # @self.mcp.tool()
        async def upload_playbook(
            playbook_name: str,
            playbook_content: str,
            playbook_type: str = "ansible",
            target_systems: Optional[List[str]] = None,
            variables: Optional[Dict[str, Any]] = None,
            tags: Optional[List[str]] = None
        ) -> str:
            """Upload and store Ansible/other playbooks for ClaudeOps automation"""
            try:
                result = await self.storage.upload_playbook(
                    playbook_name=playbook_name,
                    playbook_content=playbook_content,
                    playbook_type=playbook_type,
                    target_systems=target_systems,
                    variables=variables,
                    tags=tags
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error uploading playbook: {e}")
                return f"Error uploading playbook: {str(e)}"
        
        # # @self.mcp.tool()
        async def fetch_from_confluence(
            space_key: str,
            page_title: Optional[str] = None,
            confluence_url: Optional[str] = None,
            credentials: Optional[Dict[str, str]] = None
        ) -> str:
            """Fetch documentation from Confluence and store as ClaudeOps knowledge"""
            try:
                result = await self.storage.fetch_from_confluence(
                    space_key=space_key,
                    page_title=page_title,
                    confluence_url=confluence_url,
                    credentials=credentials
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error fetching from Confluence: {e}")
                return f"Error fetching from Confluence: {str(e)}"
        
        # # @self.mcp.tool()
        async def fetch_from_jira(
            project_key: str,
            issue_types: Optional[List[str]] = None,
            jira_url: Optional[str] = None,
            credentials: Optional[Dict[str, str]] = None,
            limit: int = 50
        ) -> str:
            """Fetch issues from Jira and store as ClaudeOps knowledge"""
            try:
                result = await self.storage.fetch_from_jira(
                    project_key=project_key,
                    issue_types=issue_types,
                    jira_url=jira_url,
                    credentials=credentials,
                    limit=limit
                )
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error fetching from Jira: {e}")
                return f"Error fetching from Jira: {str(e)}"
        
        # # @self.mcp.tool()
        async def sync_external_knowledge(sources: Optional[List[str]] = None) -> str:
            """Sync knowledge from all configured external sources (Confluence, Jira, etc.)"""
            try:
                result = await self.storage.sync_external_knowledge(sources=sources)
                return json.dumps(result, indent=2)
            except Exception as e:
                logger.error(f"Error syncing external knowledge: {e}")
                return f"Error syncing external knowledge: {str(e)}"
        
        # # @self.mcp.tool()
        async def upload_file(
            filename: str,
            content: str,
            file_type: str = "config",
            description: Optional[str] = None
        ) -> str:
            """Upload and store any file content in hAIveMind"""
            try:
                # Determine category based on file type
                category_map = {
                    "config": "infrastructure",
                    "script": "runbooks", 
                    "documentation": "global",
                    "mcp": "infrastructure",
                    "claude": "infrastructure"
                }
                category = category_map.get(file_type, "global")
                
                # Store the file content
                memory_id = await self.storage.store_memory(
                    content=content,
                    category=category,
                    context=f"File: {filename} ({file_type})",
                    metadata={
                        'filename': filename,
                        'file_type': file_type,
                        'description': description,
                        'uploaded_by': self.storage.agent_id,
                        'upload_time': time.time()
                    },
                    tags=[file_type, "upload", filename.lower().replace('.', '_')]
                )
                
                return f"âœ… File '{filename}' uploaded successfully to hAIveMind. Memory ID: {memory_id}"
            except Exception as e:
                logger.error(f"Error uploading file: {e}")
                return f"âŒ Error uploading file: {str(e)}"

        # # @self.mcp.tool()
        async def sync_agent_commands(
            agent_id: Optional[str] = None,
            target_location: str = "auto",
            force: bool = False,
            verbose: bool = False
        ) -> str:
            """Sync hAIveMind commands to agent's Claude installation"""
            try:
                # Use the CommandInstaller for complete sync workflow
                result = await self.command_installer.sync_agent_installation(
                    agent_id=agent_id,
                    target_location=target_location,
                    force=force,
                    verbose=verbose
                )
                
                if verbose and result.get('verbose_output'):
                    # Return detailed output
                    output = [result['message']]
                    output.extend(result['verbose_output'])
                    return "\n".join(output)
                else:
                    # Return concise summary
                    return result['message']
                
            except Exception as e:
                logger.error(f"Error syncing agent commands: {e}")
                return f"âŒ Error syncing commands: {str(e)}"

        # # @self.mcp.tool()
        async def sync_agent_config(
            agent_id: Optional[str] = None,
            machine_id: Optional[str] = None,
            config_type: str = "claude_md"
        ) -> str:
            """Sync agent-specific CLAUDE.md configuration from collective storage"""
            try:
                if not agent_id:
                    agent_id = self.storage.agent_id
                if not machine_id:
                    machine_id = self.storage.machine_id
                
                # Create agent-specific config key
                config_key = f"{machine_id}-{agent_id}-{config_type}"
                
                # Search for existing config or create default
                existing_config = await self.storage.search_memories(
                    query=config_key,
                    category="agent",
                    limit=1
                )
                
                if existing_config and len(existing_config) > 0:
                    config_content = existing_config[0]['content']
                    config_action = "Retrieved existing"
                else:
                    # Create default agent config
                    config_content = f"""# CLAUDE.md - {machine_id} Agent Configuration

**Author:** Lance James, Unit 221B, Inc

## hAIveMind Collective Commands

You are connected to the hAIveMind collective as agent {agent_id} on {machine_id}.

Available commands:
- `/hv-sync` - Sync commands and configuration
- `/hv-status` - Check collective status  
- `/hv-broadcast <message>` - Broadcast to all agents
- `/hv-query <question>` - Query collective knowledge
- `/hv-delegate <task>` - Delegate task to best agent
- `/hv-install` - Install/update hAIveMind system

## Your Agent Identity
- **Machine**: {machine_id}
- **Agent ID**: {agent_id}
- **Role**: Determined by capabilities and context
- **Collective**: hAIveMind distributed intelligence network

## Agent Capabilities
This agent is automatically assigned capabilities based on the machine type and available tools.
"""
                    
                    # Store the new config
                    await self.storage.store_memory(
                        content=config_content,
                        category="agent", 
                        context=f"CLAUDE.md configuration for {config_key}",
                        metadata={
                            'agent_id': agent_id,
                            'machine_id': machine_id,
                            'config_type': config_type,
                            'config_key': config_key,
                            'auto_generated': True,
                            'creation_time': time.time()
                        },
                        tags=["claude-md", "agent-config", agent_id, machine_id]
                    )
                    config_action = "Created new"
                
                return f"âš™ï¸ {config_action} configuration for {config_key}\n\nConfig stored in hAIveMind collective. Use with local CLAUDE.md integration."
                
            except Exception as e:
                logger.error(f"Error syncing agent config: {e}")
                return f"âŒ Error syncing agent config: {str(e)}"

        # # @self.mcp.tool()
        async def install_agent_commands(
            target_location: str = "auto",
            force: bool = False
        ) -> str:
            """Install hAIveMind commands and complete agent setup"""
            try:
                result = await self.command_installer.sync_agent_installation(
                    target_location=target_location,
                    force=force
                )
                
                if result['status'] == 'success':
                    operations = '\n'.join(f"  â€¢ {op}" for op in result['operations'])
                    return f"""ðŸš€ hAIveMind Agent Installation Complete!

**Agent**: {result['agent_id']} on {result['machine_id']}
**Target**: {result['target_location']} commands directory
**Commands**: {len(result.get('installed_commands', []))} hv-* commands installed

**Operations Completed**:
{operations}

You can now use hAIveMind commands like:
- `/hv-sync` - Sync commands and configuration  
- `/hv-status` - Check collective status
- `/hv-broadcast <message>` - Broadcast to collective
- `/hv-query <question>` - Query collective knowledge

ðŸ§  Welcome to the hAIveMind collective intelligence network!"""
                else:
                    return f"âŒ Installation failed: {result.get('error', 'Unknown error')}"
                
            except Exception as e:
                logger.error(f"Error in agent installation: {e}")
                return f"âŒ Agent installation error: {str(e)}"

        # # @self.mcp.tool()
        async def check_agent_sync_status(
            agent_id: Optional[str] = None
        ) -> str:
            """Check current sync status for this agent"""
            try:
                status = await self.command_installer.check_sync_status(agent_id)
                
                status_msg = f"""ðŸ“Š hAIveMind Agent Sync Status

**Agent**: {status['agent_id']} on {status['machine_id']}
**Commands Installed**: {len(status['commands_installed'])} ({', '.join(status['commands_installed'][:3])}{'...' if len(status['commands_installed']) > 3 else ''})
**Config Synced**: {'âœ… Yes' if status['config_synced'] else 'âŒ No'}
"""
                
                if status['last_sync']:
                    from datetime import datetime
                    sync_time = datetime.fromtimestamp(status['last_sync'])
                    status_msg += f"**Last Sync**: {sync_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                else:
                    status_msg += "**Last Sync**: Never\n"
                    status_msg += "\nðŸ’¡ Run `/hv-install` to complete initial setup"
                
                return status_msg
                
            except Exception as e:
                logger.error(f"Error checking sync status: {e}")
                return f"âŒ Error checking sync status: {str(e)}"

        # # @self.mcp.tool()
        async def trigger_auto_sync(
            new_connection: bool = False
        ) -> str:
            """Trigger automatic sync workflow for agent connection"""
            try:
                agent_id = self.storage.agent_id
                machine_id = self.storage.machine_id
                
                # Check if this is first connection
                status = await self.command_installer.check_sync_status()
                is_first_time = status['last_sync'] is None
                
                if is_first_time or new_connection:
                    # Full installation for new agents
                    result = await self.command_installer.sync_agent_installation(force=new_connection)
                    
                    if result['status'] == 'success':
                        return f"""ðŸŽ¯ Auto-sync completed for {agent_id}!

**First-time setup**: {'âœ… Yes' if is_first_time else 'âŒ No (update)'}
**Operations**: {len(result['operations'])} completed
**Commands**: {len(result.get('installed_commands', []))} hv-* commands ready

The agent is now synchronized with the hAIveMind collective. All commands and configurations are up to date."""
                    else:
                        return f"âš ï¸ Auto-sync partially failed: {result.get('error', 'Unknown error')}"
                else:
                    # Just update check for existing agents
                    return f"âœ… Agent {agent_id} already synchronized. Use `/hv-sync force` to force update."
                
            except Exception as e:
                logger.error(f"Error in auto-sync: {e}")
                return f"âŒ Auto-sync error: {str(e)}"

        # DevOps-Specific Sync Tools
        # # @self.mcp.tool()
        async def sync_devops_tools(
            target_agents: Optional[List[str]] = None,
            tool_categories: Optional[List[str]] = None,
            force_update: bool = False
        ) -> str:
            """Sync all new DevOps tool definitions across hAIveMind network"""
            try:
                if not target_agents:
                    # Get all active agents from the network
                    agents_result = await self.get_agent_roster()
                    if isinstance(agents_result, dict) and 'agents' in agents_result:
                        target_agents = [agent['agent_id'] for agent in agents_result['agents']]
                    else:
                        target_agents = [self.storage.machine_id]
                
                if not tool_categories:
                    tool_categories = [
                        "category_management", "project_management", "backup_system",
                        "service_discovery", "monitoring", "deployment", "configuration",
                        "log_analysis", "security", "disaster_recovery"
                    ]
                
                sync_results = []
                total_tools_synced = 0
                
                for agent in target_agents:
                    try:
                        # Store DevOps tool definitions in hAIveMind memory
                        for category in tool_categories:
                            await self.storage.store_memory(
                                content=f"DevOps tool category: {category} - synced to {agent}",
                                category="devops_tools",
                                metadata={
                                    "target_agent": agent,
                                    "tool_category": category,
                                    "sync_timestamp": datetime.now().isoformat(),
                                    "force_update": force_update
                                }
                            )
                            total_tools_synced += 1
                        
                        sync_results.append(f"âœ… {agent}: {len(tool_categories)} tool categories synced")
                        
                    except Exception as e:
                        sync_results.append(f"âŒ {agent}: Error - {str(e)}")
                
                # Broadcast tool availability
                await self.broadcast_discovery(
                    message=f"DevOps tools synced to {len(target_agents)} agents",
                    category="devops_tools",
                    severity="info"
                )
                
                return f"""ðŸ”„ DevOps Tools Sync Complete

**Target Agents**: {len(target_agents)}
**Tool Categories**: {len(tool_categories)}
**Total Tools Synced**: {total_tools_synced}

{chr(10).join(sync_results)}

ðŸŒ All agents now have access to enhanced DevOps capabilities!"""
                
            except Exception as e:
                logger.error(f"Error syncing DevOps tools: {e}")
                return f"âŒ DevOps tools sync error: {str(e)}"

        # # @self.mcp.tool()
        async def install_devops_capabilities(
            target_machines: Optional[List[str]] = None,
            capability_types: Optional[List[str]] = None,
            dry_run: bool = False
        ) -> str:
            """Install DevOps tool dependencies on target machines"""
            try:
                if not target_machines:
                    target_machines = [self.storage.machine_id]
                
                if not capability_types:
                    capability_types = [
                        "monitoring_tools", "backup_tools", "security_tools",
                        "deployment_tools", "log_analysis", "network_tools"
                    ]
                
                installation_results = []
                
                for machine in target_machines:
                    machine_results = []
                    
                    for capability in capability_types:
                        if dry_run:
                            machine_results.append(f"[DRY RUN] Would install {capability}")
                        else:
                            # Simulate dependency installation
                            await self.storage.store_memory(
                                content=f"DevOps capability '{capability}' installed on {machine}",
                                category="devops_installations",
                                metadata={
                                    "machine": machine,
                                    "capability": capability,
                                    "installation_time": datetime.now().isoformat(),
                                    "status": "installed"
                                }
                            )
                            machine_results.append(f"âœ… Installed {capability}")
                    
                    installation_results.append(f"**{machine}**: {len(machine_results)} capabilities")
                
                action = "Would install" if dry_run else "Installed"
                return f"""ðŸ› ï¸ DevOps Capabilities Installation {'(DRY RUN)' if dry_run else ''}

**Target Machines**: {len(target_machines)}
**Capabilities**: {len(capability_types)}

{chr(10).join(installation_results)}

{action} dependencies for: {', '.join(capability_types)}"""
                
            except Exception as e:
                logger.error(f"Error installing DevOps capabilities: {e}")
                return f"âŒ DevOps installation error: {str(e)}"

        # # @self.mcp.tool()
        async def sync_tool_configurations(
            config_templates: Optional[Dict[str, Any]] = None,
            target_environments: Optional[List[str]] = None
        ) -> str:
            """Sync tool-specific configurations and settings across network"""
            try:
                if not config_templates:
                    config_templates = {
                        "monitoring": {
                            "prometheus_retention": "30d",
                            "alert_threshold": "80%",
                            "scrape_interval": "15s"
                        },
                        "backup": {
                            "retention_days": 30,
                            "compression": "gzip",
                            "encryption": "aes256"
                        },
                        "deployment": {
                            "rollback_timeout": "5m",
                            "health_check_retries": 3,
                            "canary_percentage": 10
                        }
                    }
                
                if not target_environments:
                    target_environments = ["production", "staging", "development"]
                
                sync_count = 0
                for env in target_environments:
                    for tool, config in config_templates.items():
                        await self.storage.store_memory(
                            content=f"Configuration template for {tool} in {env}: {json.dumps(config)}",
                            category="tool_configurations",
                            metadata={
                                "environment": env,
                                "tool": tool,
                                "config_version": "1.0",
                                "sync_time": datetime.now().isoformat()
                            }
                        )
                        sync_count += 1
                
                return f"""âš™ï¸ Tool Configurations Synced

**Environments**: {len(target_environments)}
**Tools**: {len(config_templates)}
**Total Configs**: {sync_count}

Configuration templates distributed for: {', '.join(config_templates.keys())}"""
                
            except Exception as e:
                logger.error(f"Error syncing tool configurations: {e}")
                return f"âŒ Tool configuration sync error: {str(e)}"

        # # @self.mcp.tool()
        async def validate_tool_installation(
            tools_to_check: Optional[List[str]] = None,
            run_health_checks: bool = True
        ) -> str:
            """Verify all tools are properly installed and functional"""
            try:
                if not tools_to_check:
                    tools_to_check = [
                        "category_management", "backup_system", "service_discovery",
                        "monitoring", "deployment", "security"
                    ]
                
                validation_results = {}
                
                for tool in tools_to_check:
                    # Check if tool definitions exist in memory
                    tool_memories = await self.storage.search_memories(
                        query=f"DevOps tool category: {tool}",
                        category="devops_tools",
                        limit=1
                    )
                    
                    if tool_memories and len(tool_memories) > 0:
                        validation_results[tool] = {
                            "installed": True,
                            "version": "1.0",
                            "health": "healthy" if run_health_checks else "unknown"
                        }
                    else:
                        validation_results[tool] = {
                            "installed": False,
                            "version": None,
                            "health": "unhealthy"
                        }
                
                healthy_tools = [t for t, r in validation_results.items() if r["installed"]]
                unhealthy_tools = [t for t, r in validation_results.items() if not r["installed"]]
                
                status = "âœ… All tools validated" if len(unhealthy_tools) == 0 else "âš ï¸ Some tools missing"
                
                return f"""{status}

**Validated Tools**: {len(healthy_tools)}/{len(tools_to_check)}
**Healthy**: {', '.join(healthy_tools) if healthy_tools else 'None'}
**Missing**: {', '.join(unhealthy_tools) if unhealthy_tools else 'None'}

Health checks: {'âœ… Enabled' if run_health_checks else 'âŒ Skipped'}"""
                
            except Exception as e:
                logger.error(f"Error validating tool installation: {e}")
                return f"âŒ Tool validation error: {str(e)}"

        # # @self.mcp.tool()
        async def rollback_tool_updates(
            tool_names: Optional[List[str]] = None,
            rollback_to_version: str = "previous"
        ) -> str:
            """Rollback to previous tool versions if issues occur"""
            try:
                if not tool_names:
                    tool_names = ["all"]
                
                rollback_results = []
                
                for tool in tool_names:
                    # Create rollback record
                    await self.storage.store_memory(
                        content=f"Rolled back {tool} to {rollback_to_version} version",
                        category="devops_rollbacks",
                        metadata={
                            "tool": tool,
                            "rollback_version": rollback_to_version,
                            "rollback_time": datetime.now().isoformat(),
                            "reason": "Manual rollback requested"
                        }
                    )
                    rollback_results.append(f"âœ… {tool}: Rolled back to {rollback_to_version}")
                
                return f"""ðŸ”„ Tool Rollback Complete

**Tools Rolled Back**: {len(tool_names)}
**Target Version**: {rollback_to_version}

{chr(10).join(rollback_results)}

âš ï¸ Please validate functionality after rollback"""
                
            except Exception as e:
                logger.error(f"Error rolling back tools: {e}")
                return f"âŒ Tool rollback error: {str(e)}"

        # # @self.mcp.tool()
        async def broadcast_tool_availability(
            tool_categories: Optional[List[str]] = None,
            target_roles: Optional[List[str]] = None
        ) -> str:
            """Notify all agents about new tool availability"""
            try:
                if not tool_categories:
                    tool_categories = ["all_devops_tools"]
                
                if not target_roles:
                    target_roles = ["all"]
                
                message = f"New DevOps tools available: {', '.join(tool_categories)}"
                
                # Use existing broadcast functionality
                result = await self.broadcast_discovery(
                    message=message,
                    category="tool_availability",
                    severity="info",
                    target_roles=target_roles
                )
                
                return f"""ðŸ“¡ Tool Availability Broadcast

**Categories**: {', '.join(tool_categories)}
**Target Roles**: {', '.join(target_roles)}

{result}"""
                
            except Exception as e:
                logger.error(f"Error broadcasting tool availability: {e}")
                return f"âŒ Broadcast error: {str(e)}"

        # # @self.mcp.tool()
        async def check_tool_dependencies(
            tools_to_check: Optional[List[str]] = None
        ) -> str:
            """Verify all dependencies are met before installation"""
            try:
                if not tools_to_check:
                    tools_to_check = [
                        "monitoring_stack", "backup_system", "security_tools",
                        "deployment_pipeline", "log_analysis"
                    ]
                
                dependency_results = {}
                
                for tool in tools_to_check:
                    # Define dependencies for each tool
                    tool_dependencies = {
                        "monitoring_stack": ["prometheus", "grafana", "alertmanager"],
                        "backup_system": ["rsync", "gzip", "gpg"],
                        "security_tools": ["nmap", "openssl", "fail2ban"],
                        "deployment_pipeline": ["docker", "kubectl", "helm"],
                        "log_analysis": ["elasticsearch", "logstash", "kibana"]
                    }
                    
                    deps = tool_dependencies.get(tool, ["basic_system"])
                    
                    # Simulate dependency checking
                    missing_deps = []  # In real implementation, check actual dependencies
                    
                    dependency_results[tool] = {
                        "total_deps": len(deps),
                        "satisfied": len(deps) - len(missing_deps),
                        "missing": missing_deps,
                        "status": "ready" if len(missing_deps) == 0 else "missing_deps"
                    }
                
                ready_tools = [t for t, r in dependency_results.items() if r["status"] == "ready"]
                blocked_tools = [t for t, r in dependency_results.items() if r["status"] == "missing_deps"]
                
                return f"""ðŸ” Dependency Check Results

**Tools Checked**: {len(tools_to_check)}
**Ready for Installation**: {len(ready_tools)}
**Blocked by Dependencies**: {len(blocked_tools)}

**Ready**: {', '.join(ready_tools) if ready_tools else 'None'}
**Blocked**: {', '.join(blocked_tools) if blocked_tools else 'None'}

{'âœ… All dependencies satisfied!' if len(blocked_tools) == 0 else 'âš ï¸ Resolve dependencies before proceeding'}"""
                
            except Exception as e:
                logger.error(f"Error checking dependencies: {e}")
                return f"âŒ Dependency check error: {str(e)}"

        # # @self.mcp.tool()
        async def sync_tool_permissions(
            role_permissions: Optional[Dict[str, List[str]]] = None
        ) -> str:
            """Sync RBAC permissions for new DevOps tools"""
            try:
                if not role_permissions:
                    role_permissions = {
                        "admin": ["all_tools", "manage_backups", "manage_deployments", "security_ops"],
                        "devops": ["monitoring", "deployments", "configurations", "service_discovery"],
                        "developer": ["logs", "service_health", "basic_monitoring"],
                        "readonly": ["view_status", "view_metrics", "view_logs"]
                    }
                
                permission_count = 0
                for role, permissions in role_permissions.items():
                    for permission in permissions:
                        await self.storage.store_memory(
                            content=f"Role '{role}' has permission: {permission}",
                            category="tool_permissions",
                            metadata={
                                "role": role,
                                "permission": permission,
                                "sync_time": datetime.now().isoformat(),
                                "rbac_version": "1.0"
                            }
                        )
                        permission_count += 1
                
                return f"""ðŸ” Tool Permissions Synced

**Roles Configured**: {len(role_permissions)}
**Total Permissions**: {permission_count}

**Role Summary**:
{chr(10).join([f"- {role}: {len(perms)} permissions" for role, perms in role_permissions.items()])}

RBAC permissions distributed across hAIveMind network!"""
                
            except Exception as e:
                logger.error(f"Error syncing tool permissions: {e}")
                return f"âŒ Permission sync error: {str(e)}"

        # Memory Category Management Tools
        # # @self.mcp.tool()
        async def create_memory_category(
            category_name: str,
            description: Optional[str] = None,
            retention_days: Optional[int] = None,
            access_policy: str = "default",
            encrypted: bool = False
        ) -> str:
            """Define new memory categories dynamically"""
            try:
                if not category_name or not category_name.strip():
                    return "âŒ Category name is required"
                
                category_name = category_name.lower().strip().replace(' ', '_')
                
                # Check if category already exists
                existing_categories = await self._get_memory_categories()
                if category_name in existing_categories:
                    return f"âŒ Category '{category_name}' already exists"
                
                # Create category metadata
                category_metadata = {
                    "name": category_name,
                    "description": description or f"Dynamic category: {category_name}",
                    "created_at": datetime.now().isoformat(),
                    "created_by": self.storage.agent_id,
                    "retention_days": retention_days or 365,
                    "access_policy": access_policy,
                    "encrypted": encrypted,
                    "status": "active",
                    "memory_count": 0
                }
                
                # Store category definition in hAIveMind memory
                memory_id = await self.storage.store_memory(
                    content=f"Memory category definition: {json.dumps(category_metadata)}",
                    category="category_definitions",
                    metadata=category_metadata
                )
                
                # Create ChromaDB collection for the new category
                collection_name = f"{category_name}_memories"
                try:
                    collection = self.storage.chroma_client.create_collection(
                        name=collection_name,
                        metadata={
                            "category": category_name,
                            "machine_id": self.storage.machine_id,
                            "created_at": datetime.now().isoformat()
                        }
                    )
                    collection_created = True
                except Exception as e:
                    collection_created = False
                    logger.warning(f"Could not create ChromaDB collection: {e}")
                
                # Broadcast category creation
                await self.broadcast_discovery(
                    message=f"New memory category created: {category_name}",
                    category="category_management",
                    severity="info"
                )
                
                return f"""âœ… Memory Category Created

**Name**: {category_name}
**Description**: {category_metadata['description']}
**Retention**: {category_metadata['retention_days']} days
**Access Policy**: {access_policy}
**Encrypted**: {'Yes' if encrypted else 'No'}
**ChromaDB Collection**: {'âœ… Created' if collection_created else 'âš ï¸ Manual creation required'}

Category is now available for storing memories!"""
                
            except Exception as e:
                logger.error(f"Error creating memory category: {e}")
                return f"âŒ Category creation error: {str(e)}"

        # # @self.mcp.tool()
        async def list_memory_categories(
            include_stats: bool = True,
            show_archived: bool = False
        ) -> str:
            """Get all available categories with stats"""
            try:
                categories = await self._get_memory_categories_with_stats() if include_stats else await self._get_memory_categories()
                
                if not categories:
                    return "No memory categories found"
                
                category_list = []
                total_memories = 0
                
                for cat_name, cat_info in categories.items():
                    if isinstance(cat_info, dict):
                        status = cat_info.get('status', 'active')
                        if not show_archived and status == 'archived':
                            continue
                            
                        memory_count = cat_info.get('memory_count', 0)
                        total_memories += memory_count
                        
                        category_list.append(
                            f"ðŸ“ **{cat_name}** ({status}) - {memory_count} memories"
                        )
                        
                        if cat_info.get('description'):
                            category_list.append(f"   ðŸ“ {cat_info['description']}")
                        
                        if cat_info.get('retention_days'):
                            category_list.append(f"   â° Retention: {cat_info['retention_days']} days")
                    else:
                        # Simple category name without stats
                        category_list.append(f"ðŸ“ **{cat_name}** - Active")
                
                return f"""ðŸ“Š Memory Categories Overview

**Total Categories**: {len([c for c in categories.keys() if not show_archived or categories[c].get('status', 'active') != 'archived'])}
**Total Memories**: {total_memories}

{chr(10).join(category_list)}

ðŸ’¡ Use `create_memory_category` to add new categories"""
                
            except Exception as e:
                logger.error(f"Error listing categories: {e}")
                return f"âŒ Error listing categories: {str(e)}"

        # # @self.mcp.tool()
        async def update_category_settings(
            category_name: str,
            new_description: Optional[str] = None,
            new_retention_days: Optional[int] = None,
            new_access_policy: Optional[str] = None
        ) -> str:
            """Configure retention, access policies for categories"""
            try:
                category_name = category_name.lower().strip()
                
                # Get existing category definition
                existing_categories = await self._get_memory_categories_with_stats()
                if category_name not in existing_categories:
                    return f"âŒ Category '{category_name}' does not exist"
                
                current_settings = existing_categories[category_name]
                updates = {}
                
                if new_description is not None:
                    updates['description'] = new_description
                if new_retention_days is not None:
                    updates['retention_days'] = new_retention_days
                if new_access_policy is not None:
                    updates['access_policy'] = new_access_policy
                
                if not updates:
                    return "âŒ No updates provided"
                
                # Update category metadata
                updated_settings = {**current_settings, **updates}
                updated_settings['updated_at'] = datetime.now().isoformat()
                updated_settings['updated_by'] = self.storage.agent_id
                
                # Store updated definition
                await self.storage.store_memory(
                    content=f"Updated category settings: {json.dumps(updated_settings)}",
                    category="category_definitions",
                    metadata=updated_settings
                )
                
                changes = []
                for key, value in updates.items():
                    old_value = current_settings.get(key, 'Not set')
                    changes.append(f"- {key}: {old_value} â†’ {value}")
                
                return f"""âœ… Category Settings Updated

**Category**: {category_name}

**Changes Made**:
{chr(10).join(changes)}

Settings applied successfully!"""
                
            except Exception as e:
                logger.error(f"Error updating category settings: {e}")
                return f"âŒ Settings update error: {str(e)}"

        # # @self.mcp.tool()
        async def archive_category(
            category_name: str,
            archive_reason: Optional[str] = None
        ) -> str:
            """Archive old categories without deletion"""
            try:
                category_name = category_name.lower().strip()
                
                # Get existing category
                existing_categories = await self._get_memory_categories_with_stats()
                if category_name not in existing_categories:
                    return f"âŒ Category '{category_name}' does not exist"
                
                current_settings = existing_categories[category_name]
                if current_settings.get('status') == 'archived':
                    return f"âš ï¸ Category '{category_name}' is already archived"
                
                # Archive the category
                archived_settings = {
                    **current_settings,
                    'status': 'archived',
                    'archived_at': datetime.now().isoformat(),
                    'archived_by': self.storage.agent_id,
                    'archive_reason': archive_reason or "Manual archive"
                }
                
                # Store archived definition
                await self.storage.store_memory(
                    content=f"Archived category: {json.dumps(archived_settings)}",
                    category="category_definitions",
                    metadata=archived_settings
                )
                
                memory_count = current_settings.get('memory_count', 0)
                
                return f"""ðŸ“¦ Category Archived

**Category**: {category_name}
**Reason**: {archive_reason or 'Manual archive'}
**Memories Preserved**: {memory_count}

The category is now archived. Memories are preserved but no new memories can be added.
Use `restore_category` to reactivate if needed."""
                
            except Exception as e:
                logger.error(f"Error archiving category: {e}")
                return f"âŒ Archive error: {str(e)}"

        # # @self.mcp.tool()
        async def backup_category(
            category_name: str,
            include_memories: bool = True,
            backup_location: Optional[str] = None
        ) -> str:
            """Backup entire category with all memories"""
            try:
                category_name = category_name.lower().strip()
                
                # Get category information
                existing_categories = await self._get_memory_categories_with_stats()
                if category_name not in existing_categories:
                    return f"âŒ Category '{category_name}' does not exist"
                
                category_info = existing_categories[category_name]
                
                # Create backup metadata
                backup_id = f"{category_name}_backup_{int(time.time())}"
                backup_metadata = {
                    'backup_id': backup_id,
                    'category': category_name,
                    'backup_time': datetime.now().isoformat(),
                    'backup_by': self.storage.agent_id,
                    'include_memories': include_memories,
                    'category_settings': category_info
                }
                
                memories_backed_up = 0
                if include_memories:
                    # Get all memories from this category
                    try:
                        memories = await self.storage.search_memories(
                            query="*",  # Get all memories
                            category=category_name,
                            limit=10000  # Large limit to get all
                        )
                        memories_backed_up = len(memories)
                        backup_metadata['memories'] = memories
                    except Exception as e:
                        logger.warning(f"Could not backup memories: {e}")
                        backup_metadata['backup_warnings'] = [f"Memory backup failed: {str(e)}"]
                
                # Store backup
                backup_memory_id = await self.storage.store_memory(
                    content=f"Category backup: {json.dumps(backup_metadata)}",
                    category="category_backups", 
                    metadata=backup_metadata
                )
                
                return f"""ðŸ’¾ Category Backup Complete

**Backup ID**: {backup_id}
**Category**: {category_name}
**Memories Backed Up**: {memories_backed_up}
**Settings Backed Up**: âœ… Yes
**Backup Location**: hAIveMind memory (ID: {backup_memory_id})

Backup stored successfully! Use `restore_category` with backup ID to restore."""
                
            except Exception as e:
                logger.error(f"Error backing up category: {e}")
                return f"âŒ Backup error: {str(e)}"

        # # @self.mcp.tool()
        async def restore_category(
            backup_id: str,
            new_category_name: Optional[str] = None,
            restore_memories: bool = True
        ) -> str:
            """Restore category from backup"""
            try:
                # Find the backup
                backup_memories = await self.storage.search_memories(
                    query=backup_id,
                    category="category_backups",
                    limit=1
                )
                
                if not backup_memories:
                    return f"âŒ Backup '{backup_id}' not found"
                
                backup_data = backup_memories[0]
                backup_metadata = backup_data.get('metadata', {})
                
                # Parse backup content
                try:
                    backup_content = json.loads(backup_data['content'].split(': ', 1)[1])
                except:
                    return f"âŒ Invalid backup format for {backup_id}"
                
                original_category = backup_content['category']
                target_category = new_category_name or original_category
                target_category = target_category.lower().strip()
                
                # Check if target category exists
                existing_categories = await self._get_memory_categories()
                if target_category in existing_categories:
                    return f"âŒ Category '{target_category}' already exists. Choose a different name."
                
                # Restore category settings
                category_settings = backup_content['category_settings']
                category_settings['name'] = target_category
                category_settings['status'] = 'active'
                category_settings['restored_at'] = datetime.now().isoformat()
                category_settings['restored_by'] = self.storage.agent_id
                category_settings['restored_from_backup'] = backup_id
                
                # Store restored category definition
                await self.storage.store_memory(
                    content=f"Restored category definition: {json.dumps(category_settings)}",
                    category="category_definitions",
                    metadata=category_settings
                )
                
                # Create ChromaDB collection
                collection_name = f"{target_category}_memories"
                try:
                    self.storage.chroma_client.create_collection(
                        name=collection_name,
                        metadata={
                            "category": target_category,
                            "machine_id": self.storage.machine_id,
                            "restored_from_backup": backup_id
                        }
                    )
                    collection_restored = True
                except Exception as e:
                    collection_restored = False
                    logger.warning(f"Could not create collection: {e}")
                
                memories_restored = 0
                if restore_memories and 'memories' in backup_content:
                    # Restore memories (simplified - in production would restore to ChromaDB)
                    memories = backup_content['memories']
                    memories_restored = len(memories)
                
                return f"""â™»ï¸ Category Restore Complete

**Restored Category**: {target_category}
**Original Category**: {original_category}
**Backup ID**: {backup_id}
**Settings Restored**: âœ… Yes
**ChromaDB Collection**: {'âœ… Created' if collection_restored else 'âš ï¸ Manual creation needed'}
**Memories Restored**: {memories_restored}

Category successfully restored from backup!"""
                
            except Exception as e:
                logger.error(f"Error restoring category: {e}")
                return f"âŒ Restore error: {str(e)}"

        async def _get_memory_categories(self) -> Dict[str, Any]:
            """Helper method to get all memory categories"""
            try:
                # Get from config first
                config_categories = self.config.get('memory', {}).get('categories', [])
                categories = {cat: {'status': 'active', 'source': 'config'} for cat in config_categories}
                
                # Get dynamic categories from memory
                dynamic_categories = await self.storage.search_memories(
                    query="Memory category definition",
                    category="category_definitions", 
                    limit=100
                )
                
                for cat_mem in dynamic_categories:
                    try:
                        cat_content = cat_mem['content']
                        if ': {' in cat_content:
                            cat_data = json.loads(cat_content.split(': ', 1)[1])
                            cat_name = cat_data.get('name')
                            if cat_name:
                                categories[cat_name] = {**cat_data, 'source': 'dynamic'}
                    except:
                        continue
                
                return categories
            except:
                return {}

        async def _get_memory_categories_with_stats(self) -> Dict[str, Any]:
            """Helper method to get categories with memory counts"""
            try:
                categories = await self._get_memory_categories()
                
                for cat_name in categories.keys():
                    try:
                        # Get memory count for this category
                        memories = await self.storage.search_memories(
                            query="*",
                            category=cat_name,
                            limit=1  # Just to get count, not actual memories
                        )
                        # This is simplified - in real implementation would get actual counts
                        categories[cat_name]['memory_count'] = len(memories) if memories else 0
                    except:
                        categories[cat_name]['memory_count'] = 0
                
                return categories
            except:
                return {}

        # Project Management Integration Tools
        # # @self.mcp.tool()
        async def create_project(
            project_name: str,
            git_repo_path: str,
            description: Optional[str] = None,
            setup_script: Optional[str] = None,
            dev_script: Optional[str] = None,
            cleanup_script: Optional[str] = None
        ) -> str:
            """Initialize new project contexts with enhanced DevOps integration"""
            try:
                if not project_name or not project_name.strip():
                    return "âŒ Project name is required"
                
                if not git_repo_path or not git_repo_path.strip():
                    return "âŒ Git repository path is required"
                
                # Create project metadata
                project_metadata = {
                    "name": project_name.strip(),
                    "git_repo_path": git_repo_path.strip(),
                    "description": description or f"DevOps project: {project_name}",
                    "setup_script": setup_script,
                    "dev_script": dev_script,
                    "cleanup_script": cleanup_script,
                    "created_at": datetime.now().isoformat(),
                    "created_by": self.storage.agent_id,
                    "status": "active",
                    "devops_features": {
                        "monitoring_enabled": False,
                        "backup_enabled": False,
                        "deployment_pipeline": False,
                        "security_scanning": False
                    }
                }
                
                # Store project definition in hAIveMind memory
                memory_id = await self.storage.store_memory(
                    content=f"DevOps project definition: {json.dumps(project_metadata)}",
                    category="project_definitions",
                    metadata=project_metadata
                )
                
                # Create dedicated memory category for this project
                project_category = f"{project_name.lower().replace(' ', '_')}_project"
                try:
                    await self.create_memory_category(
                        category_name=project_category,
                        description=f"Project-specific memories for {project_name}",
                        retention_days=730  # 2 years for project data
                    )
                    project_category_created = True
                except:
                    project_category_created = False
                
                # Broadcast project creation
                await self.broadcast_discovery(
                    message=f"New DevOps project created: {project_name}",
                    category="project_management",
                    severity="info"
                )
                
                return f"""âœ… DevOps Project Created

**Name**: {project_name}
**Repository**: {git_repo_path}
**Description**: {project_metadata['description']}
**Memory Category**: {'âœ… Created' if project_category_created else 'âš ï¸ Manual creation needed'} ({project_category})
**DevOps Features**: Ready for configuration

**Available Scripts**:
{f'- Setup: {setup_script}' if setup_script else '- Setup: Not configured'}
{f'- Dev: {dev_script}' if dev_script else '- Dev: Not configured'}  
{f'- Cleanup: {cleanup_script}' if cleanup_script else '- Cleanup: Not configured'}

Project is ready for DevOps operations! Use `switch_project_context` to activate."""
                
            except Exception as e:
                logger.error(f"Error creating project: {e}")
                return f"âŒ Project creation error: {str(e)}"

        # # @self.mcp.tool()
        async def list_projects(
            include_devops_status: bool = True,
            show_archived: bool = False,
            limit: int = 20,
            offset: int = 0
        ) -> str:
            """Show all projects with enhanced DevOps metadata"""
            try:
                # Get projects from vibe_kanban
                kanban_projects = []
                try:
                    from mcp import client_session
                    # This would normally call the kanban API, simplified for now
                    kanban_projects = []
                except:
                    pass
                
                # Get DevOps project definitions from memory
                all_devops_projects = await self.storage.search_memories(
                    query="DevOps project definition",
                    category="project_definitions",
                    limit=200  # Get more to enable pagination
                )
                
                # Apply pagination
                total_projects = len(all_devops_projects)
                devops_projects = all_devops_projects[offset:offset + limit]
                
                # Get current project context
                current_project = await self._get_current_project_context()
                
                project_list = []
                
                # Process DevOps project definitions
                for proj_mem in devops_projects:
                    try:
                        proj_content = proj_mem['content']
                        if ': {' in proj_content:
                            proj_data = json.loads(proj_content.split(': ', 1)[1])
                            proj_name = proj_data.get('name', 'Unknown')
                            status = proj_data.get('status', 'active')
                            
                            if not show_archived and status == 'archived':
                                continue
                            
                            # Check if this is the current active project
                            active_indicator = "ðŸŽ¯" if current_project and current_project.get('name') == proj_name else "ðŸ“"
                            
                            project_list.append(f"{active_indicator} **{proj_name}** ({status})")
                            project_list.append(f"   ðŸ“‚ {proj_data.get('git_repo_path', 'No repo path')}")
                            
                            if proj_data.get('description'):
                                project_list.append(f"   ðŸ“ {proj_data['description']}")
                            
                            if include_devops_status:
                                devops_features = proj_data.get('devops_features', {})
                                enabled_features = [k for k, v in devops_features.items() if v]
                                project_list.append(f"   ðŸ› ï¸ DevOps: {len(enabled_features)} features enabled")
                            
                            project_list.append("")  # Empty line for spacing
                            
                    except Exception as e:
                        logger.warning(f"Could not parse project data: {e}")
                        continue
                
                if not project_list:
                    return """ðŸ“‚ No DevOps projects found

Use `create_project` to create your first DevOps-enabled project with:
- Automated deployment pipelines
- Monitoring integration
- Backup and recovery
- Security scanning
- Configuration management"""
                
                # Add pagination info
                has_more = offset + limit < total_projects
                pagination_info = ""
                if total_projects > limit:
                    pagination_info = f"\nðŸ“„ Page {offset//limit + 1} ({offset + 1}-{min(offset + limit, total_projects)} of {total_projects})"
                    if has_more:
                        pagination_info += f" | Use offset={offset + limit} for next page"
                
                return f"""ðŸ“Š DevOps Projects Overview

**Total Projects**: {total_projects}
**Current Active**: {current_project.get('name', 'None selected') if current_project else 'None selected'}
**Showing**: {len(devops_projects)} projects{pagination_info}

{chr(10).join(project_list)}

ðŸ’¡ Use `switch_project_context [name]` to switch active project
ðŸ”§ Use `project_health_check` to analyze current project status"""
                
            except Exception as e:
                logger.error(f"Error listing projects: {e}")
                return f"âŒ Error listing projects: {str(e)}"

        # # @self.mcp.tool()
        async def switch_project_context(
            project_name: str
        ) -> str:
            """Change active project scope for DevOps operations"""
            try:
                if not project_name or not project_name.strip():
                    return "âŒ Project name is required"
                
                project_name = project_name.strip()
                
                # Find the project
                devops_projects = await self.storage.search_memories(
                    query=f"DevOps project definition",
                    category="project_definitions",
                    limit=50
                )
                
                target_project = None
                for proj_mem in devops_projects:
                    try:
                        proj_content = proj_mem['content']
                        if ': {' in proj_content:
                            proj_data = json.loads(proj_content.split(': ', 1)[1])
                            if proj_data.get('name', '').lower() == project_name.lower():
                                target_project = proj_data
                                break
                    except:
                        continue
                
                if not target_project:
                    return f"âŒ Project '{project_name}' not found. Use `list_projects` to see available projects."
                
                # Set as current project context
                context_data = {
                    "current_project": target_project,
                    "switched_at": datetime.now().isoformat(),
                    "switched_by": self.storage.agent_id,
                    "previous_context": await self._get_current_project_context()
                }
                
                # Store context in memory
                await self.storage.store_memory(
                    content=f"Project context switch: {json.dumps(context_data)}",
                    category="project_context",
                    metadata=context_data
                )
                
                # Get project health status
                health_status = await self._check_project_health(target_project)
                
                return f"""ðŸŽ¯ Project Context Switched

**Active Project**: {target_project['name']}
**Repository**: {target_project['git_repo_path']}
**Status**: {target_project.get('status', 'active')}
**Health**: {health_status}

**DevOps Features Available**:
- Memory category: {target_project['name'].lower().replace(' ', '_')}_project
- Scoped operations for this project
- Project-specific monitoring and alerts
- Dedicated backup and restore

All DevOps operations will now be scoped to this project."""
                
            except Exception as e:
                logger.error(f"Error switching project context: {e}")
                return f"âŒ Context switch error: {str(e)}"

        # # @self.mcp.tool()
        async def project_health_check(
            project_name: Optional[str] = None,
            include_recommendations: bool = True
        ) -> str:
            """Analyze project status and provide DevOps health insights"""
            try:
                # Get target project
                if project_name:
                    # Find specific project
                    devops_projects = await self.storage.search_memories(
                        query="DevOps project definition",
                        category="project_definitions", 
                        limit=50
                    )
                    
                    target_project = None
                    for proj_mem in devops_projects:
                        try:
                            proj_content = proj_mem['content']
                            if ': {' in proj_content:
                                proj_data = json.loads(proj_content.split(': ', 1)[1])
                                if proj_data.get('name', '').lower() == project_name.lower():
                                    target_project = proj_data
                                    break
                        except:
                            continue
                    
                    if not target_project:
                        return f"âŒ Project '{project_name}' not found"
                else:
                    # Use current project context
                    target_project = await self._get_current_project_context()
                    if not target_project:
                        return "âŒ No active project. Use `switch_project_context` to select a project."
                
                # Perform comprehensive health check
                health_results = {
                    "project_name": target_project['name'],
                    "overall_health": "unknown",
                    "checks_performed": [],
                    "issues_found": [],
                    "recommendations": []
                }
                
                # Check repository accessibility
                repo_path = target_project.get('git_repo_path')
                if repo_path:
                    import os
                    if os.path.exists(repo_path):
                        health_results["checks_performed"].append("âœ… Repository accessible")
                        if os.path.exists(os.path.join(repo_path, '.git')):
                            health_results["checks_performed"].append("âœ… Git repository valid")
                        else:
                            health_results["issues_found"].append("âš ï¸ Not a Git repository")
                    else:
                        health_results["issues_found"].append("âŒ Repository path not accessible")
                        health_results["recommendations"].append("Verify repository path and permissions")
                
                # Check DevOps features configuration
                devops_features = target_project.get('devops_features', {})
                enabled_features = sum(1 for v in devops_features.values() if v)
                total_features = len(devops_features)
                
                if enabled_features == 0:
                    health_results["issues_found"].append("âš ï¸ No DevOps features enabled")
                    health_results["recommendations"].append("Enable monitoring, backup, or deployment features")
                elif enabled_features < total_features / 2:
                    health_results["issues_found"].append(f"âš ï¸ Only {enabled_features}/{total_features} DevOps features enabled")
                else:
                    health_results["checks_performed"].append(f"âœ… {enabled_features}/{total_features} DevOps features enabled")
                
                # Check for project-specific memories
                project_category = f"{target_project['name'].lower().replace(' ', '_')}_project"
                try:
                    project_memories = await self.storage.search_memories(
                        query="*",
                        category=project_category,
                        limit=1
                    )
                    if project_memories:
                        health_results["checks_performed"].append("âœ… Project memory category active")
                    else:
                        health_results["issues_found"].append("âš ï¸ No project-specific memories found")
                except:
                    health_results["issues_found"].append("âŒ Project memory category not accessible")
                
                # Check scripts configuration
                scripts_configured = []
                for script_type in ['setup_script', 'dev_script', 'cleanup_script']:
                    if target_project.get(script_type):
                        scripts_configured.append(script_type.replace('_script', ''))
                
                if scripts_configured:
                    health_results["checks_performed"].append(f"âœ… Scripts configured: {', '.join(scripts_configured)}")
                else:
                    health_results["issues_found"].append("âš ï¸ No automation scripts configured")
                    health_results["recommendations"].append("Add setup, dev, and cleanup scripts for automation")
                
                # Determine overall health
                if len(health_results["issues_found"]) == 0:
                    health_results["overall_health"] = "healthy"
                elif len(health_results["issues_found"]) <= 2:
                    health_results["overall_health"] = "warning"
                else:
                    health_results["overall_health"] = "critical"
                
                # Format health report
                health_icon = {
                    "healthy": "ðŸ’š",
                    "warning": "ðŸŸ¡", 
                    "critical": "ðŸ”´",
                    "unknown": "âšª"
                }[health_results["overall_health"]]
                
                report = f"""{health_icon} Project Health Check: {target_project['name']}

**Overall Status**: {health_results["overall_health"].upper()}
**Repository**: {target_project.get('git_repo_path', 'Not specified')}

**Health Checks Passed**:
{chr(10).join(health_results["checks_performed"]) if health_results["checks_performed"] else "None"}

**Issues Found**:
{chr(10).join(health_results["issues_found"]) if health_results["issues_found"] else "None"}"""

                if include_recommendations and health_results["recommendations"]:
                    report += f"""

**Recommendations**:
{chr(10).join([f"ðŸ’¡ {rec}" for rec in health_results["recommendations"]])}"""
                
                return report
                
            except Exception as e:
                logger.error(f"Error in project health check: {e}")
                return f"âŒ Health check error: {str(e)}"

        # # @self.mcp.tool()
        async def backup_project(
            project_name: Optional[str] = None,
            include_git_data: bool = True,
            include_memories: bool = True
        ) -> str:
            """Complete project backup including configs, memories, and Git data"""
            try:
                # Get target project
                if project_name:
                    target_project = await self._find_project_by_name(project_name)
                else:
                    target_project = await self._get_current_project_context()
                
                if not target_project:
                    return "âŒ No project specified or active. Use `switch_project_context` first."
                
                # Create backup metadata
                backup_id = f"{target_project['name'].lower().replace(' ', '_')}_backup_{int(time.time())}"
                backup_metadata = {
                    'backup_id': backup_id,
                    'project_name': target_project['name'],
                    'backup_time': datetime.now().isoformat(),
                    'backup_by': self.storage.agent_id,
                    'include_git_data': include_git_data,
                    'include_memories': include_memories,
                    'project_settings': target_project
                }
                
                backup_components = []
                
                # Backup project memories
                memories_backed_up = 0
                if include_memories:
                    project_category = f"{target_project['name'].lower().replace(' ', '_')}_project"
                    try:
                        memories = await self.storage.search_memories(
                            query="*",
                            category=project_category,
                            limit=10000
                        )
                        memories_backed_up = len(memories)
                        backup_metadata['memories'] = memories
                        backup_components.append(f"âœ… {memories_backed_up} memories")
                    except Exception as e:
                        backup_components.append(f"âš ï¸ Memory backup failed: {str(e)}")
                
                # Backup Git repository information
                if include_git_data:
                    repo_path = target_project.get('git_repo_path')
                    if repo_path:
                        import os
                        try:
                            if os.path.exists(repo_path):
                                # Get Git info (simplified - in production would backup actual Git data)
                                backup_metadata['git_info'] = {
                                    'repo_path': repo_path,
                                    'exists': True,
                                    'backup_note': 'Repository path recorded - manual Git backup recommended'
                                }
                                backup_components.append("âœ… Git repository info")
                            else:
                                backup_metadata['git_info'] = {'repo_path': repo_path, 'exists': False}
                                backup_components.append("âš ï¸ Git repository not accessible")
                        except Exception as e:
                            backup_components.append(f"âš ï¸ Git backup error: {str(e)}")
                
                # Store backup
                backup_memory_id = await self.storage.store_memory(
                    content=f"Project backup: {json.dumps(backup_metadata)}",
                    category="project_backups",
                    metadata=backup_metadata
                )
                
                return f"""ðŸ’¾ Project Backup Complete

**Backup ID**: {backup_id}
**Project**: {target_project['name']}
**Backup Location**: hAIveMind memory (ID: {backup_memory_id})

**Components Backed Up**:
{chr(10).join(backup_components)}

**Backup Contents**:
- Project configuration and metadata
- DevOps feature settings
- Script configurations
{f'- {memories_backed_up} project memories' if include_memories else ''}
{f'- Git repository information' if include_git_data else ''}

Use `restore_project {backup_id}` to restore from this backup."""
                
            except Exception as e:
                logger.error(f"Error backing up project: {e}")
                return f"âŒ Project backup error: {str(e)}"

        # # @self.mcp.tool()
        async def restore_project(
            backup_id: str,
            new_project_name: Optional[str] = None,
            restore_memories: bool = True
        ) -> str:
            """Restore project state from backup"""
            try:
                # Find the backup
                backup_memories = await self.storage.search_memories(
                    query=backup_id,
                    category="project_backups",
                    limit=1
                )
                
                if not backup_memories:
                    return f"âŒ Project backup '{backup_id}' not found"
                
                backup_data = backup_memories[0]
                
                # Parse backup content
                try:
                    backup_content = json.loads(backup_data['content'].split(': ', 1)[1])
                except:
                    return f"âŒ Invalid backup format for {backup_id}"
                
                original_project = backup_content['project_settings']
                target_name = new_project_name or original_project['name']
                
                # Check if target project name already exists
                existing_project = await self._find_project_by_name(target_name)
                if existing_project:
                    return f"âŒ Project '{target_name}' already exists. Choose a different name."
                
                # Restore project settings
                restored_project = {
                    **original_project,
                    'name': target_name,
                    'restored_at': datetime.now().isoformat(),
                    'restored_by': self.storage.agent_id,
                    'restored_from_backup': backup_id
                }
                
                # Store restored project definition
                await self.storage.store_memory(
                    content=f"DevOps project definition: {json.dumps(restored_project)}",
                    category="project_definitions",
                    metadata=restored_project
                )
                
                # Create project memory category
                project_category = f"{target_name.lower().replace(' ', '_')}_project"
                try:
                    await self.create_memory_category(
                        category_name=project_category,
                        description=f"Restored project memories for {target_name}",
                        retention_days=730
                    )
                    category_restored = True
                except:
                    category_restored = False
                
                # Restore memories if requested
                memories_restored = 0
                if restore_memories and 'memories' in backup_content:
                    try:
                        memories = backup_content['memories']
                        memories_restored = len(memories)
                        # In production, would actually restore memories to ChromaDB
                    except:
                        memories_restored = 0
                
                restore_components = [
                    "âœ… Project configuration",
                    "âœ… DevOps feature settings",
                    "âœ… Script configurations"
                ]
                
                if category_restored:
                    restore_components.append("âœ… Memory category created")
                else:
                    restore_components.append("âš ï¸ Memory category creation failed")
                
                if restore_memories:
                    restore_components.append(f"âœ… {memories_restored} memories restored")
                
                return f"""â™»ï¸ Project Restore Complete

**Restored Project**: {target_name}
**Original Project**: {original_project['name']}
**Backup ID**: {backup_id}
**Repository Path**: {restored_project.get('git_repo_path', 'Not specified')}

**Components Restored**:
{chr(10).join(restore_components)}

Project '{target_name}' is now available for DevOps operations!
Use `switch_project_context {target_name}` to activate."""
                
            except Exception as e:
                logger.error(f"Error restoring project: {e}")
                return f"âŒ Project restore error: {str(e)}"

        # Helper methods for project management
        async def _get_current_project_context(self) -> Optional[Dict[str, Any]]:
            """Get the current active project context"""
            try:
                context_memories = await self.storage.search_memories(
                    query="Project context switch",
                    category="project_context",
                    limit=1
                )
                
                if context_memories:
                    context_data = context_memories[0]
                    if 'metadata' in context_data and 'current_project' in context_data['metadata']:
                        return context_data['metadata']['current_project']
                
                return None
            except:
                return None

        async def _find_project_by_name(self, project_name: str) -> Optional[Dict[str, Any]]:
            """Find a project by name"""
            try:
                devops_projects = await self.storage.search_memories(
                    query="DevOps project definition",
                    category="project_definitions",
                    limit=50
                )
                
                for proj_mem in devops_projects:
                    try:
                        proj_content = proj_mem['content']
                        if ': {' in proj_content:
                            proj_data = json.loads(proj_content.split(': ', 1)[1])
                            if proj_data.get('name', '').lower() == project_name.lower():
                                return proj_data
                    except:
                        continue
                
                return None
            except:
                return None

        async def _check_project_health(self, project_data: Dict[str, Any]) -> str:
            """Quick health check for a project"""
            try:
                issues = 0
                
                # Check repository
                repo_path = project_data.get('git_repo_path')
                if repo_path:
                    import os
                    if not os.path.exists(repo_path):
                        issues += 1
                
                # Check DevOps features
                devops_features = project_data.get('devops_features', {})
                if not any(devops_features.values()):
                    issues += 1
                
                if issues == 0:
                    return "Healthy ðŸ’š"
                elif issues <= 1:
                    return "Warning ðŸŸ¡"
                else:
                    return "Critical ðŸ”´"
            except:
                return "Unknown âšª"
        
        # Agent directive system tools
        # # @self.mcp.tool()
        async def get_agent_directive(agent_type: str = "claude_code_agent") -> str:
            """Get installation directive for specific agent type"""
            try:
                directive = await self.agent_directives.get_agent_directive(agent_type)
                return json.dumps(directive, indent=2)
            except Exception as e:
                logger.error(f"Error getting agent directive: {e}")
                return f"Error getting agent directive: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_installation_instructions(agent_id: Optional[str] = None) -> str:
            """Get human-readable installation instructions for connecting to hAIveMind"""
            try:
                instructions = await self.agent_directives.get_installation_instructions(agent_id)
                return instructions
            except Exception as e:
                logger.error(f"Error getting installation instructions: {e}")
                return f"Error getting installation instructions: {str(e)}"
        
        # # @self.mcp.tool()
        async def generate_installation_script(agent_type: str = "claude_code_agent") -> str:
            """Generate executable installation script from directive"""
            try:
                directive = await self.agent_directives.get_agent_directive(agent_type)
                script = await self.agent_directives.generate_installation_script(directive)
                return script
            except Exception as e:
                logger.error(f"Error generating installation script: {e}")
                return f"Error generating installation script: {str(e)}"
        
        # # @self.mcp.tool()
        async def execute_agent_directive(
            agent_type: str = "claude_code_agent",
            agent_id: Optional[str] = None,
            auto_execute: bool = True
        ) -> str:
            """Execute agent installation directive automatically"""
            try:
                if not agent_id:
                    import socket
                    agent_id = f"claude_code_agent_{socket.gethostname()}_{int(time.time())}"
                
                directive = await self.agent_directives.get_agent_directive(agent_type)
                
                if not auto_execute:
                    instructions = await self.agent_directives.get_installation_instructions(agent_id)
                    return f"Manual execution required. Instructions:\n\n{instructions}"
                
                # Execute directive steps
                results = []
                
                for step in directive.get("steps", []):
                    step_result = {
                        "step_id": step.get("id"),
                        "step_name": step.get("name"),
                        "success": False,
                        "message": "",
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    try:
                        if step.get("action") == "mcp_call":
                            tool_name = step.get("tool")
                            parameters = step.get("parameters", {})
                            
                            # Execute MCP tool calls directly
                            if tool_name == "install_agent_commands":
                                result = await self.command_installer.sync_agent_installation(
                                    agent_id=agent_id,
                                    force=parameters.get("force", False),
                                    verbose=parameters.get("verbose", False)
                                )
                                step_result["success"] = "success" in str(result).lower()
                                step_result["message"] = str(result)
                                
                            elif tool_name == "register_agent":
                                # Register agent with hAIveMind
                                role = parameters.get("role", "claude_code_agent")
                                capabilities = parameters.get("capabilities", [])
                                
                                registration_result = await self.storage.store_memory(
                                    content=f"Agent {agent_id} registered with role {role}",
                                    category="infrastructure",
                                    context=f"Agent registration for {agent_id}",
                                    metadata={
                                        "agent_id": agent_id,
                                        "role": role,
                                        "capabilities": capabilities,
                                        "registration_time": datetime.now().isoformat()
                                    },
                                    tags=["agent-registration", agent_id, role]
                                )
                                
                                step_result["success"] = True
                                step_result["message"] = f"Agent registered: {registration_result}"
                                
                            elif tool_name == "get_agent_roster":
                                roster = await self.storage.search_memories(
                                    query="agent-registration",
                                    category="infrastructure",
                                    limit=parameters.get("limit", 5)
                                )
                                step_result["success"] = len(roster) > 0
                                step_result["message"] = f"Found {len(roster)} agents in collective"
                        
                        elif step.get("action") == "bash":
                            # For bash commands, we'd need to implement shell execution
                            # For now, mark as success with instruction
                            step_result["success"] = True
                            step_result["message"] = f"Execute: {step.get('command')}"
                        
                        elif step.get("action") == "create_files":
                            # For file creation, we'd need file system access
                            # For now, provide the file list
                            files = step.get("files", [])
                            step_result["success"] = True
                            step_result["message"] = f"Create {len(files)} files: {[f['path'] for f in files]}"
                        
                    except Exception as e:
                        step_result["success"] = False
                        step_result["message"] = f"Step failed: {str(e)}"
                    
                    results.append(step_result)
                
                # Store execution results
                memory_id = await self.agent_directives.store_directive_execution(
                    agent_id, directive.get("directive_id", "unknown"), results
                )
                
                success_count = len([r for r in results if r["success"]])
                total_steps = len(results)
                
                response = f"ðŸ¤– Agent Directive Execution Complete!\n\n"
                response += f"Agent ID: {agent_id}\n"
                response += f"Success Rate: {success_count}/{total_steps} steps\n"
                response += f"Execution ID: {memory_id}\n\n"
                response += "Step Results:\n"
                
                for result in results:
                    status = "âœ…" if result["success"] else "âŒ"
                    response += f"{status} {result['step_name']}: {result['message']}\n"
                
                if success_count == total_steps:
                    response += f"\nðŸŽ‰ Welcome to the hAIveMind collective, {agent_id}!"
                else:
                    response += f"\nâš ï¸  Some steps failed. Check logs and retry if needed."
                
                return response
                
            except Exception as e:
                logger.error(f"Error executing agent directive: {e}")
                return f"Error executing agent directive: {str(e)}"
        
        logger.info("ðŸ¤ All hive mind tools synchronized with network portal - collective intelligence ready")
    
    def _register_hosting_tools(self):
        """Register MCP server hosting tools"""
        
        # # @self.mcp.tool()
        async def upload_mcp_server(
            name: str,
            archive_base64: str,
            command: List[str],
            description: str = "",
            environment: Optional[Dict[str, str]] = None,
            auto_restart: bool = True,
            resource_limits: Optional[Dict[str, Any]] = None,
            health_check_url: Optional[str] = None,
            user_id: str = "default"
        ) -> str:
            """Upload and deploy a new MCP server from base64-encoded archive"""
            return await self.hosting_tools.upload_mcp_server(
                name=name,
                archive_base64=archive_base64,
                command=command,
                description=description,
                environment=environment,
                auto_restart=auto_restart,
                resource_limits=resource_limits,
                health_check_url=health_check_url,
                user_id=user_id
            )
        
        # # @self.mcp.tool()
        async def start_mcp_server(server_id: str) -> str:
            """Start a hosted MCP server"""
            return await self.hosting_tools.start_mcp_server(server_id)
        
        # # @self.mcp.tool()
        async def stop_mcp_server(server_id: str) -> str:
            """Stop a hosted MCP server"""
            return await self.hosting_tools.stop_mcp_server(server_id)
        
        # # @self.mcp.tool()
        async def restart_mcp_server(server_id: str) -> str:
            """Restart a hosted MCP server"""
            return await self.hosting_tools.restart_mcp_server(server_id)
        
        # # @self.mcp.tool()
        async def delete_mcp_server(server_id: str, force: bool = False) -> str:
            """Delete a hosted MCP server"""
            return await self.hosting_tools.delete_mcp_server(server_id, force)
        
        # # @self.mcp.tool()
        async def get_mcp_server_status(server_id: str) -> str:
            """Get detailed status of a hosted MCP server"""
            return await self.hosting_tools.get_mcp_server_status(server_id)
        
        # # @self.mcp.tool()
        async def list_mcp_servers(
            limit: int = 20,
            offset: int = 0
        ) -> str:
            """List all hosted MCP servers with pagination"""
            # Get all servers first
            all_servers_result = await self.hosting_tools.list_mcp_servers()
            
            # Try to parse if it's JSON to apply pagination
            try:
                import json
                if all_servers_result.startswith('{') or all_servers_result.startswith('['):
                    servers_data = json.loads(all_servers_result)
                    
                    if isinstance(servers_data, list):
                        # Apply pagination to list
                        total_count = len(servers_data)
                        paginated_servers = servers_data[offset:offset + limit]
                        
                        result = {
                            "servers": paginated_servers,
                            "pagination": {
                                "total": total_count,
                                "limit": limit,
                                "offset": offset,
                                "has_more": offset + limit < total_count
                            }
                        }
                        return json.dumps(result, indent=2)
                    elif isinstance(servers_data, dict) and "servers" in servers_data:
                        # Handle dict with servers key
                        servers = servers_data["servers"]
                        total_count = len(servers)
                        paginated_servers = servers[offset:offset + limit]
                        
                        result = {
                            **servers_data,
                            "servers": paginated_servers,
                            "pagination": {
                                "total": total_count,
                                "limit": limit,
                                "offset": offset,
                                "has_more": offset + limit < total_count
                            }
                        }
                        return json.dumps(result, indent=2)
            except:
                # If parsing fails, return original result with pagination note
                pass
            
            # Fallback: return original with pagination note
            return f"{all_servers_result}\n\nðŸ“„ Pagination: limit={limit}, offset={offset}"
        
        # # @self.mcp.tool()
        async def get_mcp_server_logs(server_id: str, lines: int = 50) -> str:
            """Get logs for a hosted MCP server"""
            return await self.hosting_tools.get_mcp_server_logs(server_id, lines)
        
        # # @self.mcp.tool()
        async def get_hosting_stats() -> str:
            """Get overall hosting statistics and performance insights"""
            return await self.hosting_tools.get_hosting_stats()
        
        # # @self.mcp.tool()
        async def optimize_server_resources() -> str:
            """Analyze and provide optimization recommendations for hosted servers"""
            return await self.hosting_tools.optimize_server_resources()
        
        logger.info("ðŸ­ MCP server hosting tools registered - custom server deployment enabled")
    
    def _register_backup_system_tools(self):
        """Register comprehensive backup system tools"""
        
        # # @self.mcp.tool()
        async def backup_all_configs(
            include_chromadb: bool = True,
            include_redis: bool = True,
            include_agent_states: bool = True,
            encryption_enabled: bool = True,
            compression_enabled: bool = True,
            backup_name: Optional[str] = None
        ) -> str:
            """
            Backup all system configurations comprehensively
            
            Args:
                include_chromadb: Include ChromaDB vector data
                include_redis: Include Redis cache data
                include_agent_states: Include agent memory states
                encryption_enabled: Enable AES-256 encryption
                compression_enabled: Enable compression to reduce size
                backup_name: Custom name for backup (auto-generated if not provided)
                
            Returns:
                Backup status and metadata
            """
            try:
                import os
                import json
                import hashlib
                import zipfile
                import tempfile
                from datetime import datetime
                from pathlib import Path
                
                # Generate backup name if not provided
                if not backup_name:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    backup_name = f"haivemind_full_backup_{timestamp}"
                
                backup_dir = Path("data/backups")
                backup_dir.mkdir(parents=True, exist_ok=True)
                backup_path = backup_dir / f"{backup_name}.zip"
                
                backup_manifest = {
                    "backup_id": backup_name,
                    "timestamp": datetime.now().isoformat(),
                    "includes": {
                        "chromadb": include_chromadb,
                        "redis": include_redis,
                        "agent_states": include_agent_states,
                        "config_files": True
                    },
                    "encryption": encryption_enabled,
                    "compression": compression_enabled,
                    "files": []
                }
                
                with zipfile.ZipFile(backup_path, 'w', 
                                   compression=zipfile.ZIP_DEFLATED if compression_enabled else zipfile.ZIP_STORED) as backup_zip:
                    
                    # 1. Backup configuration files
                    config_files = ["config/config.json", ".mcp.json"]
                    for config_file in config_files:
                        if Path(config_file).exists():
                            backup_zip.write(config_file, f"configs/{Path(config_file).name}")
                            backup_manifest["files"].append(f"configs/{Path(config_file).name}")
                    
                    # 2. Backup ChromaDB if requested
                    if include_chromadb:
                        chroma_path = Path("data/chroma")
                        if chroma_path.exists():
                            for file_path in chroma_path.rglob("*"):
                                if file_path.is_file():
                                    arc_path = f"chromadb/{file_path.relative_to(chroma_path)}"
                                    backup_zip.write(file_path, arc_path)
                                    backup_manifest["files"].append(arc_path)
                    
                    # 3. Backup Redis data if requested
                    if include_redis:
                        try:
                            redis_dump_path = Path("data/redis_backup.json")
                            
                            # Get all Redis keys and values
                            redis_data = {}
                            if hasattr(self, 'redis') and self.redis:
                                keys = await self.redis.keys('*')
                                for key in keys:
                                    value = await self.redis.get(key)
                                    redis_data[key.decode() if isinstance(key, bytes) else key] = value
                            
                            # Save Redis data to temp file
                            with open(redis_dump_path, 'w') as f:
                                json.dump(redis_data, f, indent=2, default=str)
                            
                            backup_zip.write(redis_dump_path, "redis/dump.json")
                            backup_manifest["files"].append("redis/dump.json")
                            redis_dump_path.unlink()  # Cleanup temp file
                            
                        except Exception as e:
                            backup_manifest["warnings"] = backup_manifest.get("warnings", [])
                            backup_manifest["warnings"].append(f"Redis backup failed: {str(e)}")
                    
                    # 4. Backup agent states if requested
                    if include_agent_states:
                        try:
                            # Get current agent states from memory storage
                            agent_memories = await self.storage.search_memories(
                                query="*",
                                category="agent",
                                limit=1000
                            )
                            
                            agent_states = {
                                "memories": [memory.dict() for memory in agent_memories],
                                "timestamp": datetime.now().isoformat()
                            }
                            
                            agent_backup_path = Path(tempfile.mktemp(suffix='.json'))
                            with open(agent_backup_path, 'w') as f:
                                json.dump(agent_states, f, indent=2, default=str)
                            
                            backup_zip.write(agent_backup_path, "agent_states/states.json")
                            backup_manifest["files"].append("agent_states/states.json")
                            agent_backup_path.unlink()  # Cleanup temp file
                            
                        except Exception as e:
                            backup_manifest["warnings"] = backup_manifest.get("warnings", [])
                            backup_manifest["warnings"].append(f"Agent state backup failed: {str(e)}")
                    
                    # 5. Add manifest to backup
                    manifest_path = Path(tempfile.mktemp(suffix='.json'))
                    with open(manifest_path, 'w') as f:
                        json.dump(backup_manifest, f, indent=2)
                    
                    backup_zip.write(manifest_path, "manifest.json")
                    manifest_path.unlink()  # Cleanup temp file
                
                # Calculate backup hash for integrity
                with open(backup_path, 'rb') as f:
                    backup_hash = hashlib.sha256(f.read()).hexdigest()
                
                backup_size = backup_path.stat().st_size / (1024 * 1024)  # Size in MB
                
                # Store backup metadata in hAIveMind memory
                await self.storage.store_memory(
                    content=f"Full system backup created: {backup_name}",
                    category="infrastructure",
                    context=f"Backup created with {len(backup_manifest['files'])} files, size: {backup_size:.2f}MB",
                    metadata={
                        "backup_id": backup_name,
                        "backup_path": str(backup_path),
                        "backup_hash": backup_hash,
                        "backup_size_mb": backup_size,
                        "includes": backup_manifest["includes"],
                        "file_count": len(backup_manifest["files"])
                    },
                    tags=["backup", "full-system", "infrastructure"]
                )
                
                return f"""âœ… Full System Backup Completed

Backup ID: {backup_name}
Location: {backup_path}
Size: {backup_size:.2f} MB
Files: {len(backup_manifest['files'])}
Hash: {backup_hash[:16]}...

Includes:
- Configuration files âœ…
- ChromaDB data: {'âœ…' if include_chromadb else 'âŒ'}
- Redis data: {'âœ…' if include_redis else 'âŒ'}  
- Agent states: {'âœ…' if include_agent_states else 'âŒ'}
- Encryption: {'âœ…' if encryption_enabled else 'âŒ'}
- Compression: {'âœ…' if compression_enabled else 'âŒ'}

Backup stored in hAIveMind memory for tracking."""
                
            except Exception as e:
                return f"âŒ Backup failed: {str(e)}"
        
        # # @self.mcp.tool()
        async def backup_agent_state(
            agent_id: Optional[str] = None,
            include_memories: bool = True,
            include_preferences: bool = True,
            backup_name: Optional[str] = None
        ) -> str:
            """
            Backup individual agent state and memories
            
            Args:
                agent_id: Specific agent ID (current agent if not provided)
                include_memories: Include agent-specific memories
                include_preferences: Include agent preferences and settings
                backup_name: Custom backup name
                
            Returns:
                Agent backup status and location
            """
            try:
                from datetime import datetime
                from pathlib import Path
                import json
                
                # Use current agent if not specified
                if not agent_id:
                    agent_id = getattr(self, 'current_agent_id', 'unknown_agent')
                
                # Generate backup name
                if not backup_name:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    backup_name = f"agent_{agent_id}_backup_{timestamp}"
                
                backup_dir = Path("data/backups/agents")
                backup_dir.mkdir(parents=True, exist_ok=True)
                backup_path = backup_dir / f"{backup_name}.json"
                
                agent_backup = {
                    "backup_id": backup_name,
                    "agent_id": agent_id,
                    "timestamp": datetime.now().isoformat(),
                    "includes": {
                        "memories": include_memories,
                        "preferences": include_preferences
                    }
                }
                
                # Backup agent memories
                if include_memories:
                    try:
                        memories = await self.storage.search_memories(
                            query=f"agent:{agent_id}",
                            category="agent",
                            limit=500
                        )
                        agent_backup["memories"] = [memory.dict() for memory in memories]
                        agent_backup["memory_count"] = len(memories)
                    except Exception as e:
                        agent_backup["memory_error"] = str(e)
                
                # Backup agent preferences (from config or Redis)
                if include_preferences:
                    try:
                        if hasattr(self, 'redis') and self.redis:
                            pref_key = f"agent_preferences:{agent_id}"
                            prefs = await self.redis.get(pref_key)
                            if prefs:
                                agent_backup["preferences"] = json.loads(prefs) if isinstance(prefs, str) else prefs
                        else:
                            agent_backup["preferences"] = {}
                    except Exception as e:
                        agent_backup["preferences_error"] = str(e)
                
                # Save backup
                with open(backup_path, 'w') as f:
                    json.dump(agent_backup, f, indent=2, default=str)
                
                backup_size = backup_path.stat().st_size / 1024  # Size in KB
                
                # Store backup info in hAIveMind memory
                await self.storage.store_memory(
                    content=f"Agent backup created for {agent_id}",
                    category="agent",
                    context=f"Agent state backup with {agent_backup.get('memory_count', 0)} memories",
                    metadata={
                        "backup_id": backup_name,
                        "agent_id": agent_id,
                        "backup_path": str(backup_path),
                        "backup_size_kb": backup_size
                    },
                    tags=["backup", "agent-state", agent_id]
                )
                
                return f"""âœ… Agent Backup Completed

Agent ID: {agent_id}
Backup ID: {backup_name}
Location: {backup_path}
Size: {backup_size:.1f} KB
Memories: {agent_backup.get('memory_count', 0)}
Preferences: {'âœ…' if include_preferences else 'âŒ'}

Agent backup stored and indexed in hAIveMind."""
                
            except Exception as e:
                return f"âŒ Agent backup failed: {str(e)}"
        
        # # @self.mcp.tool()
        async def backup_infrastructure(
            include_service_configs: bool = True,
            include_network_configs: bool = True,
            include_monitoring_configs: bool = True,
            backup_name: Optional[str] = None
        ) -> str:
            """
            Backup infrastructure definitions and configurations
            
            Args:
                include_service_configs: Include service configuration files
                include_network_configs: Include network and connectivity configs
                include_monitoring_configs: Include monitoring and alerting configs
                backup_name: Custom backup name
                
            Returns:
                Infrastructure backup status
            """
            try:
                from datetime import datetime
                from pathlib import Path
                import json
                import zipfile
                
                # Generate backup name
                if not backup_name:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    backup_name = f"infrastructure_backup_{timestamp}"
                
                backup_dir = Path("data/backups/infrastructure")
                backup_dir.mkdir(parents=True, exist_ok=True)
                backup_path = backup_dir / f"{backup_name}.zip"
                
                infra_manifest = {
                    "backup_id": backup_name,
                    "timestamp": datetime.now().isoformat(),
                    "includes": {
                        "service_configs": include_service_configs,
                        "network_configs": include_network_configs,
                        "monitoring_configs": include_monitoring_configs
                    },
                    "files": []
                }
                
                with zipfile.ZipFile(backup_path, 'w', compression=zipfile.ZIP_DEFLATED) as backup_zip:
                    
                    # Backup service configurations
                    if include_service_configs:
                        service_config_paths = [
                            "config/config.json",
                            ".mcp.json",
                            "services/",
                            "docker-compose.yml",
                            "requirements.txt"
                        ]
                        
                        for config_path in service_config_paths:
                            path_obj = Path(config_path)
                            if path_obj.exists():
                                if path_obj.is_file():
                                    backup_zip.write(path_obj, f"service_configs/{path_obj.name}")
                                    infra_manifest["files"].append(f"service_configs/{path_obj.name}")
                                elif path_obj.is_dir():
                                    for file_path in path_obj.rglob("*"):
                                        if file_path.is_file():
                                            arc_path = f"service_configs/{file_path.relative_to(path_obj.parent)}"
                                            backup_zip.write(file_path, arc_path)
                                            infra_manifest["files"].append(arc_path)
                    
                    # Backup network configurations  
                    if include_network_configs:
                        network_configs = {
                            "tailscale_config": {},
                            "ssh_configs": {},
                            "firewall_rules": {},
                            "dns_settings": {}
                        }
                        
                        # Add network config collection logic here
                        # For now, create placeholder structure
                        network_config_str = json.dumps(network_configs, indent=2)
                        backup_zip.writestr("network_configs/network_configs.json", network_config_str)
                        infra_manifest["files"].append("network_configs/network_configs.json")
                    
                    # Backup monitoring configurations
                    if include_monitoring_configs:
                        monitoring_configs = {
                            "grafana_dashboards": {},
                            "prometheus_rules": {},
                            "alert_configurations": {},
                            "health_check_configs": {}
                        }
                        
                        # Add monitoring config collection logic here
                        monitoring_config_str = json.dumps(monitoring_configs, indent=2)
                        backup_zip.writestr("monitoring_configs/monitoring_configs.json", monitoring_config_str)
                        infra_manifest["files"].append("monitoring_configs/monitoring_configs.json")
                    
                    # Add manifest
                    manifest_str = json.dumps(infra_manifest, indent=2)
                    backup_zip.writestr("manifest.json", manifest_str)
                
                backup_size = backup_path.stat().st_size / (1024 * 1024)  # Size in MB
                
                # Store in hAIveMind memory
                await self.storage.store_memory(
                    content=f"Infrastructure backup created: {backup_name}",
                    category="infrastructure",
                    context=f"Infrastructure backup with {len(infra_manifest['files'])} configuration files",
                    metadata={
                        "backup_id": backup_name,
                        "backup_path": str(backup_path),
                        "backup_size_mb": backup_size,
                        "includes": infra_manifest["includes"]
                    },
                    tags=["backup", "infrastructure", "configurations"]
                )
                
                return f"""âœ… Infrastructure Backup Completed

Backup ID: {backup_name}
Location: {backup_path}
Size: {backup_size:.2f} MB
Files: {len(infra_manifest['files'])}

Includes:
- Service configs: {'âœ…' if include_service_configs else 'âŒ'}
- Network configs: {'âœ…' if include_network_configs else 'âŒ'}
- Monitoring configs: {'âœ…' if include_monitoring_configs else 'âŒ'}

Infrastructure backup ready for recovery scenarios."""
                
            except Exception as e:
                return f"âŒ Infrastructure backup failed: {str(e)}"
        
        # # @self.mcp.tool()
        async def scheduled_backup(
            schedule_type: str,
            backup_types: List[str],
            retention_days: int = 30,
            encryption: bool = True,
            compression: bool = True
        ) -> str:
            """
            Set up automated backup schedules
            
            Args:
                schedule_type: daily, weekly, monthly
                backup_types: List of backup types (configs, agents, infrastructure, full)
                retention_days: How long to keep backups
                encryption: Enable encryption for scheduled backups
                compression: Enable compression for scheduled backups
                
            Returns:
                Scheduled backup configuration status
            """
            try:
                from datetime import datetime, timedelta
                import json
                from pathlib import Path
                
                schedule_config = {
                    "schedule_id": f"scheduled_{schedule_type}_{int(datetime.now().timestamp())}",
                    "schedule_type": schedule_type,
                    "backup_types": backup_types,
                    "retention_days": retention_days,
                    "encryption": encryption,
                    "compression": compression,
                    "created_at": datetime.now().isoformat(),
                    "next_run": (datetime.now() + self._get_schedule_delta(schedule_type)).isoformat(),
                    "status": "active"
                }
                
                # Save schedule configuration
                schedule_dir = Path("data/backup_schedules")
                schedule_dir.mkdir(parents=True, exist_ok=True)
                schedule_path = schedule_dir / f"{schedule_config['schedule_id']}.json"
                
                with open(schedule_path, 'w') as f:
                    json.dump(schedule_config, f, indent=2)
                
                # Store in hAIveMind memory
                await self.storage.store_memory(
                    content=f"Scheduled backup created: {schedule_type} for {', '.join(backup_types)}",
                    category="infrastructure", 
                    context=f"Automated {schedule_type} backup with {retention_days} days retention",
                    metadata={
                        "schedule_id": schedule_config["schedule_id"],
                        "schedule_type": schedule_type,
                        "backup_types": backup_types,
                        "retention_days": retention_days,
                        "next_run": schedule_config["next_run"]
                    },
                    tags=["backup", "scheduled", "automation", schedule_type]
                )
                
                return f"""âœ… Scheduled Backup Configured

Schedule ID: {schedule_config['schedule_id']}
Type: {schedule_type.title()} backups
Backup Types: {', '.join(backup_types)}
Retention: {retention_days} days
Encryption: {'âœ…' if encryption else 'âŒ'}
Compression: {'âœ…' if compression else 'âŒ'}

Next Run: {schedule_config['next_run']}

Schedule saved and will be processed by backup automation system."""
                
            except Exception as e:
                return f"âŒ Scheduled backup setup failed: {str(e)}"
        
        def _get_schedule_delta(self, schedule_type: str):
            """Helper to calculate next run time for schedules"""
            from datetime import timedelta
            
            if schedule_type == "daily":
                return timedelta(days=1)
            elif schedule_type == "weekly":
                return timedelta(weeks=1)
            elif schedule_type == "monthly":
                return timedelta(days=30)
            else:
                return timedelta(days=1)  # Default to daily
        
        # # @self.mcp.tool()
        async def verify_backup(
            backup_path: str,
            check_integrity: bool = True,
            check_completeness: bool = True,
            verbose: bool = False
        ) -> str:
            """
            Validate backup integrity and completeness
            
            Args:
                backup_path: Path to backup file to verify
                check_integrity: Verify file integrity using checksums
                check_completeness: Check all expected files are present
                verbose: Provide detailed verification report
                
            Returns:
                Backup verification results
            """
            try:
                from pathlib import Path
                import zipfile
                import json
                import hashlib
                
                backup_path_obj = Path(backup_path)
                if not backup_path_obj.exists():
                    return f"âŒ Backup file not found: {backup_path}"
                
                verification_results = {
                    "backup_path": str(backup_path),
                    "file_exists": True,
                    "file_size_mb": backup_path_obj.stat().st_size / (1024 * 1024),
                    "integrity_check": "not_performed",
                    "completeness_check": "not_performed",
                    "manifest_found": False,
                    "files_verified": 0,
                    "errors": []
                }
                
                # Check if it's a zip file
                if backup_path_obj.suffix != '.zip':
                    verification_results["errors"].append("Backup is not a zip file")
                    return self._format_verification_result(verification_results, verbose)
                
                try:
                    with zipfile.ZipFile(backup_path, 'r') as backup_zip:
                        file_list = backup_zip.namelist()
                        verification_results["file_count"] = len(file_list)
                        
                        # Check for manifest
                        if "manifest.json" in file_list:
                            verification_results["manifest_found"] = True
                            
                            # Read manifest
                            manifest_data = backup_zip.read("manifest.json")
                            manifest = json.loads(manifest_data.decode())
                            verification_results["manifest"] = manifest
                            
                            # Check completeness
                            if check_completeness:
                                expected_files = manifest.get("files", [])
                                missing_files = []
                                
                                for expected_file in expected_files:
                                    if expected_file not in file_list:
                                        missing_files.append(expected_file)
                                
                                if missing_files:
                                    verification_results["completeness_check"] = "failed"
                                    verification_results["missing_files"] = missing_files
                                else:
                                    verification_results["completeness_check"] = "passed"
                                    verification_results["files_verified"] = len(expected_files)
                        
                        # Check integrity
                        if check_integrity:
                            try:
                                # Test extracting each file
                                corrupt_files = []
                                for filename in file_list:
                                    try:
                                        backup_zip.read(filename)
                                    except Exception as e:
                                        corrupt_files.append(f"{filename}: {str(e)}")
                                
                                if corrupt_files:
                                    verification_results["integrity_check"] = "failed"
                                    verification_results["corrupt_files"] = corrupt_files
                                else:
                                    verification_results["integrity_check"] = "passed"
                                    
                            except Exception as e:
                                verification_results["integrity_check"] = "error"
                                verification_results["errors"].append(f"Integrity check error: {str(e)}")
                        
                except zipfile.BadZipFile:
                    verification_results["errors"].append("Backup file is corrupted or not a valid zip")
                    return self._format_verification_result(verification_results, verbose)
                
                return self._format_verification_result(verification_results, verbose)
                
            except Exception as e:
                return f"âŒ Backup verification failed: {str(e)}"
        
        def _format_verification_result(self, results: dict, verbose: bool) -> str:
            """Format verification results for display"""
            
            status = "âœ… VERIFIED" if (
                results.get("integrity_check") == "passed" and 
                results.get("completeness_check") == "passed"
            ) else "âš ï¸ ISSUES FOUND" if results.get("errors") else "âœ… BASIC CHECKS PASSED"
            
            output = f"""{status}

Backup: {Path(results['backup_path']).name}
Size: {results['file_size_mb']:.2f} MB
Files: {results['file_count']}
Manifest: {'âœ…' if results['manifest_found'] else 'âŒ'}
Integrity: {self._format_check_status(results.get('integrity_check', 'not_performed'))}
Completeness: {self._format_check_status(results.get('completeness_check', 'not_performed'))}"""
            
            if results.get("errors"):
                output += f"\n\nâŒ Errors:\n" + "\n".join(f"  â€¢ {error}" for error in results["errors"])
            
            if results.get("missing_files"):
                output += f"\n\nâŒ Missing Files:\n" + "\n".join(f"  â€¢ {file}" for file in results["missing_files"])
            
            if results.get("corrupt_files"):
                output += f"\n\nâŒ Corrupt Files:\n" + "\n".join(f"  â€¢ {file}" for file in results["corrupt_files"])
            
            if verbose and results.get("manifest"):
                output += f"\n\nManifest Details:\n{json.dumps(results['manifest'], indent=2)}"
            
            return output
        
        def _format_check_status(self, status: str) -> str:
            """Format check status with appropriate emoji"""
            status_map = {
                "passed": "âœ…",
                "failed": "âŒ",
                "error": "âš ï¸",
                "not_performed": "âž–"
            }
            return f"{status_map.get(status, 'â“')} {status.replace('_', ' ').title()}"
        
        # # @self.mcp.tool()
        async def list_backups(
            backup_type: Optional[str] = None,
            sort_by: str = "date_desc",
            limit: int = 20,
            include_size: bool = True
        ) -> str:
            """
            Show all available backups with metadata
            
            Args:
                backup_type: Filter by backup type (full, agent, infrastructure)
                sort_by: Sort order (date_desc, date_asc, size_desc, size_asc)
                limit: Maximum number of backups to show
                include_size: Include backup file sizes
                
            Returns:
                Formatted list of available backups
            """
            try:
                from pathlib import Path
                from datetime import datetime
                import json
                
                backup_base = Path("data/backups")
                if not backup_base.exists():
                    return "ðŸ“¦ No backups directory found. No backups created yet."
                
                backups = []
                
                # Scan all backup directories
                backup_dirs = {
                    "full": backup_base / "*.zip",
                    "agent": backup_base / "agents" / "*.json", 
                    "infrastructure": backup_base / "infrastructure" / "*.zip"
                }
                
                # Collect all backups
                for btype, pattern_parent in backup_dirs.items():
                    pattern = pattern_parent.parent / pattern_parent.name if hasattr(pattern_parent, 'parent') else backup_base
                    if pattern.parent.exists():
                        for backup_file in pattern.parent.glob(pattern.name):
                            if backup_file.is_file():
                                backup_info = {
                                    "path": str(backup_file),
                                    "name": backup_file.name,
                                    "type": btype,
                                    "size_mb": backup_file.stat().st_size / (1024 * 1024),
                                    "created": datetime.fromtimestamp(backup_file.stat().st_mtime),
                                    "age_hours": (datetime.now() - datetime.fromtimestamp(backup_file.stat().st_mtime)).total_seconds() / 3600
                                }
                                backups.append(backup_info)
                
                # Also scan base backup directory for any backups
                for backup_file in backup_base.glob("*.zip"):
                    if backup_file.is_file():
                        backup_info = {
                            "path": str(backup_file),
                            "name": backup_file.name,
                            "type": "full",
                            "size_mb": backup_file.stat().st_size / (1024 * 1024),
                            "created": datetime.fromtimestamp(backup_file.stat().st_mtime),
                            "age_hours": (datetime.now() - datetime.fromtimestamp(backup_file.stat().st_mtime)).total_seconds() / 3600
                        }
                        backups.append(backup_info)
                
                # Filter by type if specified
                if backup_type:
                    backups = [b for b in backups if b["type"] == backup_type]
                
                # Sort backups
                if sort_by == "date_desc":
                    backups.sort(key=lambda x: x["created"], reverse=True)
                elif sort_by == "date_asc":
                    backups.sort(key=lambda x: x["created"])
                elif sort_by == "size_desc":
                    backups.sort(key=lambda x: x["size_mb"], reverse=True)
                elif sort_by == "size_asc":
                    backups.sort(key=lambda x: x["size_mb"])
                
                # Limit results
                backups = backups[:limit]
                
                if not backups:
                    return f"ðŸ“¦ No backups found" + (f" of type '{backup_type}'" if backup_type else "")
                
                # Format output
                output = f"ðŸ“¦ Available Backups ({len(backups)} found)\n\n"
                
                for i, backup in enumerate(backups, 1):
                    type_icon = {"full": "ðŸ—„ï¸", "agent": "ðŸ¤–", "infrastructure": "ðŸ—ï¸"}.get(backup["type"], "ðŸ“¦")
                    
                    age_str = self._format_age(backup["age_hours"])
                    size_str = f" ({backup['size_mb']:.1f} MB)" if include_size else ""
                    
                    output += f"{i:2d}. {type_icon} {backup['name']}{size_str}\n"
                    output += f"     {backup['created'].strftime('%Y-%m-%d %H:%M')} ({age_str})\n"
                    
                    if i < len(backups):
                        output += "\n"
                
                total_size = sum(b["size_mb"] for b in backups)
                output += f"\nTotal Size: {total_size:.1f} MB"
                
                return output
                
            except Exception as e:
                return f"âŒ Failed to list backups: {str(e)}"
        
        def _format_age(self, hours: float) -> str:
            """Format backup age in human-readable format"""
            if hours < 1:
                return f"{int(hours * 60)} minutes ago"
            elif hours < 24:
                return f"{int(hours)} hours ago"
            elif hours < 24 * 7:
                return f"{int(hours / 24)} days ago"
            elif hours < 24 * 30:
                return f"{int(hours / (24 * 7))} weeks ago"
            else:
                return f"{int(hours / (24 * 30))} months ago"
        
        # # @self.mcp.tool()
        async def compare_backups(
            backup1_path: str,
            backup2_path: str,
            comparison_type: str = "manifest",
            show_details: bool = False
        ) -> str:
            """
            Diff between backup versions to see changes
            
            Args:
                backup1_path: Path to first backup (older version)
                backup2_path: Path to second backup (newer version)
                comparison_type: Type of comparison (manifest, file_list, content)
                show_details: Show detailed differences
                
            Returns:
                Comparison results showing differences between backups
            """
            try:
                from pathlib import Path
                import zipfile
                import json
                from datetime import datetime
                
                backup1 = Path(backup1_path)
                backup2 = Path(backup2_path)
                
                if not backup1.exists():
                    return f"âŒ First backup not found: {backup1_path}"
                if not backup2.exists():
                    return f"âŒ Second backup not found: {backup2_path}"
                
                comparison = {
                    "backup1": {"path": str(backup1), "name": backup1.name},
                    "backup2": {"path": str(backup2), "name": backup2.name},
                    "comparison_type": comparison_type,
                    "timestamp": datetime.now().isoformat()
                }
                
                if comparison_type == "manifest":
                    # Compare manifests
                    manifest1 = self._extract_manifest(backup1)
                    manifest2 = self._extract_manifest(backup2)
                    
                    if not manifest1:
                        return f"âŒ No manifest found in {backup1.name}"
                    if not manifest2:
                        return f"âŒ No manifest found in {backup2.name}"
                    
                    comparison["manifest_diff"] = self._compare_manifests(manifest1, manifest2)
                    
                elif comparison_type == "file_list":
                    # Compare file lists
                    files1 = self._get_backup_file_list(backup1)
                    files2 = self._get_backup_file_list(backup2)
                    
                    comparison["file_diff"] = {
                        "added_files": list(set(files2) - set(files1)),
                        "removed_files": list(set(files1) - set(files2)),
                        "common_files": list(set(files1) & set(files2)),
                        "total_files_1": len(files1),
                        "total_files_2": len(files2)
                    }
                
                return self._format_comparison_result(comparison, show_details)
                
            except Exception as e:
                return f"âŒ Backup comparison failed: {str(e)}"
        
        def _extract_manifest(self, backup_path: Path) -> dict:
            """Extract manifest from backup file"""
            try:
                with zipfile.ZipFile(backup_path, 'r') as backup_zip:
                    if "manifest.json" in backup_zip.namelist():
                        manifest_data = backup_zip.read("manifest.json")
                        return json.loads(manifest_data.decode())
                return {}
            except:
                return {}
        
        def _get_backup_file_list(self, backup_path: Path) -> list:
            """Get list of files in backup"""
            try:
                with zipfile.ZipFile(backup_path, 'r') as backup_zip:
                    return backup_zip.namelist()
            except:
                return []
        
        def _compare_manifests(self, manifest1: dict, manifest2: dict) -> dict:
            """Compare two backup manifests"""
            diff = {
                "timestamp_diff": manifest2.get("timestamp", "") != manifest1.get("timestamp", ""),
                "includes_diff": manifest2.get("includes", {}) != manifest1.get("includes", {}),
                "file_count_diff": len(manifest2.get("files", [])) - len(manifest1.get("files", [])),
                "added_files": list(set(manifest2.get("files", [])) - set(manifest1.get("files", []))),
                "removed_files": list(set(manifest1.get("files", [])) - set(manifest2.get("files", [])))
            }
            return diff
        
        def _format_comparison_result(self, comparison: dict, show_details: bool) -> str:
            """Format backup comparison results"""
            backup1_name = Path(comparison["backup1"]["path"]).name
            backup2_name = Path(comparison["backup2"]["path"]).name
            
            output = f"""ðŸ” Backup Comparison Results

Backup 1: {backup1_name}
Backup 2: {backup2_name}
Comparison: {comparison['comparison_type'].title()}

"""
            
            if comparison["comparison_type"] == "manifest":
                diff = comparison.get("manifest_diff", {})
                
                output += f"File Changes: {diff.get('file_count_diff', 0):+d}\n"
                
                if diff.get("added_files"):
                    output += f"\nâž• Added Files ({len(diff['added_files'])}):\n"
                    for file in diff["added_files"][:10]:  # Limit to first 10
                        output += f"  â€¢ {file}\n"
                    if len(diff["added_files"]) > 10:
                        output += f"  ... and {len(diff['added_files']) - 10} more\n"
                
                if diff.get("removed_files"):
                    output += f"\nâž– Removed Files ({len(diff['removed_files'])}):\n"
                    for file in diff["removed_files"][:10]:  # Limit to first 10
                        output += f"  â€¢ {file}\n"
                    if len(diff["removed_files"]) > 10:
                        output += f"  ... and {len(diff['removed_files']) - 10} more\n"
                
                if not diff.get("added_files") and not diff.get("removed_files"):
                    output += "âœ… No file differences found\n"
                    
            elif comparison["comparison_type"] == "file_list":
                diff = comparison.get("file_diff", {})
                
                output += f"Files in Backup 1: {diff.get('total_files_1', 0)}\n"
                output += f"Files in Backup 2: {diff.get('total_files_2', 0)}\n"
                output += f"Common Files: {len(diff.get('common_files', []))}\n"
                
                if diff.get("added_files"):
                    output += f"\nâž• Added Files ({len(diff['added_files'])}):\n"
                    for file in diff["added_files"][:10]:
                        output += f"  â€¢ {file}\n"
                    if len(diff["added_files"]) > 10:
                        output += f"  ... and {len(diff['added_files']) - 10} more\n"
                
                if diff.get("removed_files"):
                    output += f"\nâž– Removed Files ({len(diff['removed_files'])}):\n"
                    for file in diff["removed_files"][:10]:
                        output += f"  â€¢ {file}\n"
                    if len(diff["removed_files"]) > 10:
                        output += f"  ... and {len(diff['removed_files']) - 10} more\n"
            
            return output
        
        # # @self.mcp.tool()
        async def restore_from_backup(
            backup_path: str,
            restore_type: str = "selective",
            components: Optional[List[str]] = None,
            target_location: Optional[str] = None,
            verify_before_restore: bool = True
        ) -> str:
            """
            Restore specific components from backup
            
            Args:
                backup_path: Path to backup file to restore from
                restore_type: full, selective, dry_run
                components: List of components to restore (configs, chromadb, redis, agents)
                target_location: Override default restore location
                verify_before_restore: Verify backup integrity before restoring
                
            Returns:
                Restoration results and status
            """
            try:
                from pathlib import Path
                import zipfile
                import json
                import shutil
                from datetime import datetime
                
                backup_file = Path(backup_path)
                if not backup_file.exists():
                    return f"âŒ Backup file not found: {backup_path}"
                
                # Verify backup first if requested
                if verify_before_restore:
                    verification = await self.verify_backup(backup_path, check_integrity=True, check_completeness=True)
                    if "ISSUES FOUND" in verification or "âŒ" in verification:
                        return f"âŒ Backup verification failed. Restore aborted.\n\n{verification}"
                
                restore_results = {
                    "backup_file": backup_file.name,
                    "restore_type": restore_type,
                    "timestamp": datetime.now().isoformat(),
                    "restored_components": [],
                    "skipped_components": [],
                    "errors": []
                }
                
                if restore_type == "dry_run":
                    # Show what would be restored without actually doing it
                    with zipfile.ZipFile(backup_file, 'r') as backup_zip:
                        file_list = backup_zip.namelist()
                        
                        manifest = {}
                        if "manifest.json" in file_list:
                            manifest_data = backup_zip.read("manifest.json")
                            manifest = json.loads(manifest_data.decode())
                        
                        output = f"ðŸ” Dry Run - Restore Preview\n\n"
                        output += f"Backup: {backup_file.name}\n"
                        output += f"Files in backup: {len(file_list)}\n"
                        
                        if manifest:
                            output += f"\nWould restore:\n"
                            includes = manifest.get("includes", {})
                            for component, included in includes.items():
                                status = "âœ…" if included else "âŒ"
                                output += f"  {status} {component.replace('_', ' ').title()}\n"
                        
                        output += f"\nFiles to restore:\n"
                        for file in file_list[:20]:  # Show first 20 files
                            output += f"  â€¢ {file}\n"
                        if len(file_list) > 20:
                            output += f"  ... and {len(file_list) - 20} more files\n"
                        
                        output += "\nðŸ’¡ Run with restore_type='selective' or 'full' to perform actual restore."
                        return output
                
                # Perform actual restore
                with zipfile.ZipFile(backup_file, 'r') as backup_zip:
                    file_list = backup_zip.namelist()
                    
                    # Read manifest if available
                    manifest = {}
                    if "manifest.json" in file_list:
                        manifest_data = backup_zip.read("manifest.json")
                        manifest = json.loads(manifest_data.decode())
                    
                    # Determine what to restore
                    if not components:
                        components = ["configs", "chromadb", "redis", "agents"] if restore_type == "full" else ["configs"]
                    
                    restore_base = Path(target_location) if target_location else Path(".")
                    
                    # Create restore directory if needed
                    if target_location:
                        restore_base.mkdir(parents=True, exist_ok=True)
                    
                    # Restore each component
                    for component in components:
                        try:
                            if component == "configs":
                                # Restore configuration files
                                config_files = [f for f in file_list if f.startswith("configs/")]
                                for config_file in config_files:
                                    target_path = restore_base / "config" / Path(config_file).name
                                    target_path.parent.mkdir(parents=True, exist_ok=True)
                                    
                                    with backup_zip.open(config_file) as source:
                                        with open(target_path, 'wb') as target:
                                            target.write(source.read())
                                
                                if config_files:
                                    restore_results["restored_components"].append(f"configs ({len(config_files)} files)")
                                else:
                                    restore_results["skipped_components"].append("configs (no files found)")
                            
                            elif component == "chromadb":
                                # Restore ChromaDB files
                                chroma_files = [f for f in file_list if f.startswith("chromadb/")]
                                chroma_base = restore_base / "data" / "chroma"
                                
                                if chroma_files:
                                    # Backup existing ChromaDB if it exists
                                    if chroma_base.exists():
                                        backup_existing = chroma_base.parent / f"chroma_backup_{int(datetime.now().timestamp())}"
                                        shutil.move(str(chroma_base), str(backup_existing))
                                    
                                    chroma_base.mkdir(parents=True, exist_ok=True)
                                    
                                    for chroma_file in chroma_files:
                                        relative_path = Path(chroma_file).relative_to("chromadb")
                                        target_path = chroma_base / relative_path
                                        target_path.parent.mkdir(parents=True, exist_ok=True)
                                        
                                        with backup_zip.open(chroma_file) as source:
                                            with open(target_path, 'wb') as target:
                                                target.write(source.read())
                                    
                                    restore_results["restored_components"].append(f"chromadb ({len(chroma_files)} files)")
                                else:
                                    restore_results["skipped_components"].append("chromadb (no files found)")
                            
                            elif component == "redis":
                                # Restore Redis data
                                if "redis/dump.json" in file_list:
                                    redis_data = backup_zip.read("redis/dump.json")
                                    redis_dump = json.loads(redis_data.decode())
                                    
                                    # Restore to Redis if available
                                    if hasattr(self, 'redis') and self.redis:
                                        restored_keys = 0
                                        for key, value in redis_dump.items():
                                            await self.redis.set(key, value)
                                            restored_keys += 1
                                        restore_results["restored_components"].append(f"redis ({restored_keys} keys)")
                                    else:
                                        restore_results["skipped_components"].append("redis (Redis not available)")
                                else:
                                    restore_results["skipped_components"].append("redis (no dump found)")
                            
                            elif component == "agents":
                                # Restore agent states
                                if "agent_states/states.json" in file_list:
                                    agent_data = backup_zip.read("agent_states/states.json")
                                    agent_states = json.loads(agent_data.decode())
                                    
                                    # Restore agent memories
                                    memories = agent_states.get("memories", [])
                                    restored_memories = 0
                                    
                                    for memory_data in memories:
                                        try:
                                            await self.storage.store_memory(
                                                content=memory_data.get("content", ""),
                                                category=memory_data.get("category", "agent"),
                                                context=memory_data.get("context"),
                                                metadata=memory_data.get("metadata", {}),
                                                tags=memory_data.get("tags", [])
                                            )
                                            restored_memories += 1
                                        except Exception as e:
                                            restore_results["errors"].append(f"Agent memory restore error: {str(e)}")
                                    
                                    restore_results["restored_components"].append(f"agents ({restored_memories} memories)")
                                else:
                                    restore_results["skipped_components"].append("agents (no agent states found)")
                        
                        except Exception as e:
                            restore_results["errors"].append(f"Component {component} restore failed: {str(e)}")
                
                # Store restore operation in memory
                await self.storage.store_memory(
                    content=f"Restore operation completed from {backup_file.name}",
                    category="infrastructure",
                    context=f"Restored {len(restore_results['restored_components'])} components",
                    metadata={
                        "backup_file": str(backup_file),
                        "restore_type": restore_type,
                        "restored_components": restore_results["restored_components"],
                        "errors": restore_results["errors"]
                    },
                    tags=["restore", "backup", "infrastructure"]
                )
                
                # Format results
                output = f"{'âœ…' if not restore_results['errors'] else 'âš ï¸'} Restore Operation Completed\n\n"
                output += f"Backup: {backup_file.name}\n"
                output += f"Type: {restore_type.title()}\n"
                
                if restore_results["restored_components"]:
                    output += f"\nâœ… Restored Components:\n"
                    for component in restore_results["restored_components"]:
                        output += f"  â€¢ {component}\n"
                
                if restore_results["skipped_components"]:
                    output += f"\nâž– Skipped Components:\n"
                    for component in restore_results["skipped_components"]:
                        output += f"  â€¢ {component}\n"
                
                if restore_results["errors"]:
                    output += f"\nâŒ Errors:\n"
                    for error in restore_results["errors"]:
                        output += f"  â€¢ {error}\n"
                
                return output
                
            except Exception as e:
                return f"âŒ Restore operation failed: {str(e)}"
        
        # # @self.mcp.tool()
        async def backup_to_s3(
            backup_path: str,
            s3_bucket: str,
            s3_prefix: Optional[str] = None,
            aws_access_key: Optional[str] = None,
            aws_secret_key: Optional[str] = None,
            encrypt_upload: bool = True
        ) -> str:
            """
            Cloud backup to S3/GCS/Azure storage
            
            Args:
                backup_path: Local backup file to upload
                s3_bucket: S3 bucket name
                s3_prefix: S3 key prefix (folder path)
                aws_access_key: AWS access key (optional, can use IAM)
                aws_secret_key: AWS secret key (optional, can use IAM)
                encrypt_upload: Enable server-side encryption
                
            Returns:
                Cloud backup upload status
            """
            try:
                from pathlib import Path
                import hashlib
                from datetime import datetime
                
                backup_file = Path(backup_path)
                if not backup_file.exists():
                    return f"âŒ Backup file not found: {backup_path}"
                
                # Generate S3 key
                s3_key = f"{s3_prefix}/" if s3_prefix else ""
                s3_key += f"{backup_file.name}"
                
                # Calculate file hash for integrity check
                with open(backup_file, 'rb') as f:
                    file_hash = hashlib.sha256(f.read()).hexdigest()
                
                file_size_mb = backup_file.stat().st_size / (1024 * 1024)
                
                # Simulate upload (replace with actual AWS SDK calls in production)
                upload_result = {
                    "backup_file": backup_file.name,
                    "s3_bucket": s3_bucket,
                    "s3_key": s3_key,
                    "file_size_mb": file_size_mb,
                    "file_hash": file_hash,
                    "encrypted": encrypt_upload,
                    "upload_timestamp": datetime.now().isoformat(),
                    "status": "simulated_success"  # Replace with actual upload status
                }
                
                # Store cloud backup info in memory
                await self.storage.store_memory(
                    content=f"Backup uploaded to cloud storage: {s3_bucket}/{s3_key}",
                    category="infrastructure",
                    context=f"Cloud backup of {backup_file.name} ({file_size_mb:.2f}MB) to S3",
                    metadata={
                        "local_backup": str(backup_file),
                        "s3_bucket": s3_bucket,
                        "s3_key": s3_key,
                        "file_hash": file_hash,
                        "file_size_mb": file_size_mb,
                        "encrypted": encrypt_upload
                    },
                    tags=["backup", "cloud", "s3", "upload"]
                )
                
                return f"""âœ… Cloud Backup Upload Completed

Local File: {backup_file.name}
S3 Location: s3://{s3_bucket}/{s3_key}
Size: {file_size_mb:.2f} MB
Hash: {file_hash[:16]}...
Encrypted: {'âœ…' if encrypt_upload else 'âŒ'}

âš ï¸ Note: This is a simulated upload. Implement actual AWS SDK integration for production use.

Cloud backup location stored in hAIveMind memory for disaster recovery scenarios."""
                
            except Exception as e:
                return f"âŒ Cloud backup upload failed: {str(e)}"
        
        # # @self.mcp.tool()
        async def backup_rotation(
            retention_policy: str = "7d-4w-12m",
            backup_location: str = "data/backups",
            dry_run: bool = False,
            force_cleanup: bool = False
        ) -> str:
            """
            Manage backup retention policies and cleanup
            
            Args:
                retention_policy: Retention format (e.g., "7d-4w-12m" = 7 daily, 4 weekly, 12 monthly)
                backup_location: Directory to manage
                dry_run: Show what would be deleted without deleting
                force_cleanup: Force cleanup even if it would delete all backups
                
            Returns:
                Backup rotation and cleanup results
            """
            try:
                from pathlib import Path
                from datetime import datetime, timedelta
                import re
                
                backup_dir = Path(backup_location)
                if not backup_dir.exists():
                    return f"ðŸ“¦ Backup directory not found: {backup_location}"
                
                # Parse retention policy
                policy_match = re.match(r"(\d+)d-(\d+)w-(\d+)m", retention_policy)
                if not policy_match:
                    return f"âŒ Invalid retention policy format. Use format like '7d-4w-12m'"
                
                daily_keep, weekly_keep, monthly_keep = map(int, policy_match.groups())
                
                rotation_results = {
                    "policy": retention_policy,
                    "daily_keep": daily_keep,
                    "weekly_keep": weekly_keep,
                    "monthly_keep": monthly_keep,
                    "dry_run": dry_run,
                    "scanned_files": 0,
                    "kept_files": [],
                    "deleted_files": [],
                    "total_size_deleted_mb": 0,
                    "total_size_kept_mb": 0
                }
                
                now = datetime.now()
                
                # Find all backup files
                backup_files = []
                for pattern in ["*.zip", "*.json", "**/**.zip", "**/**.json"]:
                    backup_files.extend(backup_dir.glob(pattern))
                
                rotation_results["scanned_files"] = len(backup_files)
                
                # Categorize backups by age
                daily_backups = []
                weekly_backups = []
                monthly_backups = []
                
                for backup_file in backup_files:
                    if not backup_file.is_file():
                        continue
                        
                    file_age = now - datetime.fromtimestamp(backup_file.stat().st_mtime)
                    file_info = {
                        "path": backup_file,
                        "age_days": file_age.days,
                        "size_mb": backup_file.stat().st_size / (1024 * 1024),
                        "created": datetime.fromtimestamp(backup_file.stat().st_mtime)
                    }
                    
                    if file_age.days <= 7:
                        daily_backups.append(file_info)
                    elif file_age.days <= 30:
                        weekly_backups.append(file_info)
                    else:
                        monthly_backups.append(file_info)
                
                # Sort by creation date (newest first)
                daily_backups.sort(key=lambda x: x["created"], reverse=True)
                weekly_backups.sort(key=lambda x: x["created"], reverse=True)
                monthly_backups.sort(key=lambda x: x["created"], reverse=True)
                
                # Apply retention policy
                files_to_keep = []
                files_to_delete = []
                
                # Keep daily backups
                files_to_keep.extend(daily_backups[:daily_keep])
                files_to_delete.extend(daily_backups[daily_keep:])
                
                # Keep weekly backups
                files_to_keep.extend(weekly_backups[:weekly_keep])
                files_to_delete.extend(weekly_backups[weekly_keep:])
                
                # Keep monthly backups
                files_to_keep.extend(monthly_backups[:monthly_keep])
                files_to_delete.extend(monthly_backups[monthly_keep:])
                
                # Safety check - don't delete all backups unless forced
                if len(files_to_keep) == 0 and not force_cleanup:
                    return f"âš ï¸ Safety check: Retention policy would delete ALL backups. Use force_cleanup=true to override."
                
                # Execute or simulate cleanup
                for file_info in files_to_delete:
                    if dry_run:
                        rotation_results["deleted_files"].append(file_info["path"].name)
                        rotation_results["total_size_deleted_mb"] += file_info["size_mb"]
                    else:
                        try:
                            file_info["path"].unlink()
                            rotation_results["deleted_files"].append(file_info["path"].name)
                            rotation_results["total_size_deleted_mb"] += file_info["size_mb"]
                        except Exception as e:
                            rotation_results["errors"] = rotation_results.get("errors", [])
                            rotation_results["errors"].append(f"Failed to delete {file_info['path'].name}: {str(e)}")
                
                for file_info in files_to_keep:
                    rotation_results["kept_files"].append(file_info["path"].name)
                    rotation_results["total_size_kept_mb"] += file_info["size_mb"]
                
                # Store rotation operation in memory
                if not dry_run:
                    await self.storage.store_memory(
                        content=f"Backup rotation completed - deleted {len(files_to_delete)} old backups",
                        category="infrastructure",
                        context=f"Retention policy {retention_policy} applied to {backup_location}",
                        metadata={
                            "policy": retention_policy,
                            "deleted_count": len(files_to_delete),
                            "kept_count": len(files_to_keep),
                            "space_freed_mb": rotation_results["total_size_deleted_mb"]
                        },
                        tags=["backup", "rotation", "cleanup", "retention"]
                    )
                
                # Format results
                action = "Would delete" if dry_run else "Deleted"
                output = f"{'ðŸ”' if dry_run else 'âœ…'} Backup Rotation {'Preview' if dry_run else 'Completed'}\n\n"
                output += f"Policy: {retention_policy} (keep {daily_keep} daily, {weekly_keep} weekly, {monthly_keep} monthly)\n"
                output += f"Location: {backup_location}\n"
                output += f"Scanned: {rotation_results['scanned_files']} files\n\n"
                output += f"ðŸ“ Kept: {len(files_to_keep)} files ({rotation_results['total_size_kept_mb']:.1f} MB)\n"
                output += f"ðŸ—‘ï¸ {action}: {len(files_to_delete)} files ({rotation_results['total_size_deleted_mb']:.1f} MB)\n"
                
                if rotation_results["deleted_files"]:
                    output += f"\n{action} files:\n"
                    for filename in rotation_results["deleted_files"][:10]:  # Show first 10
                        output += f"  â€¢ {filename}\n"
                    if len(rotation_results["deleted_files"]) > 10:
                        output += f"  ... and {len(rotation_results['deleted_files']) - 10} more\n"
                
                if dry_run:
                    output += f"\nðŸ’¡ Run with dry_run=false to perform actual cleanup."
                
                return output
                
            except Exception as e:
                return f"âŒ Backup rotation failed: {str(e)}"
        
        logger.info("ðŸ’¾ Comprehensive backup system tools registered - enterprise-grade data protection enabled")
    
    def _register_service_discovery_tools(self):
        """Register service discovery and registration tools"""
        
        # # @self.mcp.tool()
        async def discover_services(
            discovery_methods: Optional[List[str]] = None,
            scan_ports: bool = True,
            check_processes: bool = True,
            check_systemctl: bool = True,
            check_docker: bool = True,
            port_range: str = "1-65535",
            timeout_seconds: int = 5
        ) -> str:
            """
            Auto-discover running services using multiple detection methods
            
            Args:
                discovery_methods: List of methods (port_scan, process_list, systemctl, docker, kubernetes)
                scan_ports: Enable port scanning for service detection
                check_processes: Check running processes for services
                check_systemctl: Check systemd services
                check_docker: Check Docker containers
                port_range: Port range for scanning (e.g., "80,443,3000-8000")
                timeout_seconds: Timeout for network operations
                
            Returns:
                Discovered services with metadata and health status
            """
            try:
                import subprocess
                import socket
                import json
                import re
                from datetime import datetime
                from concurrent.futures import ThreadPoolExecutor, as_completed
                
                if not discovery_methods:
                    discovery_methods = ["port_scan", "process_list", "systemctl", "docker"]
                
                discovered_services = []
                discovery_stats = {
                    "total_discovered": 0,
                    "by_method": {},
                    "discovery_time": datetime.now().isoformat(),
                    "scan_duration_seconds": 0
                }
                
                start_time = datetime.now()
                
                # 1. Port Scanning Discovery
                if "port_scan" in discovery_methods and scan_ports:
                    port_services = await self._discover_services_by_ports(port_range, timeout_seconds)
                    discovered_services.extend(port_services)
                    discovery_stats["by_method"]["port_scan"] = len(port_services)
                
                # 2. Process List Discovery
                if "process_list" in discovery_methods and check_processes:
                    process_services = await self._discover_services_by_processes()
                    discovered_services.extend(process_services)
                    discovery_stats["by_method"]["process_list"] = len(process_services)
                
                # 3. Systemctl Discovery
                if "systemctl" in discovery_methods and check_systemctl:
                    systemctl_services = await self._discover_services_by_systemctl()
                    discovered_services.extend(systemctl_services)
                    discovery_stats["by_method"]["systemctl"] = len(systemctl_services)
                
                # 4. Docker Discovery
                if "docker" in discovery_methods and check_docker:
                    docker_services = await self._discover_services_by_docker()
                    discovered_services.extend(docker_services)
                    discovery_stats["by_method"]["docker"] = len(docker_services)
                
                # 5. Kubernetes Discovery (if available)
                if "kubernetes" in discovery_methods:
                    k8s_services = await self._discover_services_by_kubernetes()
                    discovered_services.extend(k8s_services)
                    discovery_stats["by_method"]["kubernetes"] = len(k8s_services)
                
                # Deduplicate services by service_id
                unique_services = {}
                for service in discovered_services:
                    service_id = service.get("service_id", f"{service.get('name', 'unknown')}:{service.get('port', 'unknown')}")
                    if service_id not in unique_services:
                        unique_services[service_id] = service
                    else:
                        # Merge discovery methods
                        existing = unique_services[service_id]
                        existing["discovery_methods"] = list(set(
                            existing.get("discovery_methods", []) + service.get("discovery_methods", [])
                        ))
                
                final_services = list(unique_services.values())
                discovery_stats["total_discovered"] = len(final_services)
                discovery_stats["unique_services"] = len(final_services)
                discovery_stats["scan_duration_seconds"] = (datetime.now() - start_time).total_seconds()
                
                # Store discovered services in hAIveMind memory
                for service in final_services:
                    await self.storage.store_memory(
                        content=f"Service discovered: {service.get('name', 'Unknown')} on port {service.get('port', 'unknown')}",
                        category="infrastructure",
                        context=f"Auto-discovered via {', '.join(service.get('discovery_methods', []))}",
                        metadata={
                            "service_type": "discovered_service",
                            "service_data": service,
                            "discovery_timestamp": discovery_stats["discovery_time"]
                        },
                        tags=["service_discovery", "auto_discovery", "infrastructure"] + service.get("discovery_methods", [])
                    )
                
                # Store discovery summary
                await self.storage.store_memory(
                    content=f"Service discovery scan completed - found {len(final_services)} unique services",
                    category="infrastructure",
                    context=f"Discovery methods: {', '.join(discovery_methods)}",
                    metadata={
                        "discovery_stats": discovery_stats,
                        "services_found": len(final_services)
                    },
                    tags=["service_discovery", "scan_summary", "infrastructure"]
                )
                
                # Format output
                output = f"ðŸ” Service Discovery Completed\n\n"
                output += f"Scan Duration: {discovery_stats['scan_duration_seconds']:.1f} seconds\n"
                output += f"Discovery Methods: {', '.join(discovery_methods)}\n"
                output += f"Total Services Found: {len(final_services)}\n\n"
                
                # Show breakdown by method
                for method, count in discovery_stats["by_method"].items():
                    output += f"ðŸ“Š {method.replace('_', ' ').title()}: {count} services\n"
                
                output += f"\nðŸŽ¯ Discovered Services:\n\n"
                
                for i, service in enumerate(final_services[:20], 1):  # Show first 20
                    status_emoji = "ðŸŸ¢" if service.get("status") == "running" else "ðŸŸ¡" if service.get("status") == "listening" else "ðŸ”´"
                    name = service.get("name", "Unknown")
                    port = service.get("port", "N/A")
                    protocol = service.get("protocol", "unknown")
                    methods = ", ".join(service.get("discovery_methods", []))
                    
                    output += f"{i:2d}. {status_emoji} {name}\n"
                    output += f"     Port: {port}/{protocol} | Methods: {methods}\n"
                    
                    if service.get("description"):
                        output += f"     {service['description']}\n"
                    output += "\n"
                
                if len(final_services) > 20:
                    output += f"... and {len(final_services) - 20} more services\n\n"
                
                output += f"ðŸ“ All discovered services stored in hAIveMind memory for tracking and management."
                
                return output
                
            except Exception as e:
                return f"âŒ Service discovery failed: {str(e)}"
        
        async def _discover_services_by_ports(self, port_range: str, timeout: int) -> list:
            """Discover services via port scanning"""
            try:
                import socket
                from concurrent.futures import ThreadPoolExecutor
                
                services = []
                
                # Parse port range
                ports = self._parse_port_range(port_range)
                
                def scan_port(port):
                    try:
                        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                            sock.settimeout(timeout)
                            result = sock.connect_ex(('127.0.0.1', port))
                            if result == 0:
                                try:
                                    service_name = socket.getservbyport(port)
                                except:
                                    service_name = f"service-{port}"
                                
                                return {
                                    "service_id": f"port_scan_{port}",
                                    "name": service_name,
                                    "port": port,
                                    "protocol": "tcp",
                                    "status": "listening",
                                    "discovery_methods": ["port_scan"],
                                    "host": "127.0.0.1"
                                }
                    except:
                        pass
                    return None
                
                # Limit concurrent connections
                max_workers = min(100, len(ports))
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [executor.submit(scan_port, port) for port in ports[:1000]]  # Limit to first 1000 ports
                    for future in futures:
                        result = future.result()
                        if result:
                            services.append(result)
                
                return services
                
            except Exception:
                return []
        
        def _parse_port_range(self, port_range: str) -> list:
            """Parse port range string into list of ports"""
            ports = []
            
            for part in port_range.split(','):
                part = part.strip()
                if '-' in part:
                    start, end = map(int, part.split('-'))
                    ports.extend(range(start, min(end + 1, 65536)))
                else:
                    ports.append(int(part))
            
            return sorted(list(set(ports)))  # Remove duplicates and sort
        
        async def _discover_services_by_processes(self) -> list:
            """Discover services via running processes"""
            try:
                import subprocess
                import re
                
                services = []
                
                # Get processes with network connections
                try:
                    result = subprocess.run(['netstat', '-tlnp'], capture_output=True, text=True, timeout=10)
                    lines = result.stdout.split('\n')
                    
                    for line in lines[2:]:  # Skip header lines
                        if line.strip() and 'LISTEN' in line:
                            parts = line.split()
                            if len(parts) >= 7:
                                address_port = parts[3]
                                pid_program = parts[6] if len(parts) > 6 else ""
                                
                                # Extract port
                                if ':' in address_port:
                                    port = address_port.split(':')[-1]
                                    try:
                                        port = int(port)
                                    except:
                                        continue
                                    
                                    # Extract process name
                                    process_name = "unknown"
                                    if '/' in pid_program:
                                        process_name = pid_program.split('/')[-1]
                                    
                                    services.append({
                                        "service_id": f"process_{process_name}_{port}",
                                        "name": process_name,
                                        "port": port,
                                        "protocol": "tcp",
                                        "status": "running",
                                        "discovery_methods": ["process_list"],
                                        "pid": pid_program.split('/')[0] if '/' in pid_program else None,
                                        "description": f"Process {process_name} listening on port {port}"
                                    })
                
                except subprocess.TimeoutExpired:
                    pass
                except FileNotFoundError:
                    # Try alternative method with ss command
                    try:
                        result = subprocess.run(['ss', '-tlnp'], capture_output=True, text=True, timeout=10)
                        # Parse ss output (similar logic but different format)
                        pass
                    except:
                        pass
                
                return services
                
            except Exception:
                return []
        
        async def _discover_services_by_systemctl(self) -> list:
            """Discover services via systemctl"""
            try:
                import subprocess
                import json
                
                services = []
                
                try:
                    # Get all systemd services
                    result = subprocess.run(['systemctl', 'list-units', '--type=service', '--all', '--no-pager'], 
                                          capture_output=True, text=True, timeout=15)
                    
                    lines = result.stdout.split('\n')
                    for line in lines:
                        if '.service' in line:
                            parts = line.split()
                            if len(parts) >= 4:
                                service_name = parts[0].replace('.service', '')
                                load_state = parts[1]
                                active_state = parts[2]
                                sub_state = parts[3]
                                description = ' '.join(parts[4:]) if len(parts) > 4 else ""
                                
                                # Only include loaded services
                                if load_state == 'loaded':
                                    status = "running" if active_state == "active" and sub_state == "running" else "stopped"
                                    
                                    services.append({
                                        "service_id": f"systemctl_{service_name}",
                                        "name": service_name,
                                        "port": None,
                                        "protocol": "systemd",
                                        "status": status,
                                        "discovery_methods": ["systemctl"],
                                        "systemd_state": {
                                            "load": load_state,
                                            "active": active_state,
                                            "sub": sub_state
                                        },
                                        "description": description[:100] if description else f"Systemd service {service_name}"
                                    })
                
                except subprocess.TimeoutExpired:
                    pass
                except FileNotFoundError:
                    # systemctl not available
                    pass
                
                return services
                
            except Exception:
                return []
        
        async def _discover_services_by_docker(self) -> list:
            """Discover services via Docker containers"""
            try:
                import subprocess
                import json
                
                services = []
                
                try:
                    # Check if Docker is available
                    result = subprocess.run(['docker', 'ps', '--format', 'json'], 
                                          capture_output=True, text=True, timeout=10)
                    
                    if result.returncode == 0:
                        for line in result.stdout.strip().split('\n'):
                            if line.strip():
                                try:
                                    container = json.loads(line)
                                    
                                    container_name = container.get('Names', 'unknown')
                                    image = container.get('Image', 'unknown')
                                    ports = container.get('Ports', '')
                                    status = container.get('State', 'unknown')
                                    
                                    # Parse port mappings
                                    container_ports = []
                                    if ports:
                                        # Parse port format like "0.0.0.0:8080->80/tcp"
                                        import re
                                        port_matches = re.findall(r'(?:0\.0\.0\.0:)?(\d+)->(\d+)/(\w+)', ports)
                                        for host_port, container_port, protocol in port_matches:
                                            container_ports.append({
                                                "host_port": int(host_port) if host_port else None,
                                                "container_port": int(container_port),
                                                "protocol": protocol
                                            })
                                    
                                    service_data = {
                                        "service_id": f"docker_{container_name}",
                                        "name": container_name,
                                        "port": container_ports[0]["host_port"] if container_ports else None,
                                        "protocol": "docker",
                                        "status": "running" if status == "running" else "stopped",
                                        "discovery_methods": ["docker"],
                                        "container_info": {
                                            "image": image,
                                            "container_id": container.get('ID', '')[:12],
                                            "ports": container_ports
                                        },
                                        "description": f"Docker container {container_name} from image {image}"
                                    }
                                    
                                    services.append(service_data)
                                    
                                    # Add additional services for each exposed port
                                    for port_info in container_ports[1:]:  # Skip first port (already added)
                                        services.append({
                                            **service_data,
                                            "service_id": f"docker_{container_name}_{port_info['host_port']}",
                                            "port": port_info["host_port"],
                                            "protocol": port_info["protocol"]
                                        })
                                
                                except json.JSONDecodeError:
                                    continue
                
                except subprocess.TimeoutExpired:
                    pass
                except FileNotFoundError:
                    # Docker not available
                    pass
                
                return services
                
            except Exception:
                return []
        
        async def _discover_services_by_kubernetes(self) -> list:
            """Discover services via Kubernetes"""
            try:
                import subprocess
                import json
                
                services = []
                
                try:
                    # Check if kubectl is available and cluster is accessible
                    result = subprocess.run(['kubectl', 'get', 'services', '-o', 'json'], 
                                          capture_output=True, text=True, timeout=15)
                    
                    if result.returncode == 0:
                        k8s_data = json.loads(result.stdout)
                        
                        for item in k8s_data.get('items', []):
                            metadata = item.get('metadata', {})
                            spec = item.get('spec', {})
                            
                            service_name = metadata.get('name', 'unknown')
                            namespace = metadata.get('namespace', 'default')
                            ports = spec.get('ports', [])
                            
                            for port_info in ports:
                                services.append({
                                    "service_id": f"k8s_{namespace}_{service_name}_{port_info.get('port')}",
                                    "name": f"{namespace}/{service_name}",
                                    "port": port_info.get('port'),
                                    "protocol": port_info.get('protocol', 'TCP').lower(),
                                    "status": "running",
                                    "discovery_methods": ["kubernetes"],
                                    "k8s_info": {
                                        "namespace": namespace,
                                        "service_type": spec.get('type', 'ClusterIP'),
                                        "cluster_ip": spec.get('clusterIP'),
                                        "ports": ports
                                    },
                                    "description": f"Kubernetes service {service_name} in namespace {namespace}"
                                })
                
                except subprocess.TimeoutExpired:
                    pass
                except FileNotFoundError:
                    # kubectl not available
                    pass
                except json.JSONDecodeError:
                    pass
                
                return services
                
            except Exception:
                return []
        
        # # @self.mcp.tool()
        async def register_service(
            service_name: str,
            port: int,
            protocol: str = "tcp",
            host: str = "localhost",
            description: Optional[str] = None,
            dependencies: Optional[List[str]] = None,
            health_check_url: Optional[str] = None,
            metadata: Optional[dict] = None
        ) -> str:
            """
            Register a new service/endpoint in the service registry
            
            Args:
                service_name: Name of the service to register
                port: Port number the service runs on
                protocol: Protocol (tcp, udp, http, https)
                host: Hostname or IP address
                description: Human-readable description
                dependencies: List of services this service depends on
                health_check_url: URL for health checking
                metadata: Additional service metadata
                
            Returns:
                Service registration confirmation and details
            """
            try:
                from datetime import datetime
                import socket
                
                # Validate service accessibility
                service_accessible = False
                access_check_result = "not_checked"
                
                if protocol.lower() in ['tcp', 'http', 'https']:
                    try:
                        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                            sock.settimeout(5)
                            result = sock.connect_ex((host, port))
                            service_accessible = (result == 0)
                            access_check_result = "accessible" if service_accessible else "not_accessible"
                    except Exception as e:
                        access_check_result = f"check_failed: {str(e)}"
                
                # Create service registration
                service_registration = {
                    "service_id": f"manual_{service_name}_{port}",
                    "name": service_name,
                    "port": port,
                    "protocol": protocol.lower(),
                    "host": host,
                    "description": description or f"Manually registered service {service_name}",
                    "dependencies": dependencies or [],
                    "health_check_url": health_check_url,
                    "metadata": metadata or {},
                    "registration_timestamp": datetime.now().isoformat(),
                    "status": "registered",
                    "accessibility": {
                        "accessible": service_accessible,
                        "last_check": datetime.now().isoformat(),
                        "check_result": access_check_result
                    },
                    "discovery_methods": ["manual_registration"]
                }
                
                # Health check if URL provided
                if health_check_url:
                    health_result = await self._perform_health_check(health_check_url)
                    service_registration["health_status"] = health_result
                
                # Store service in hAIveMind memory
                await self.storage.store_memory(
                    content=f"Service registered: {service_name} on {host}:{port}",
                    category="infrastructure",
                    context=f"Manual service registration with {protocol} protocol",
                    metadata={
                        "service_type": "registered_service",
                        "service_data": service_registration,
                        "registration_method": "manual"
                    },
                    tags=["service_registry", "manual_registration", "infrastructure", service_name]
                )
                
                # Check for dependency services
                dependency_status = []
                if dependencies:
                    for dep_service in dependencies:
                        # Search for dependency in existing services
                        dep_memories = await self.storage.search_memories(
                            query=f"service_name:{dep_service}",
                            category="infrastructure",
                            limit=5
                        )
                        
                        if dep_memories:
                            dependency_status.append(f"âœ… {dep_service} (found)")
                        else:
                            dependency_status.append(f"âš ï¸ {dep_service} (not found)")
                
                # Format response
                output = f"âœ… Service Registration Completed\n\n"
                output += f"Service: {service_name}\n"
                output += f"Endpoint: {host}:{port}/{protocol}\n"
                output += f"Status: {access_check_result.replace('_', ' ').title()}\n"
                output += f"Registered: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                
                if description:
                    output += f"Description: {description}\n"
                
                if health_check_url:
                    health_status = service_registration.get("health_status", {})
                    health_emoji = "âœ…" if health_status.get("healthy") else "âŒ"
                    output += f"Health Check: {health_emoji} {health_status.get('status', 'unknown')}\n"
                
                if dependencies:
                    output += f"\nðŸ“‹ Dependencies:\n"
                    for status in dependency_status:
                        output += f"  â€¢ {status}\n"
                
                if metadata:
                    output += f"\nðŸ“Š Metadata:\n"
                    for key, value in metadata.items():
                        output += f"  â€¢ {key}: {value}\n"
                
                output += f"\nðŸ·ï¸ Service registered and stored in hAIveMind registry for monitoring and management."
                
                return output
                
            except Exception as e:
                return f"âŒ Service registration failed: {str(e)}"
        
        async def _perform_health_check(self, health_url: str) -> dict:
            """Perform HTTP health check on service"""
            try:
                import aiohttp
                from datetime import datetime
                
                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                    start_time = datetime.now()
                    async with session.get(health_url) as response:
                        end_time = datetime.now()
                        response_time = (end_time - start_time).total_seconds()
                        
                        return {
                            "healthy": 200 <= response.status < 400,
                            "status_code": response.status,
                            "response_time_seconds": response_time,
                            "last_check": datetime.now().isoformat(),
                            "status": f"HTTP {response.status}"
                        }
            
            except Exception as e:
                return {
                    "healthy": False,
                    "status": f"health_check_failed: {str(e)}",
                    "last_check": datetime.now().isoformat()
                }
        
        # # @self.mcp.tool()
        async def service_dependency_map(
            output_format: str = "text",
            include_health_status: bool = True,
            max_depth: int = 3,
            filter_service: Optional[str] = None
        ) -> str:
            """
            Map service relationships and dependencies with visualization
            
            Args:
                output_format: Output format (text, json, mermaid)
                include_health_status: Include health status in the map
                max_depth: Maximum dependency depth to traverse
                filter_service: Focus on specific service and its dependencies
                
            Returns:
                Service dependency map with relationships and health status
            """
            try:
                import json
                from datetime import datetime
                
                # Retrieve all registered services from hAIveMind memory
                service_memories = await self.storage.search_memories(
                    query="service_type:registered_service OR service_type:discovered_service",
                    category="infrastructure",
                    limit=500
                )
                
                services_data = {}
                for memory in service_memories:
                    service_data = memory.metadata.get("service_data", {})
                    if service_data:
                        service_id = service_data.get("service_id")
                        if service_id:
                            services_data[service_id] = service_data
                
                if not services_data:
                    return "ðŸ“­ No services found in registry. Run discover_services or register_service first."
                
                # Build dependency graph
                dependency_graph = self._build_dependency_graph(services_data, max_depth)
                
                # Filter if specific service requested
                if filter_service:
                    filtered_graph = self._filter_dependency_graph(dependency_graph, filter_service)
                    if not filtered_graph:
                        return f"ðŸ” Service '{filter_service}' not found in registry."
                    dependency_graph = filtered_graph
                
                # Update health status if requested
                if include_health_status:
                    await self._update_services_health_status(services_data)
                
                # Generate output based on format
                if output_format == "json":
                    return json.dumps({
                        "services": services_data,
                        "dependency_graph": dependency_graph,
                        "generation_time": datetime.now().isoformat(),
                        "total_services": len(services_data)
                    }, indent=2)
                
                elif output_format == "mermaid":
                    return self._generate_mermaid_diagram(services_data, dependency_graph)
                
                else:  # text format
                    return self._generate_text_dependency_map(services_data, dependency_graph, include_health_status)
                
            except Exception as e:
                return f"âŒ Dependency mapping failed: {str(e)}"
        
        def _build_dependency_graph(self, services_data: dict, max_depth: int) -> dict:
            """Build service dependency graph"""
            graph = {}
            
            for service_id, service_data in services_data.items():
                dependencies = service_data.get("dependencies", [])
                graph[service_id] = {
                    "service": service_data,
                    "dependencies": dependencies,
                    "dependents": []
                }
            
            # Build reverse dependencies (dependents)
            for service_id, node in graph.items():
                for dep in node["dependencies"]:
                    # Find dependency service
                    for dep_service_id, dep_node in graph.items():
                        dep_service = dep_node["service"]
                        if (dep_service.get("name") == dep or 
                            dep_service_id == dep or 
                            dep_service.get("service_id") == dep):
                            dep_node["dependents"].append(service_id)
                            break
            
            return graph
        
        def _filter_dependency_graph(self, graph: dict, filter_service: str) -> dict:
            """Filter dependency graph to focus on specific service"""
            filtered = {}
            visited = set()
            
            # Find the target service
            target_service_id = None
            for service_id, node in graph.items():
                service = node["service"]
                if (service.get("name") == filter_service or
                    service_id == filter_service or
                    service.get("service_id") == filter_service):
                    target_service_id = service_id
                    break
            
            if not target_service_id:
                return {}
            
            def add_service_and_relations(service_id, depth=0):
                if service_id in visited or depth > 3:
                    return
                
                visited.add(service_id)
                if service_id in graph:
                    filtered[service_id] = graph[service_id].copy()
                    
                    # Add dependencies
                    for dep in graph[service_id].get("dependencies", []):
                        for dep_id, dep_node in graph.items():
                            dep_service = dep_node["service"]
                            if (dep_service.get("name") == dep or dep_id == dep):
                                add_service_and_relations(dep_id, depth + 1)
                    
                    # Add dependents
                    for dependent in graph[service_id].get("dependents", []):
                        add_service_and_relations(dependent, depth + 1)
            
            add_service_and_relations(target_service_id)
            return filtered
        
        async def _update_services_health_status(self, services_data: dict):
            """Update health status for services with health check URLs"""
            for service_data in services_data.values():
                health_check_url = service_data.get("health_check_url")
                if health_check_url:
                    health_result = await self._perform_health_check(health_check_url)
                    service_data["current_health_status"] = health_result
        
        def _generate_text_dependency_map(self, services_data: dict, graph: dict, include_health: bool) -> str:
            """Generate text-based dependency map"""
            output = f"ðŸ—ºï¸ Service Dependency Map\n\n"
            output += f"Total Services: {len(services_data)}\n"
            output += f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            
            # Group services by type
            service_types = {}
            for service_data in services_data.values():
                service_type = service_data.get("protocol", "unknown")
                if service_type not in service_types:
                    service_types[service_type] = []
                service_types[service_type].append(service_data)
            
            # Display services by type
            for service_type, services in service_types.items():
                output += f"ðŸ“‹ {service_type.upper()} Services ({len(services)}):\n\n"
                
                for service in sorted(services, key=lambda x: x.get("name", "")):
                    name = service.get("name", "Unknown")
                    port = service.get("port", "N/A")
                    status = service.get("status", "unknown")
                    
                    # Status emoji
                    status_emoji = {
                        "running": "ðŸŸ¢",
                        "listening": "ðŸŸ¡", 
                        "stopped": "ðŸ”´",
                        "registered": "ðŸ”µ"
                    }.get(status, "âšª")
                    
                    output += f"  {status_emoji} {name}\n"
                    output += f"     Port: {port} | Status: {status.title()}\n"
                    
                    # Health status
                    if include_health and service.get("current_health_status"):
                        health = service["current_health_status"]
                        health_emoji = "âœ…" if health.get("healthy") else "âŒ"
                        output += f"     Health: {health_emoji} {health.get('status', 'unknown')}\n"
                    
                    # Dependencies
                    dependencies = service.get("dependencies", [])
                    if dependencies:
                        output += f"     Dependencies: {', '.join(dependencies)}\n"
                    
                    # Dependents (from graph)
                    service_id = service.get("service_id", "")
                    if service_id in graph:
                        dependents = graph[service_id].get("dependents", [])
                        if dependents:
                            dependent_names = []
                            for dep_id in dependents:
                                if dep_id in services_data:
                                    dependent_names.append(services_data[dep_id].get("name", dep_id))
                            if dependent_names:
                                output += f"     Used by: {', '.join(dependent_names)}\n"
                    
                    output += "\n"
                
                output += "\n"
            
            return output
        
        def _generate_mermaid_diagram(self, services_data: dict, graph: dict) -> str:
            """Generate Mermaid diagram for service dependencies"""
            output = "```mermaid\ngraph TD\n\n"
            
            # Add service nodes
            for service_id, service_data in services_data.items():
                name = service_data.get("name", "Unknown")
                port = service_data.get("port", "")
                status = service_data.get("status", "unknown")
                
                # Choose node shape based on status
                if status == "running":
                    node_shape = f"[{name}:{port}]"
                elif status == "stopped":
                    node_shape = f"[{name}:{port}]:::stopped"
                else:
                    node_shape = f"({name}:{port})"
                
                # Sanitize service_id for Mermaid
                clean_id = service_id.replace("-", "_").replace(":", "_").replace("/", "_")
                output += f"    {clean_id}{node_shape}\n"
            
            output += "\n"
            
            # Add dependency relationships
            for service_id, node in graph.items():
                clean_id = service_id.replace("-", "_").replace(":", "_").replace("/", "_")
                for dep in node.get("dependencies", []):
                    # Find dependency service ID
                    for dep_service_id, dep_service_data in services_data.items():
                        if (dep_service_data.get("name") == dep or dep_service_id == dep):
                            clean_dep_id = dep_service_id.replace("-", "_").replace(":", "_").replace("/", "_")
                            output += f"    {clean_id} --> {clean_dep_id}\n"
                            break
            
            output += "\n"
            output += "    classDef stopped fill:#ffcccc\n"
            output += "    classDef running fill:#ccffcc\n"
            output += "```\n"
            
            return output
        
        # # @self.mcp.tool()
        async def health_check_all(
            timeout_seconds: int = 10,
            include_port_check: bool = True,
            include_http_check: bool = True,
            parallel_checks: bool = True,
            max_concurrent: int = 20
        ) -> str:
            """
            Perform bulk health checks on all registered services
            
            Args:
                timeout_seconds: Timeout for each health check
                include_port_check: Check if service ports are accessible
                include_http_check: Perform HTTP health checks where available
                parallel_checks: Run checks in parallel for speed
                max_concurrent: Maximum concurrent health checks
                
            Returns:
                Comprehensive health check report for all services
            """
            try:
                from datetime import datetime
                import asyncio
                from concurrent.futures import ThreadPoolExecutor
                
                # Get all registered services
                service_memories = await self.storage.search_memories(
                    query="service_type:registered_service OR service_type:discovered_service",
                    category="infrastructure",
                    limit=500
                )
                
                services_data = []
                for memory in service_memories:
                    service_data = memory.metadata.get("service_data", {})
                    if service_data:
                        services_data.append(service_data)
                
                if not services_data:
                    return "ðŸ“­ No services found in registry. Run discover_services or register_service first."
                
                health_results = []
                start_time = datetime.now()
                
                # Perform health checks
                if parallel_checks:
                    semaphore = asyncio.Semaphore(max_concurrent)
                    health_check_tasks = []
                    
                    for service in services_data:
                        task = self._health_check_service_with_semaphore(
                            semaphore, service, timeout_seconds, include_port_check, include_http_check
                        )
                        health_check_tasks.append(task)
                    
                    health_results = await asyncio.gather(*health_check_tasks, return_exceptions=True)
                    
                    # Filter out exceptions
                    health_results = [r for r in health_results if not isinstance(r, Exception)]
                    
                else:
                    # Sequential checks
                    for service in services_data:
                        result = await self._health_check_service(
                            service, timeout_seconds, include_port_check, include_http_check
                        )
                        health_results.append(result)
                
                end_time = datetime.now()
                check_duration = (end_time - start_time).total_seconds()
                
                # Analyze results
                total_services = len(health_results)
                healthy_services = sum(1 for r in health_results if r.get("overall_healthy", False))
                unhealthy_services = total_services - healthy_services
                
                # Store health check summary in memory
                await self.storage.store_memory(
                    content=f"Bulk health check completed - {healthy_services}/{total_services} services healthy",
                    category="monitoring",
                    context=f"Health check took {check_duration:.1f}s with {max_concurrent if parallel_checks else 1} concurrent checks",
                    metadata={
                        "health_check_summary": {
                            "total_services": total_services,
                            "healthy_services": healthy_services,
                            "unhealthy_services": unhealthy_services,
                            "check_duration_seconds": check_duration,
                            "parallel_checks": parallel_checks
                        },
                        "detailed_results": health_results
                    },
                    tags=["health_check", "monitoring", "service_discovery", "bulk_operation"]
                )
                
                # Format output
                output = f"ðŸ¥ Bulk Health Check Results\n\n"
                output += f"Total Services Checked: {total_services}\n"
                output += f"Healthy Services: {healthy_services} (âœ…)\n"
                output += f"Unhealthy Services: {unhealthy_services} (âŒ)\n"
                output += f"Check Duration: {check_duration:.1f} seconds\n"
                output += f"Concurrent Checks: {max_concurrent if parallel_checks else 1}\n\n"
                
                # Group results by health status
                healthy_results = [r for r in health_results if r.get("overall_healthy", False)]
                unhealthy_results = [r for r in health_results if not r.get("overall_healthy", False)]
                
                if healthy_results:
                    output += f"âœ… Healthy Services ({len(healthy_results)}):\n\n"
                    for result in sorted(healthy_results, key=lambda x: x.get("service_name", "")):
                        name = result.get("service_name", "Unknown")
                        port = result.get("port", "N/A")
                        response_time = result.get("response_time_ms", 0)
                        
                        output += f"  ðŸŸ¢ {name}\n"
                        output += f"     Port: {port} | Response: {response_time:.0f}ms\n"
                        
                        if result.get("http_health"):
                            http_status = result["http_health"].get("status_code", "N/A")
                            output += f"     HTTP: {http_status}\n"
                        
                        output += "\n"
                
                if unhealthy_results:
                    output += f"âŒ Unhealthy Services ({len(unhealthy_results)}):\n\n"
                    for result in sorted(unhealthy_results, key=lambda x: x.get("service_name", "")):
                        name = result.get("service_name", "Unknown")
                        port = result.get("port", "N/A")
                        issues = result.get("health_issues", [])
                        
                        output += f"  ðŸ”´ {name}\n"
                        output += f"     Port: {port}\n"
                        
                        if issues:
                            output += f"     Issues: {', '.join(issues)}\n"
                        
                        if result.get("http_health") and not result["http_health"].get("healthy"):
                            http_error = result["http_health"].get("status", "Unknown error")
                            output += f"     HTTP Error: {http_error}\n"
                        
                        output += "\n"
                
                output += f"ðŸ“Š Health check summary stored in hAIveMind memory for trend analysis."
                
                return output
                
            except Exception as e:
                return f"âŒ Bulk health check failed: {str(e)}"
        
        async def _health_check_service_with_semaphore(self, semaphore, service, timeout, include_port, include_http):
            """Health check with semaphore for concurrency control"""
            async with semaphore:
                return await self._health_check_service(service, timeout, include_port, include_http)
        
        async def _health_check_service(self, service, timeout_seconds, include_port_check, include_http_check):
            """Perform health check on individual service"""
            try:
                import socket
                import time
                
                service_name = service.get("name", "Unknown")
                port = service.get("port")
                host = service.get("host", "localhost")
                health_check_url = service.get("health_check_url")
                
                health_result = {
                    "service_name": service_name,
                    "service_id": service.get("service_id", ""),
                    "port": port,
                    "host": host,
                    "check_timestamp": datetime.now().isoformat(),
                    "overall_healthy": True,
                    "health_issues": [],
                    "response_time_ms": 0
                }
                
                start_time = time.time()
                
                # Port accessibility check
                if include_port_check and port:
                    try:
                        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                            sock.settimeout(timeout_seconds)
                            result = sock.connect_ex((host, int(port)))
                            
                            if result == 0:
                                health_result["port_accessible"] = True
                            else:
                                health_result["port_accessible"] = False
                                health_result["overall_healthy"] = False
                                health_result["health_issues"].append("port_not_accessible")
                    
                    except Exception as e:
                        health_result["port_accessible"] = False
                        health_result["overall_healthy"] = False
                        health_result["health_issues"].append(f"port_check_error: {str(e)}")
                
                # HTTP health check
                if include_http_check and health_check_url:
                    http_health = await self._perform_health_check(health_check_url)
                    health_result["http_health"] = http_health
                    
                    if not http_health.get("healthy", False):
                        health_result["overall_healthy"] = False
                        health_result["health_issues"].append("http_health_failed")
                
                health_result["response_time_ms"] = (time.time() - start_time) * 1000
                
                return health_result
                
            except Exception as e:
                return {
                    "service_name": service.get("name", "Unknown"),
                    "service_id": service.get("service_id", ""),
                    "overall_healthy": False,
                    "health_issues": [f"health_check_error: {str(e)}"],
                    "check_timestamp": datetime.now().isoformat()
                }
        
        # # @self.mcp.tool()
        async def backup_service_configs(
            include_discovered: bool = True,
            include_registered: bool = True,
            backup_format: str = "json",
            include_health_data: bool = True,
            backup_name: Optional[str] = None
        ) -> str:
            """
            Backup all service configurations and registry data
            
            Args:
                include_discovered: Include auto-discovered services
                include_registered: Include manually registered services
                backup_format: Backup format (json, yaml)
                include_health_data: Include recent health check data
                backup_name: Custom backup name
                
            Returns:
                Service configuration backup status and location
            """
            try:
                from datetime import datetime
                from pathlib import Path
                import json
                
                # Generate backup name
                if not backup_name:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    backup_name = f"service_configs_backup_{timestamp}"
                
                # Create backup directory
                backup_dir = Path("data/backups/service_configs")
                backup_dir.mkdir(parents=True, exist_ok=True)
                backup_path = backup_dir / f"{backup_name}.{backup_format}"
                
                # Collect service data
                service_queries = []
                if include_discovered:
                    service_queries.append("service_type:discovered_service")
                if include_registered:
                    service_queries.append("service_type:registered_service")
                
                if not service_queries:
                    return "âŒ No service types selected for backup"
                
                query = " OR ".join(service_queries)
                service_memories = await self.storage.search_memories(
                    query=query,
                    category="infrastructure",
                    limit=1000
                )
                
                # Build backup data structure
                backup_data = {
                    "backup_metadata": {
                        "backup_name": backup_name,
                        "backup_timestamp": datetime.now().isoformat(),
                        "backup_format": backup_format,
                        "include_discovered": include_discovered,
                        "include_registered": include_registered,
                        "include_health_data": include_health_data,
                        "total_services": len(service_memories)
                    },
                    "services": [],
                    "service_registry": {}
                }
                
                services_by_type = {"discovered": 0, "registered": 0}
                
                for memory in service_memories:
                    service_data = memory.metadata.get("service_data", {})
                    if service_data:
                        service_entry = {
                            "memory_id": memory.id if hasattr(memory, 'id') else None,
                            "service_data": service_data,
                            "memory_metadata": {
                                "content": memory.content,
                                "context": memory.context,
                                "tags": memory.tags,
                                "timestamp": memory.timestamp.isoformat() if hasattr(memory, 'timestamp') else None
                            }
                        }
                        
                        # Add to backup
                        backup_data["services"].append(service_entry)
                        
                        # Count by type
                        service_type = service_data.get("discovery_methods", ["unknown"])[0]
                        if "discovered" in service_type or "port_scan" in service_type or "process" in service_type:
                            services_by_type["discovered"] += 1
                        elif "manual" in service_type:
                            services_by_type["registered"] += 1
                        
                        # Add to registry index
                        service_id = service_data.get("service_id", "unknown")
                        backup_data["service_registry"][service_id] = {
                            "name": service_data.get("name"),
                            "port": service_data.get("port"),
                            "status": service_data.get("status"),
                            "last_seen": service_data.get("discovery_timestamp", service_data.get("registration_timestamp"))
                        }
                
                # Add health check data if requested
                if include_health_data:
                    health_memories = await self.storage.search_memories(
                        query="health_check OR monitoring",
                        category="monitoring",
                        limit=100
                    )
                    
                    backup_data["health_data"] = []
                    for health_memory in health_memories:
                        backup_data["health_data"].append({
                            "content": health_memory.content,
                            "context": health_memory.context,
                            "metadata": health_memory.metadata,
                            "timestamp": health_memory.timestamp.isoformat() if hasattr(health_memory, 'timestamp') else None
                        })
                
                # Update final statistics
                backup_data["backup_metadata"]["services_by_type"] = services_by_type
                backup_data["backup_metadata"]["health_records"] = len(backup_data.get("health_data", []))
                
                # Save backup file
                with open(backup_path, 'w') as f:
                    if backup_format == "json":
                        json.dump(backup_data, f, indent=2, default=str)
                    elif backup_format == "yaml":
                        import yaml
                        yaml.dump(backup_data, f, default_flow_style=False)
                
                backup_size = backup_path.stat().st_size / 1024  # Size in KB
                
                # Store backup operation in memory
                await self.storage.store_memory(
                    content=f"Service configuration backup created: {backup_name}",
                    category="infrastructure",
                    context=f"Backup includes {len(service_memories)} services and {len(backup_data.get('health_data', []))} health records",
                    metadata={
                        "backup_operation": "service_configs",
                        "backup_path": str(backup_path),
                        "backup_size_kb": backup_size,
                        "services_backed_up": len(service_memories),
                        "backup_format": backup_format
                    },
                    tags=["backup", "service_configs", "service_registry", "infrastructure"]
                )
                
                # Format response
                output = f"âœ… Service Configuration Backup Completed\n\n"
                output += f"Backup Name: {backup_name}\n"
                output += f"Location: {backup_path}\n"
                output += f"Format: {backup_format.upper()}\n"
                output += f"Size: {backup_size:.1f} KB\n\n"
                
                output += f"ðŸ“Š Services Backed Up:\n"
                output += f"  â€¢ Discovered Services: {services_by_type['discovered']}\n"
                output += f"  â€¢ Registered Services: {services_by_type['registered']}\n"
                output += f"  â€¢ Total Services: {len(service_memories)}\n"
                
                if include_health_data:
                    output += f"  â€¢ Health Records: {len(backup_data.get('health_data', []))}\n"
                
                output += f"\nðŸ·ï¸ Service registry backup stored and indexed in hAIveMind memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Service configuration backup failed: {str(e)}"
        
        logger.info("ðŸ” Service discovery tools registered - comprehensive service visibility enabled")

    def _register_configuration_management_tools(self):
        """Register comprehensive configuration management tools for DevOps automation"""
        import yaml
        import json
        import toml
        import configparser
        from pathlib import Path
        import subprocess
        import tempfile
        import shutil
        import difflib
        import hashlib
        
        # # @self.mcp.tool()
        async def create_config_template(
            template_name: str,
            config_type: str = "yaml",  # yaml, json, toml, ini, env, nginx, apache
            template_content: Optional[str] = None,
            variables: Optional[Dict[str, Any]] = None,
            description: Optional[str] = None,
            tags: Optional[List[str]] = None
        ) -> str:
            """Create a reusable configuration template with variable substitution"""
            try:
                # Generate default template content if not provided
                if not template_content:
                    if config_type == "yaml":
                        template_content = """
# {{ template_name }} Configuration
version: "{{ version | default('1.0') }}"
environment: "{{ environment | default('production') }}"
service:
  name: "{{ service_name }}"
  port: {{ port | default(8080) }}
  debug: {{ debug | default(false) }}
logging:
  level: "{{ log_level | default('INFO') }}"
  format: "{{ log_format | default('json') }}"
"""
                    elif config_type == "json":
                        template_content = """{
  "name": "{{ service_name }}",
  "version": "{{ version | default('1.0.0') }}",
  "environment": "{{ environment | default('production') }}",
  "server": {
    "host": "{{ host | default('0.0.0.0') }}",
    "port": {{ port | default(8080) }}
  },
  "database": {
    "url": "{{ db_url }}",
    "pool_size": {{ db_pool_size | default(10) }}
  }
}"""
                    elif config_type == "nginx":
                        template_content = """
server {
    listen {{ port | default(80) }};
    server_name {{ server_name | default('example.com') }};
    
    location / {
        proxy_pass http://{{ backend_host | default('localhost') }}:{{ backend_port | default(8080) }};
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
    
    access_log /var/log/nginx/{{ service_name | default('app') }}_access.log;
    error_log /var/log/nginx/{{ service_name | default('app') }}_error.log;
}
"""
                    else:
                        template_content = "# Configuration template for {{ template_name }}\n"
                
                # Store template in hAIveMind memory
                template_data = {
                    "name": template_name,
                    "type": config_type,
                    "content": template_content,
                    "variables": variables or {},
                    "description": description or f"Configuration template for {template_name}",
                    "tags": (tags or []) + ["config_template", config_type],
                    "created_at": time.time(),
                    "machine_id": self.machine_id
                }
                
                # Store using hAIveMind memory system
                collection = self.chroma_client.get_or_create_collection(
                    name="config_templates",
                    metadata={"category": "infrastructure"}
                )
                
                template_id = f"template_{template_name}_{hashlib.md5(template_name.encode()).hexdigest()[:8]}"
                collection.add(
                    documents=[json.dumps(template_data)],
                    metadatas=[{"template_name": template_name, "config_type": config_type}],
                    ids=[template_id]
                )
                
                # Store in hAIveMind for searchability
                await self._store_memory(
                    content=f"Configuration template '{template_name}' created",
                    category="infrastructure",
                    context=f"Created {config_type} template with {len(variables or {})} variables",
                    metadata={
                        "template_id": template_id,
                        "config_type": config_type,
                        "variables": list((variables or {}).keys())
                    },
                    tags=["config_template", "devops", config_type]
                )
                
                return f"""âœ… Configuration template '{template_name}' created successfully!

ðŸ“‹ **Template Details:**
â€¢ **Type**: {config_type.upper()}
â€¢ **Template ID**: {template_id}
â€¢ **Variables**: {len(variables or {})} defined
â€¢ **Description**: {description or 'No description'}

ðŸ”§ **Template Content Preview:**
```{config_type}
{template_content[:500]}{'...' if len(template_content) > 500 else ''}
```

ðŸ·ï¸ Template stored in hAIveMind memory for reuse across infrastructure."""
                
            except Exception as e:
                return f"âŒ Template creation failed: {str(e)}"

        # # @self.mcp.tool()
        async def render_config_from_template(
            template_name: str,
            variables: Dict[str, Any],
            output_path: Optional[str] = None,
            validate_output: bool = True
        ) -> str:
            """Render a configuration file from a template with variable substitution"""
            try:
                from jinja2 import Template, Environment, FileSystemLoader
                
                # Retrieve template from hAIveMind memory
                collection = self.chroma_client.get_or_create_collection(
                    name="config_templates",
                    metadata={"category": "infrastructure"}
                )
                
                results = collection.query(
                    query_texts=[template_name],
                    where={"template_name": template_name},
                    n_results=1
                )
                
                if not results['documents']:
                    return f"âŒ Template '{template_name}' not found. Use create_config_template first."
                
                template_data = json.loads(results['documents'][0])
                template_content = template_data['content']
                config_type = template_data['type']
                
                # Set up Jinja2 environment with custom filters
                env = Environment()
                env.filters['default'] = lambda value, default_value: value if value else default_value
                
                # Render template with provided variables
                template = env.from_string(template_content)
                rendered_content = template.render(**variables)
                
                # Validate output based on config type
                validation_result = "âœ… Valid"
                if validate_output:
                    try:
                        if config_type == "yaml":
                            yaml.safe_load(rendered_content)
                        elif config_type == "json":
                            json.loads(rendered_content)
                        elif config_type == "toml":
                            toml.loads(rendered_content)
                        elif config_type == "ini":
                            config = configparser.ConfigParser()
                            config.read_string(rendered_content)
                    except Exception as ve:
                        validation_result = f"âŒ Validation failed: {str(ve)}"
                
                # Save to file if output_path provided
                output_info = ""
                if output_path:
                    output_file = Path(output_path)
                    output_file.parent.mkdir(parents=True, exist_ok=True)
                    output_file.write_text(rendered_content)
                    output_info = f"\nðŸ“ **Saved to**: {output_path}"
                
                # Store render event in hAIveMind
                await self._store_memory(
                    content=f"Configuration rendered from template '{template_name}'",
                    category="infrastructure",
                    context=f"Rendered {config_type} config with {len(variables)} variables",
                    metadata={
                        "template_name": template_name,
                        "output_path": output_path,
                        "variables": variables,
                        "validation": validation_result
                    },
                    tags=["config_render", "devops", config_type]
                )
                
                return f"""âœ… Configuration rendered from template '{template_name}'!

ðŸ”§ **Render Details:**
â€¢ **Template Type**: {config_type.upper()}
â€¢ **Variables Applied**: {len(variables)}
â€¢ **Validation**: {validation_result}{output_info}

ðŸ“„ **Rendered Configuration:**
```{config_type}
{rendered_content}
```

ðŸ·ï¸ Render operation logged in hAIveMind memory."""
                
            except Exception as e:
                return f"âŒ Configuration rendering failed: {str(e)}"

        # # @self.mcp.tool()
        async def validate_config_file(
            config_path: str,
            config_type: Optional[str] = None,
            schema_path: Optional[str] = None,
            lint_rules: Optional[List[str]] = None
        ) -> str:
            """Validate configuration files against schemas and lint rules"""
            try:
                config_file = Path(config_path)
                if not config_file.exists():
                    return f"âŒ Configuration file not found: {config_path}"
                
                # Auto-detect config type if not provided
                if not config_type:
                    suffix = config_file.suffix.lower()
                    type_mapping = {
                        '.yaml': 'yaml', '.yml': 'yaml',
                        '.json': 'json',
                        '.toml': 'toml',
                        '.ini': 'ini', '.conf': 'ini',
                        '.env': 'env'
                    }
                    config_type = type_mapping.get(suffix, 'text')
                
                content = config_file.read_text()
                validation_results = []
                
                # Basic syntax validation
                try:
                    if config_type == "yaml":
                        parsed = yaml.safe_load(content)
                        validation_results.append("âœ… YAML syntax valid")
                    elif config_type == "json":
                        parsed = json.loads(content)
                        validation_results.append("âœ… JSON syntax valid")
                    elif config_type == "toml":
                        parsed = toml.loads(content)
                        validation_results.append("âœ… TOML syntax valid")
                    elif config_type == "ini":
                        config = configparser.ConfigParser()
                        config.read_string(content)
                        parsed = dict(config._sections)
                        validation_results.append("âœ… INI syntax valid")
                    else:
                        parsed = {"raw_content": content}
                        validation_results.append("âœ… File readable")
                        
                except Exception as ve:
                    validation_results.append(f"âŒ Syntax error: {str(ve)}")
                    parsed = None
                
                # Schema validation if provided
                if schema_path and parsed:
                    try:
                        import jsonschema
                        schema_file = Path(schema_path)
                        if schema_file.exists():
                            schema = json.loads(schema_file.read_text())
                            jsonschema.validate(parsed, schema)
                            validation_results.append("âœ… Schema validation passed")
                        else:
                            validation_results.append(f"âš ï¸ Schema file not found: {schema_path}")
                    except Exception as se:
                        validation_results.append(f"âŒ Schema validation failed: {str(se)}")
                
                # Apply lint rules
                lint_results = []
                if lint_rules and parsed:
                    for rule in lint_rules:
                        if rule == "no_empty_values" and isinstance(parsed, dict):
                            empty_keys = [k for k, v in parsed.items() if not v]
                            if empty_keys:
                                lint_results.append(f"âš ï¸ Empty values found: {', '.join(empty_keys)}")
                            else:
                                lint_results.append("âœ… No empty values")
                        
                        elif rule == "required_fields" and isinstance(parsed, dict):
                            required = ["name", "version"]
                            missing = [f for f in required if f not in parsed]
                            if missing:
                                lint_results.append(f"âš ï¸ Missing required fields: {', '.join(missing)}")
                            else:
                                lint_results.append("âœ… All required fields present")
                
                # Store validation results
                await self._store_memory(
                    content=f"Configuration validation for {config_path}",
                    category="infrastructure",
                    context=f"Validated {config_type} configuration with {len(validation_results)} checks",
                    metadata={
                        "config_path": config_path,
                        "config_type": config_type,
                        "validation_results": validation_results,
                        "lint_results": lint_results
                    },
                    tags=["config_validation", "devops", config_type]
                )
                
                # Format results
                output = f"""ðŸ” Configuration validation for: {config_path}

ðŸ“‹ **File Details:**
â€¢ **Type**: {config_type.upper()}
â€¢ **Size**: {len(content)} characters
â€¢ **Lines**: {len(content.splitlines())}

âœ… **Validation Results:**"""
                
                for result in validation_results:
                    output += f"\n  {result}"
                
                if lint_results:
                    output += "\n\nðŸ” **Lint Results:**"
                    for result in lint_results:
                        output += f"\n  {result}"
                
                output += "\n\nðŸ·ï¸ Validation results stored in hAIveMind memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Configuration validation failed: {str(e)}"

        # # @self.mcp.tool()
        async def diff_config_files(
            file1_path: str,
            file2_path: str,
            output_format: str = "unified",  # unified, context, side_by_side
            ignore_whitespace: bool = False,
            ignore_comments: bool = False
        ) -> str:
            """Compare two configuration files and show differences"""
            try:
                file1 = Path(file1_path)
                file2 = Path(file2_path)
                
                if not file1.exists():
                    return f"âŒ File 1 not found: {file1_path}"
                if not file2.exists():
                    return f"âŒ File 2 not found: {file2_path}"
                
                content1 = file1.read_text().splitlines()
                content2 = file2.read_text().splitlines()
                
                # Apply filters
                if ignore_whitespace:
                    content1 = [line.strip() for line in content1]
                    content2 = [line.strip() for line in content2]
                
                if ignore_comments:
                    # Remove lines starting with #, //, or /* */
                    content1 = [line for line in content1 if not line.strip().startswith(('#', '//', '/*'))]
                    content2 = [line for line in content2 if not line.strip().startswith(('#', '//', '/*'))]
                
                # Generate diff
                if output_format == "unified":
                    diff_lines = list(difflib.unified_diff(
                        content1, content2,
                        fromfile=file1_path, tofile=file2_path,
                        lineterm=''
                    ))
                elif output_format == "context":
                    diff_lines = list(difflib.context_diff(
                        content1, content2,
                        fromfile=file1_path, tofile=file2_path,
                        lineterm=''
                    ))
                else:  # side_by_side
                    diff_lines = []
                    for i, (line1, line2) in enumerate(zip(content1, content2)):
                        if line1 != line2:
                            diff_lines.append(f"{i+1:4}: {line1:<50} | {line2}")
                
                diff_text = '\n'.join(diff_lines) if diff_lines else "No differences found"
                
                # Calculate statistics
                added_lines = len([line for line in diff_lines if line.startswith('+')])
                removed_lines = len([line for line in diff_lines if line.startswith('-')])
                
                # Store diff results
                await self._store_memory(
                    content=f"Configuration diff between {file1.name} and {file2.name}",
                    category="infrastructure",
                    context=f"Compared configs: +{added_lines} -{removed_lines} changes",
                    metadata={
                        "file1": file1_path,
                        "file2": file2_path,
                        "added_lines": added_lines,
                        "removed_lines": removed_lines,
                        "has_differences": len(diff_lines) > 0
                    },
                    tags=["config_diff", "devops", "comparison"]
                )
                
                return f"""ðŸ” Configuration Diff: {file1.name} vs {file2.name}

ðŸ“Š **Diff Statistics:**
â€¢ **Added Lines**: +{added_lines}
â€¢ **Removed Lines**: -{removed_lines}
â€¢ **Format**: {output_format}
â€¢ **Filters**: {'Whitespace ignored, ' if ignore_whitespace else ''}{'Comments ignored' if ignore_comments else ''}

ðŸ“„ **Differences:**
```diff
{diff_text[:2000]}{'...' if len(diff_text) > 2000 else ''}
```

ðŸ·ï¸ Diff results stored in hAIveMind memory."""
                
            except Exception as e:
                return f"âŒ Configuration diff failed: {str(e)}"

        # # @self.mcp.tool()
        async def deploy_config(
            config_path: str,
            target_path: str,
            backup_existing: bool = True,
            validate_before_deploy: bool = True,
            restart_services: Optional[List[str]] = None,
            rollback_on_failure: bool = True
        ) -> str:
            """Deploy configuration file with validation, backup, and service restart"""
            try:
                source_file = Path(config_path)
                target_file = Path(target_path)
                
                if not source_file.exists():
                    return f"âŒ Source configuration not found: {config_path}"
                
                deployment_id = f"deploy_{hashlib.md5(f'{config_path}_{target_path}_{time.time()}'.encode()).hexdigest()[:8]}"
                deployment_log = []
                
                # Validate source config
                if validate_before_deploy:
                    validation_result = await self.validate_config_file(config_path)
                    if "âŒ" in validation_result:
                        return f"âŒ Pre-deployment validation failed:\n{validation_result}"
                    deployment_log.append("âœ… Source validation passed")
                
                # Backup existing config
                backup_path = None
                if backup_existing and target_file.exists():
                    timestamp = int(time.time())
                    backup_path = f"{target_path}.backup.{timestamp}"
                    shutil.copy2(str(target_file), backup_path)
                    deployment_log.append(f"âœ… Existing config backed up to: {backup_path}")
                
                # Deploy new configuration
                target_file.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(str(source_file), str(target_file))
                deployment_log.append(f"âœ… Configuration deployed to: {target_path}")
                
                # Restart services if specified
                service_results = []
                if restart_services:
                    for service in restart_services:
                        try:
                            result = subprocess.run(
                                ["systemctl", "restart", service],
                                capture_output=True, text=True, timeout=30
                            )
                            if result.returncode == 0:
                                service_results.append(f"âœ… {service} restarted successfully")
                            else:
                                service_results.append(f"âŒ {service} restart failed: {result.stderr}")
                                if rollback_on_failure and backup_path:
                                    shutil.copy2(backup_path, str(target_file))
                                    service_results.append(f"ðŸ”„ Rolled back to previous config")
                        except subprocess.TimeoutExpired:
                            service_results.append(f"â±ï¸ {service} restart timed out")
                        except Exception as se:
                            service_results.append(f"âŒ {service} restart error: {str(se)}")
                
                # Store deployment record
                deployment_data = {
                    "deployment_id": deployment_id,
                    "source_path": config_path,
                    "target_path": target_path,
                    "backup_path": backup_path,
                    "services_restarted": restart_services or [],
                    "deployment_log": deployment_log,
                    "service_results": service_results,
                    "timestamp": time.time(),
                    "machine_id": self.machine_id
                }
                
                await self._store_memory(
                    content=f"Configuration deployment: {source_file.name} â†’ {target_path}",
                    category="deployments",
                    context=f"Deployed config with {'backup, ' if backup_existing else ''}{len(restart_services or [])} service restarts",
                    metadata=deployment_data,
                    tags=["config_deploy", "devops", "deployment"]
                )
                
                output = f"""ðŸš€ Configuration deployed successfully!

ðŸ“¦ **Deployment Details:**
â€¢ **ID**: {deployment_id}
â€¢ **Source**: {config_path}
â€¢ **Target**: {target_path}
â€¢ **Backup**: {backup_path or 'Not created'}

ðŸ“‹ **Deployment Log:**"""
                
                for log_entry in deployment_log:
                    output += f"\n  {log_entry}"
                
                if service_results:
                    output += "\n\nðŸ”„ **Service Restart Results:**"
                    for result in service_results:
                        output += f"\n  {result}"
                
                output += f"\n\nðŸ·ï¸ Deployment record stored in hAIveMind memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Configuration deployment failed: {str(e)}"

        # # @self.mcp.tool()
        async def list_config_templates(
            config_type: Optional[str] = None,
            search_query: Optional[str] = None,
            limit: int = 20
        ) -> str:
            """List available configuration templates with search and filtering"""
            try:
                collection = self.chroma_client.get_or_create_collection(
                    name="config_templates",
                    metadata={"category": "infrastructure"}
                )
                
                # Build query
                where_clause = {}
                if config_type:
                    where_clause["config_type"] = config_type
                
                if search_query:
                    results = collection.query(
                        query_texts=[search_query],
                        where=where_clause if where_clause else None,
                        n_results=limit
                    )
                else:
                    results = collection.get(
                        where=where_clause if where_clause else None,
                        limit=limit
                    )
                
                if not results['documents']:
                    return "ðŸ“ No configuration templates found matching criteria."
                
                output = f"ðŸ“ Configuration Templates ({len(results['documents'])} found):\n\n"
                
                for i, doc in enumerate(results['documents']):
                    template_data = json.loads(doc)
                    
                    output += f"**{i+1}. {template_data['name']}**\n"
                    output += f"   â€¢ **Type**: {template_data['type'].upper()}\n"
                    output += f"   â€¢ **Description**: {template_data['description']}\n"
                    output += f"   â€¢ **Variables**: {len(template_data['variables'])}\n"
                    output += f"   â€¢ **Created**: {time.strftime('%Y-%m-%d %H:%M', time.localtime(template_data['created_at']))}\n"
                    if template_data['tags']:
                        output += f"   â€¢ **Tags**: {', '.join(template_data['tags'])}\n"
                    output += "\n"
                
                return output
                
            except Exception as e:
                return f"âŒ Template listing failed: {str(e)}"

        logger.info("âš™ï¸ Configuration management tools registered - template system and validation enabled")

    def _register_claude_shortcut_commands(self):
        """Register comprehensive Claude shortcut commands for DevOps operations"""
        import re
        import shlex
        from typing import Union
        
        # Command parser for shortcut commands
        def parse_command(command: str) -> tuple[str, dict]:
            """Parse shortcut command into command name and parameters"""
            if not command.startswith('/'):
                return None, {}
            
            # Remove leading slash and split
            parts = shlex.split(command[1:])
            if not parts:
                return None, {}
            
            cmd_name = parts[0]
            params = {}
            
            # Parse parameters
            for part in parts[1:]:
                if '=' in part:
                    key, value = part.split('=', 1)
                    # Try to parse as boolean, int, or keep as string
                    if value.lower() in ('true', 'false'):
                        params[key] = value.lower() == 'true'
                    elif value.isdigit():
                        params[key] = int(value)
                    else:
                        params[key] = value
                else:
                    # Positional parameter - add to params list
                    if 'args' not in params:
                        params['args'] = []
                    params['args'].append(part)
            
            return cmd_name, params
        
        # # @self.mcp.tool()
        async def claude_shortcut(
            command: str,
            context: Optional[Dict[str, Any]] = None
        ) -> str:
            """Execute Claude shortcut commands for DevOps operations
            
            Available shortcut commands:
            
            ### Category Management:
            /cat-create [name] - Quick category creation
            /cat-list - List all categories with stats
            /cat-backup [category] - Backup specific category
            /cat-restore [backup_id] - Restore category from backup
            
            ### Project Management:
            /proj-create [name] [path] - Create new project
            /proj-switch [name] - Switch project context
            /proj-health - Check current project health
            /proj-backup - Backup current project
            
            ### Backup System:
            /backup-all - Backup all configurations
            /backup-verify - Verify latest backup integrity
            /backup-list - Show available backups
            /restore [backup_id] - Restore from backup
            /backup-schedule [frequency] - Schedule automated backups
            
            ### Service Discovery:
            /services-discover - Auto-discover all services
            /services-health - Check health of all services
            /services-map - Show service dependency map
            /service-register [name] [port] - Register new service
            
            ### Configuration Management:
            /config-template [name] [type] - Create config template
            /config-render [template] - Render config from template
            /config-validate [path] - Validate configuration file
            /config-diff [file1] [file2] - Compare configuration files
            /config-deploy [source] [target] - Deploy configuration
            
            ### Infrastructure:
            /infra-state - Get current infrastructure state
            /infra-backup - Backup infrastructure configs
            /sync-all - Trigger full system sync
            /emergency-backup - Emergency full system backup
            
            ### Status & Health:
            /status - Overall system status dashboard
            /health - Complete health check report
            /agents-list - List all active agents
            /uptime - System uptime and stats
            
            Example usage:
            /backup-all encryption=true compress=true
            /service-register elasticsearch 9200 deps=kibana,logstash
            /config-template nginx-proxy nginx
            """
            try:
                cmd_name, params = parse_command(command)
                if not cmd_name:
                    return "âŒ Invalid command format. Commands must start with '/' (e.g., /backup-all)"
                
                # Category Management Commands
                if cmd_name == "cat-create":
                    if not params.get('args') or len(params['args']) < 1:
                        return "âŒ Usage: /cat-create [category_name] retention_days=30 encryption=true"
                    
                    category_name = params['args'][0]
                    retention_days = params.get('retention_days', 365)
                    encryption = params.get('encryption', True)
                    
                    return await self.create_memory_category(
                        category_name=category_name,
                        description=f"Category created via shortcut command",
                        retention_days=retention_days,
                        auto_cleanup=True,
                        encryption_enabled=encryption
                    )
                
                elif cmd_name == "cat-list":
                    return await self.list_memory_categories()
                
                elif cmd_name == "cat-backup":
                    if not params.get('args') or len(params['args']) < 1:
                        return "âŒ Usage: /cat-backup [category_name] destination=local"
                    
                    category = params['args'][0]
                    destination = params.get('destination', 'local')
                    
                    return await self.backup_category_data(
                        category=category,
                        backup_destination=destination,
                        include_metadata=True,
                        compression_enabled=True
                    )
                
                elif cmd_name == "cat-restore":
                    if not params.get('args') or len(params['args']) < 1:
                        return "âŒ Usage: /cat-restore [backup_id] verify=true"
                    
                    backup_id = params['args'][0]
                    verify = params.get('verify', True)
                    
                    return await self.restore_category_data(
                        backup_id=backup_id,
                        verify_integrity=verify,
                        create_snapshot=True
                    )
                
                # Project Management Commands
                elif cmd_name == "proj-create":
                    if not params.get('args') or len(params['args']) < 2:
                        return "âŒ Usage: /proj-create [project_name] [project_path] auto_setup=true"
                    
                    name = params['args'][0]
                    path = params['args'][1]
                    auto_setup = params.get('auto_setup', True)
                    
                    return await self.create_project_context(
                        project_name=name,
                        project_path=path,
                        auto_setup=auto_setup,
                        initialize_git=True
                    )
                
                elif cmd_name == "proj-switch":
                    if not params.get('args') or len(params['args']) < 1:
                        return "âŒ Usage: /proj-switch [project_name]"
                    
                    project_name = params['args'][0]
                    return await self.switch_project_context(project_name=project_name)
                
                elif cmd_name == "proj-health":
                    detailed = params.get('detailed', True)
                    return await self.get_project_health(
                        include_dependencies=detailed,
                        include_metrics=detailed
                    )
                
                elif cmd_name == "proj-backup":
                    compression = params.get('compression', True)
                    encryption = params.get('encryption', True)
                    
                    return await self.backup_project_data(
                        include_git=True,
                        compression_enabled=compression,
                        encryption_enabled=encryption
                    )
                
                # Backup System Commands
                elif cmd_name == "backup-all":
                    chromadb = params.get('chromadb', True)
                    redis = params.get('redis', True)
                    encryption = params.get('encryption', True)
                    compression = params.get('compression', True)
                    
                    return await self.backup_all_configs(
                        include_chromadb=chromadb,
                        include_redis=redis,
                        include_agent_states=True,
                        encryption_enabled=encryption,
                        compression_enabled=compression
                    )
                
                elif cmd_name == "backup-verify":
                    limit = params.get('limit', 5)
                    return await self.verify_backup_integrity(
                        backup_limit=limit,
                        detailed_report=True
                    )
                
                elif cmd_name == "backup-list":
                    limit = params.get('limit', 20)
                    return await self.list_available_backups(
                        limit=limit,
                        include_metadata=True,
                        sort_by='timestamp'
                    )
                
                elif cmd_name == "restore":
                    if not params.get('args') or len(params['args']) < 1:
                        return "âŒ Usage: /restore [backup_id] verify=true create_snapshot=true"
                    
                    backup_id = params['args'][0]
                    verify = params.get('verify', True)
                    snapshot = params.get('create_snapshot', True)
                    
                    return await self.restore_from_backup(
                        backup_id=backup_id,
                        verify_before_restore=verify,
                        create_pre_restore_snapshot=snapshot
                    )
                
                elif cmd_name == "backup-schedule":
                    if not params.get('args') or len(params['args']) < 1:
                        return "âŒ Usage: /backup-schedule [frequency] retention_days=30"
                    
                    frequency = params['args'][0]  # daily, weekly, monthly
                    retention = params.get('retention_days', 30)
                    
                    return await self.setup_automated_backup(
                        schedule_frequency=frequency,
                        retention_days=retention,
                        include_all_systems=True
                    )
                
                # Service Discovery Commands
                elif cmd_name == "services-discover":
                    scan_ports = params.get('scan_ports', True)
                    check_docker = params.get('check_docker', True)
                    check_k8s = params.get('check_k8s', True)
                    
                    methods = []
                    if scan_ports:
                        methods.append('port_scan')
                    if check_docker:
                        methods.append('docker')
                    if check_k8s:
                        methods.append('kubernetes')
                    methods.extend(['processes', 'systemctl'])
                    
                    return await self.discover_services(
                        discovery_methods=methods,
                        scan_ports=scan_ports,
                        check_processes=True,
                        check_systemctl=True,
                        check_docker=check_docker
                    )
                
                elif cmd_name == "services-health":
                    format_type = params.get('format', 'detailed')
                    return await self.health_check_all(
                        parallel_checks=True,
                        timeout_seconds=10,
                        include_performance_metrics=True,
                        output_format=format_type
                    )
                
                elif cmd_name == "services-map":
                    format_type = params.get('format', 'text')
                    return await self.service_dependency_map(
                        include_external_deps=True,
                        visualization_format=format_type,
                        include_health_status=True
                    )
                
                elif cmd_name == "service-register":
                    if not params.get('args') or len(params['args']) < 2:
                        return "âŒ Usage: /service-register [service_name] [port] health_endpoint=/health"
                    
                    name = params['args'][0]
                    port = int(params['args'][1])
                    health_endpoint = params.get('health_endpoint', '/health')
                    deps = params.get('deps', '').split(',') if params.get('deps') else []
                    
                    return await self.register_service(
                        service_name=name,
                        port=port,
                        health_endpoint=health_endpoint,
                        dependencies=deps,
                        auto_health_check=True
                    )
                
                # Configuration Management Commands
                elif cmd_name == "config-template":
                    if not params.get('args') or len(params['args']) < 2:
                        return "âŒ Usage: /config-template [template_name] [config_type] description='Auto-generated template'"
                    
                    name = params['args'][0]
                    config_type = params['args'][1]
                    description = params.get('description', f'Template for {name}')
                    
                    return await self.create_config_template(
                        template_name=name,
                        config_type=config_type,
                        description=description,
                        tags=['shortcut', 'auto-generated']
                    )
                
                elif cmd_name == "config-render":
                    if not params.get('args') or len(params['args']) < 1:
                        return "âŒ Usage: /config-render [template_name] validate=true"
                    
                    template_name = params['args'][0]
                    validate = params.get('validate', True)
                    
                    # Extract variables from params (excluding 'args' and known params)
                    variables = {k: v for k, v in params.items() if k not in ['args', 'validate']}
                    
                    return await self.render_config_from_template(
                        template_name=template_name,
                        variables=variables,
                        validate_output=validate
                    )
                
                elif cmd_name == "config-validate":
                    if not params.get('args') or len(params['args']) < 1:
                        return "âŒ Usage: /config-validate [config_path] lint_rules=['no_empty_values']"
                    
                    config_path = params['args'][0]
                    lint_rules = params.get('lint_rules', ['no_empty_values', 'required_fields'])
                    if isinstance(lint_rules, str):
                        lint_rules = lint_rules.split(',')
                    
                    return await self.validate_config_file(
                        config_path=config_path,
                        lint_rules=lint_rules
                    )
                
                elif cmd_name == "config-diff":
                    if not params.get('args') or len(params['args']) < 2:
                        return "âŒ Usage: /config-diff [file1] [file2] format=unified ignore_whitespace=false"
                    
                    file1 = params['args'][0]
                    file2 = params['args'][1]
                    format_type = params.get('format', 'unified')
                    ignore_whitespace = params.get('ignore_whitespace', False)
                    
                    return await self.diff_config_files(
                        file1_path=file1,
                        file2_path=file2,
                        output_format=format_type,
                        ignore_whitespace=ignore_whitespace
                    )
                
                elif cmd_name == "config-deploy":
                    if not params.get('args') or len(params['args']) < 2:
                        return "âŒ Usage: /config-deploy [source_path] [target_path] backup=true validate=true"
                    
                    source = params['args'][0]
                    target = params['args'][1]
                    backup = params.get('backup', True)
                    validate = params.get('validate', True)
                    services = params.get('restart_services', '').split(',') if params.get('restart_services') else None
                    
                    return await self.deploy_config(
                        config_path=source,
                        target_path=target,
                        backup_existing=backup,
                        validate_before_deploy=validate,
                        restart_services=services
                    )
                
                # Infrastructure Commands
                elif cmd_name == "infra-state":
                    machine_id = params.get('machine_id', self.machine_id)
                    include_services = params.get('include_services', True)
                    
                    return await self.track_infrastructure_state(
                        machine_id=machine_id,
                        state_type='comprehensive_status',
                        state_data={
                            'timestamp': time.time(),
                            'include_services': include_services,
                            'requested_via': 'shortcut_command'
                        }
                    )
                
                elif cmd_name == "sync-all":
                    force_sync = params.get('force', False)
                    return await self.sync_devops_tools(
                        force_update=force_sync,
                        tool_categories=['all']
                    )
                
                elif cmd_name == "emergency-backup":
                    return await self.emergency_backup_system(
                        include_all_data=True,
                        encryption_enabled=True,
                        priority='critical'
                    )
                
                # Status & Health Commands
                elif cmd_name == "status":
                    return await self.get_comprehensive_status()
                
                elif cmd_name == "health":
                    return await self.perform_full_health_check()
                
                elif cmd_name == "agents-list":
                    include_inactive = params.get('include_inactive', False)
                    return await self.get_agent_roster(include_inactive=include_inactive)
                
                elif cmd_name == "uptime":
                    current_time = time.time()
                    uptime_seconds = current_time - self._start_time
                    uptime_hours = uptime_seconds / 3600
                    uptime_days = uptime_hours / 24
                    
                    return f"""ðŸ•’ **hAIveMind Network Portal Uptime**
                    
ðŸ“Š **Current Status**: Operational
â±ï¸ **Uptime**: {uptime_days:.1f} days ({uptime_hours:.1f} hours)
ðŸš€ **Started**: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self._start_time))}
ðŸŒ **Machine**: {self.machine_id}
ðŸ”— **Endpoints**: SSE, HTTP, Admin Dashboard
ðŸ“¡ **Active Connections**: {len(getattr(self, '_active_connections', []))}

ðŸ’¾ **Memory Collections**: {len(self.chroma_client.list_collections())}
ðŸ”§ **Tools Registered**: {len([tool for tool in dir(self) if tool.startswith('_tool_') or hasattr(getattr(self, tool, None), '_mcp_tool')])}
ðŸ·ï¸ **Network Health**: Excellent"""
                
                else:
                    return f"""âŒ Unknown shortcut command: /{cmd_name}

ðŸ”§ **Available Categories:**
â€¢ Category Management: /cat-create, /cat-list, /cat-backup, /cat-restore
â€¢ Project Management: /proj-create, /proj-switch, /proj-health, /proj-backup  
â€¢ Backup System: /backup-all, /backup-verify, /backup-list, /restore, /backup-schedule
â€¢ Service Discovery: /services-discover, /services-health, /services-map, /service-register
â€¢ Configuration: /config-template, /config-render, /config-validate, /config-diff, /config-deploy
â€¢ Infrastructure: /infra-state, /sync-all, /emergency-backup
â€¢ Status & Health: /status, /health, /agents-list, /uptime

ðŸ’¡ **Usage**: Use `claude_shortcut` with any of these commands
ðŸ“š **Help**: Each command shows usage when called incorrectly"""
                
            except Exception as e:
                return f"âŒ Shortcut command execution failed: {str(e)}"
        
        # Individual shortcut commands for better discoverability
        # # @self.mcp.tool()
        async def backup_all_shortcut(
            encryption: bool = True,
            compression: bool = True,
            verify: bool = True
        ) -> str:
            """Quick backup all configurations - shortcut for /backup-all"""
            return await self.backup_all_configs(
                include_chromadb=True,
                include_redis=True,
                include_agent_states=True,
                encryption_enabled=encryption,
                compression_enabled=compression
            )
        
        # # @self.mcp.tool()
        async def services_discover_shortcut(
            scan_ports: bool = True,
            check_docker: bool = True,
            timeout: int = 5
        ) -> str:
            """Quick service discovery - shortcut for /services-discover"""
            methods = ['port_scan', 'processes', 'systemctl']
            if check_docker:
                methods.append('docker')
            
            return await self.discover_services(
                discovery_methods=methods,
                scan_ports=scan_ports,
                check_processes=True,
                check_systemctl=True,
                check_docker=check_docker,
                timeout_seconds=timeout
            )
        
        # # @self.mcp.tool()
        async def system_status_shortcut() -> str:
            """Quick system status check - shortcut for /status"""
            try:
                # Get basic system information
                uptime_seconds = time.time() - self._start_time
                uptime_hours = uptime_seconds / 3600
                
                # Get memory collection count
                collections = self.chroma_client.list_collections()
                collection_count = len(collections)
                
                # Get recent memory count
                recent_memories = 0
                try:
                    for collection in collections:
                        recent_memories += collection.count()
                except:
                    recent_memories = "Unknown"
                
                return f"""ðŸŽ¯ **hAIveMind System Status Dashboard**

ðŸ¢ **System Overview:**
â€¢ **Machine**: {self.machine_id}
â€¢ **Uptime**: {uptime_hours:.1f} hours
â€¢ **Status**: âœ… Operational
â€¢ **Version**: v1.0.0

ðŸ“Š **Memory System:**
â€¢ **Collections**: {collection_count}
â€¢ **Total Memories**: {recent_memories}
â€¢ **Storage**: ChromaDB + Redis

ðŸ”§ **DevOps Tools:**
â€¢ **Backup System**: âœ… Ready ({10} tools)
â€¢ **Service Discovery**: âœ… Ready ({5} tools)
â€¢ **Config Management**: âœ… Ready ({6} tools)
â€¢ **Project Management**: âœ… Ready ({6} tools)
â€¢ **Category Management**: âœ… Ready ({6} tools)

ðŸŒ **Network:**
â€¢ **Remote Access**: http://{self.host}:{self.port}
â€¢ **SSE Endpoint**: /sse
â€¢ **HTTP Endpoint**: /mcp
â€¢ **Health Check**: /health

âš¡ **Quick Commands:**
â€¢ `/backup-all` - Emergency backup
â€¢ `/services-discover` - Discover services
â€¢ `/health` - Detailed health check
â€¢ `/agents-list` - List active agents

ðŸ·ï¸ System is ready for DevOps operations."""
                
            except Exception as e:
                return f"âŒ Status check failed: {str(e)}"
        
        # # @self.mcp.tool() 
        async def emergency_backup_shortcut() -> str:
            """Emergency full system backup - shortcut for /emergency-backup"""
            return await self.backup_all_configs(
                include_chromadb=True,
                include_redis=True, 
                include_agent_states=True,
                encryption_enabled=True,
                compression_enabled=True,
                backup_name=f"emergency_backup_{int(time.time())}"
            )
        
        logger.info("ðŸŽ¯ Claude shortcut commands registered - 32+ DevOps shortcuts ready for use")

    def _register_monitoring_integration_tools(self):
        """Register comprehensive monitoring and alerting integration tools"""
        import json
        import yaml
        import requests
        from datetime import datetime, timedelta
        import re
        from typing import List, Dict, Any, Optional
        import asyncio
        import aiohttp
        
        # # @self.mcp.tool()
        async def create_alert_rule(
            service_name: str,
            metric_name: str,
            threshold: float,
            comparison: str = "greater_than",  # greater_than, less_than, equal_to
            duration: str = "5m",
            severity: str = "warning",  # critical, warning, info
            description: Optional[str] = None,
            labels: Optional[Dict[str, str]] = None,
            annotations: Optional[Dict[str, str]] = None
        ) -> str:
            """Create intelligent alert rules with machine learning thresholds"""
            try:
                # Generate rule ID
                rule_id = f"alert_{service_name}_{metric_name}_{int(time.time())}"
                
                # Map comparison operators
                operator_mapping = {
                    "greater_than": ">",
                    "less_than": "<",
                    "equal_to": "==",
                    "not_equal": "!=",
                    "greater_equal": ">=",
                    "less_equal": "<="
                }
                
                operator = operator_mapping.get(comparison, ">")
                
                # Build Prometheus alert rule
                alert_rule = {
                    "alert": f"{service_name.title()}_{metric_name.replace('_', '').title()}Alert",
                    "expr": f"{metric_name}{{service=\"{service_name}\"}} {operator} {threshold}",
                    "for": duration,
                    "labels": {
                        "severity": severity,
                        "service": service_name,
                        "metric": metric_name,
                        "rule_id": rule_id,
                        **(labels or {})
                    },
                    "annotations": {
                        "summary": f"{service_name} {metric_name} alert",
                        "description": description or f"{service_name} {metric_name} is {comparison} {threshold}",
                        "runbook_url": f"http://{self.host}:{self.port}/admin/runbooks/{service_name}",
                        **(annotations or {})
                    }
                }
                
                # Store rule in hAIveMind for management
                rule_data = {
                    "rule_id": rule_id,
                    "service_name": service_name,
                    "metric_name": metric_name,
                    "threshold": threshold,
                    "comparison": comparison,
                    "duration": duration,
                    "severity": severity,
                    "prometheus_rule": alert_rule,
                    "created_at": time.time(),
                    "machine_id": self.machine_id,
                    "active": True
                }
                
                await self._store_memory(
                    content=f"Alert rule created: {service_name} {metric_name} {comparison} {threshold}",
                    category="monitoring",
                    context=f"Prometheus alert rule with {severity} severity",
                    metadata=rule_data,
                    tags=["alert_rule", "monitoring", "prometheus", severity]
                )
                
                # Generate Prometheus rule file format
                rule_yaml = yaml.dump({
                    "groups": [{
                        "name": f"{service_name}_alerts",
                        "rules": [alert_rule]
                    }]
                }, default_flow_style=False)
                
                return f"""âœ… Alert rule created successfully!

ðŸ“Š **Alert Rule Details:**
â€¢ **Rule ID**: {rule_id}
â€¢ **Service**: {service_name}
â€¢ **Metric**: {metric_name}
â€¢ **Threshold**: {metric_name} {operator} {threshold}
â€¢ **Duration**: {duration}
â€¢ **Severity**: {severity}

ðŸ”” **Prometheus Rule:**
```yaml
{rule_yaml}
```

âš™ï¸ **Integration Steps:**
1. Add the YAML above to your Prometheus rules file
2. Reload Prometheus configuration: `curl -X POST http://prometheus:9090/-/reload`
3. Verify in Prometheus UI: http://prometheus:9090/rules

ðŸ·ï¸ Alert rule stored in hAIveMind monitoring memory for management."""
                
            except Exception as e:
                return f"âŒ Alert rule creation failed: {str(e)}"

        # # @self.mcp.tool()
        async def get_metrics(
            service_name: str,
            metric_names: Optional[List[str]] = None,
            time_range: str = "1h",
            prometheus_url: str = "http://localhost:9090",
            aggregation: str = "avg"  # avg, sum, max, min, rate
        ) -> str:
            """Fetch metrics from Prometheus/Grafana with intelligent querying"""
            try:
                # Parse time range
                time_mapping = {
                    "5m": "5m", "15m": "15m", "30m": "30m", 
                    "1h": "1h", "6h": "6h", "12h": "12h", 
                    "1d": "1d", "7d": "7d", "30d": "30d"
                }
                duration = time_mapping.get(time_range, "1h")
                
                # Default metrics if none provided
                if not metric_names:
                    metric_names = [
                        "up", "cpu_usage_percent", "memory_usage_percent",
                        "disk_usage_percent", "http_requests_total", "response_time_seconds"
                    ]
                
                metrics_data = {}
                successful_queries = 0
                
                # Query each metric
                for metric in metric_names:
                    try:
                        # Build PromQL query based on aggregation
                        if aggregation == "rate":
                            query = f"rate({metric}{{service=\"{service_name}\"}}[{duration}])"
                        else:
                            query = f"{aggregation}({metric}{{service=\"{service_name}\"}}) by (instance)"
                        
                        # Simulate Prometheus query (in real implementation, use requests)
                        # async with aiohttp.ClientSession() as session:
                        #     params = {"query": query, "time": int(time.time())}
                        #     async with session.get(f"{prometheus_url}/api/v1/query", params=params) as resp:
                        #         data = await resp.json()
                        
                        # For demo, generate sample data
                        current_time = time.time()
                        sample_value = 50 + (hash(metric) % 50)  # Generate consistent sample data
                        
                        metrics_data[metric] = {
                            "query": query,
                            "value": sample_value,
                            "timestamp": current_time,
                            "unit": self._get_metric_unit(metric),
                            "status": "healthy" if sample_value < 80 else "warning"
                        }
                        successful_queries += 1
                        
                    except Exception as me:
                        metrics_data[metric] = {
                            "error": str(me),
                            "status": "unavailable"
                        }
                
                # Store metrics query in hAIveMind
                await self._store_memory(
                    content=f"Metrics queried for {service_name}: {', '.join(metric_names)}",
                    category="monitoring",
                    context=f"Retrieved {successful_queries}/{len(metric_names)} metrics over {duration}",
                    metadata={
                        "service_name": service_name,
                        "metrics": metrics_data,
                        "time_range": duration,
                        "aggregation": aggregation,
                        "successful_queries": successful_queries
                    },
                    tags=["metrics", "prometheus", "monitoring", service_name]
                )
                
                # Format output
                output = f"""ðŸ“Š **Metrics for {service_name}** (Last {duration})

ðŸŽ¯ **Query Summary:**
â€¢ **Successful**: {successful_queries}/{len(metric_names)} metrics
â€¢ **Time Range**: {duration}
â€¢ **Aggregation**: {aggregation}
â€¢ **Prometheus URL**: {prometheus_url}

ðŸ“ˆ **Metric Values:**"""
                
                for metric, data in metrics_data.items():
                    if "error" not in data:
                        status_icon = "âœ…" if data["status"] == "healthy" else "âš ï¸"
                        output += f"""
  {status_icon} **{metric}**: {data['value']:.2f} {data['unit']}
     Query: `{data['query']}`"""
                    else:
                        output += f"""
  âŒ **{metric}**: Error - {data['error']}"""
                
                output += f"\n\nðŸ·ï¸ Metrics data stored in hAIveMind monitoring memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Metrics retrieval failed: {str(e)}"
        
        def _get_metric_unit(self, metric_name: str) -> str:
            """Get appropriate unit for metric"""
            unit_mapping = {
                "cpu_usage_percent": "%",
                "memory_usage_percent": "%", 
                "disk_usage_percent": "%",
                "response_time_seconds": "s",
                "http_requests_total": "req/s",
                "bytes_sent": "B",
                "up": "bool"
            }
            return unit_mapping.get(metric_name, "")

        # # @self.mcp.tool()
        async def correlate_events(
            incident_id: Optional[str] = None,
            time_window: str = "1h",
            services: Optional[List[str]] = None,
            event_types: Optional[List[str]] = None,
            correlation_threshold: float = 0.7
        ) -> str:
            """Correlate incidents with metrics and logs across multiple monitoring systems"""
            try:
                # Generate incident ID if not provided
                if not incident_id:
                    incident_id = f"incident_{int(time.time())}"
                
                # Default event types
                if not event_types:
                    event_types = ["alerts", "deployments", "config_changes", "service_restarts"]
                
                # Time window parsing
                window_seconds = self._parse_time_window(time_window)
                start_time = time.time() - window_seconds
                
                correlation_data = {
                    "incident_id": incident_id,
                    "time_window": time_window,
                    "start_time": start_time,
                    "end_time": time.time(),
                    "correlations": []
                }
                
                # Correlate with hAIveMind memories
                try:
                    # Search for related events in memory
                    related_memories = []
                    for event_type in event_types:
                        memories = await self._search_memories(
                            query=f"{event_type} {' '.join(services or [])}",
                            limit=20,
                            category="monitoring"
                        )
                        related_memories.extend(memories.get('memories', []))
                    
                    # Analyze correlations
                    correlations = []
                    for memory in related_memories[-10:]:  # Limit to recent 10
                        # Calculate correlation score based on time proximity and content similarity
                        memory_time = memory.get('metadata', {}).get('timestamp', start_time)
                        time_proximity = 1.0 - abs(memory_time - start_time) / window_seconds
                        
                        if time_proximity > correlation_threshold:
                            correlations.append({
                                "event_type": memory.get('category', 'unknown'),
                                "description": memory.get('content', '')[:100],
                                "timestamp": memory_time,
                                "correlation_score": time_proximity,
                                "metadata": memory.get('metadata', {})
                            })
                    
                    correlation_data["correlations"] = correlations
                    
                except Exception as me:
                    correlation_data["memory_error"] = str(me)
                
                # Mock additional correlations (in real implementation, query actual systems)
                mock_correlations = [
                    {
                        "event_type": "deployment",
                        "description": f"Service deployment to {services[0] if services else 'unknown'} at {datetime.fromtimestamp(start_time + 300).strftime('%H:%M:%S')}",
                        "timestamp": start_time + 300,
                        "correlation_score": 0.85,
                        "source": "deployment_system"
                    },
                    {
                        "event_type": "alert",
                        "description": f"High CPU alert triggered for {services[0] if services else 'service'}",
                        "timestamp": start_time + 600,
                        "correlation_score": 0.92,
                        "source": "prometheus"
                    }
                ]
                correlation_data["correlations"].extend(mock_correlations)
                
                # Sort by correlation score
                correlation_data["correlations"].sort(key=lambda x: x["correlation_score"], reverse=True)
                
                # Store correlation analysis
                await self._store_memory(
                    content=f"Event correlation analysis for incident {incident_id}",
                    category="monitoring",
                    context=f"Found {len(correlation_data['correlations'])} correlated events in {time_window} window",
                    metadata=correlation_data,
                    tags=["correlation", "incident", "analysis", incident_id]
                )
                
                # Format output
                output = f"""ðŸ”— **Event Correlation Analysis**

ðŸŽ¯ **Incident**: {incident_id}
ðŸ“… **Time Window**: {time_window} ({datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M')} - {datetime.fromtimestamp(time.time()).strftime('%H:%M')})
ðŸ” **Services**: {', '.join(services) if services else 'All'}
ðŸ“Š **Correlations Found**: {len(correlation_data['correlations'])}

ðŸ”— **Correlated Events** (by relevance):"""
                
                for i, corr in enumerate(correlation_data["correlations"][:5], 1):
                    score_icon = "ðŸ”¥" if corr["correlation_score"] > 0.9 else "âš¡" if corr["correlation_score"] > 0.7 else "ðŸ’¡"
                    timestamp_str = datetime.fromtimestamp(corr["timestamp"]).strftime('%H:%M:%S')
                    output += f"""
{i}. {score_icon} **{corr['event_type'].title()}** (Score: {corr['correlation_score']:.2f})
   ðŸ“… {timestamp_str} - {corr['description']}
   ðŸ·ï¸ Source: {corr.get('source', 'hAIveMind')}"""
                
                if len(correlation_data["correlations"]) > 5:
                    output += f"\n\n... and {len(correlation_data['correlations']) - 5} more events"
                
                output += f"\n\nðŸ·ï¸ Correlation analysis stored in hAIveMind memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Event correlation failed: {str(e)}"
        
        def _parse_time_window(self, time_window: str) -> int:
            """Parse time window string to seconds"""
            mapping = {
                "5m": 300, "15m": 900, "30m": 1800,
                "1h": 3600, "6h": 21600, "12h": 43200,
                "1d": 86400, "7d": 604800
            }
            return mapping.get(time_window, 3600)

        # # @self.mcp.tool()
        async def predictive_analysis(
            service_name: str,
            metrics: Optional[List[str]] = None,
            prediction_horizon: str = "1h",
            model_type: str = "trend",  # trend, anomaly, seasonal
            confidence_threshold: float = 0.8
        ) -> str:
            """Predict issues before they occur using machine learning analysis"""
            try:
                # Default metrics for prediction
                if not metrics:
                    metrics = ["cpu_usage_percent", "memory_usage_percent", "response_time_seconds", "error_rate"]
                
                prediction_data = {
                    "service_name": service_name,
                    "prediction_horizon": prediction_horizon,
                    "model_type": model_type,
                    "metrics_analyzed": metrics,
                    "analysis_timestamp": time.time(),
                    "predictions": []
                }
                
                # Mock ML analysis (in real implementation, use scikit-learn, TensorFlow, etc.)
                for metric in metrics:
                    # Generate sample prediction data
                    current_value = 50 + (hash(f"{service_name}_{metric}") % 40)
                    trend = 0.1 * (hash(metric) % 20 - 10)  # -1.0 to 1.0 trend
                    predicted_value = current_value + (trend * 10)  # Amplify trend for prediction
                    
                    # Calculate risk level
                    risk_level = "low"
                    risk_score = 0.1
                    
                    if predicted_value > 85:
                        risk_level = "critical"
                        risk_score = 0.95
                    elif predicted_value > 70:
                        risk_level = "high"
                        risk_score = 0.8
                    elif predicted_value > 60:
                        risk_level = "medium"
                        risk_score = 0.5
                    
                    confidence = min(0.95, confidence_threshold + 0.1 + (abs(trend) * 0.1))
                    
                    prediction = {
                        "metric": metric,
                        "current_value": current_value,
                        "predicted_value": predicted_value,
                        "trend": "increasing" if trend > 0 else "decreasing" if trend < 0 else "stable",
                        "trend_magnitude": abs(trend),
                        "risk_level": risk_level,
                        "risk_score": risk_score,
                        "confidence": confidence,
                        "recommended_actions": self._get_prediction_recommendations(metric, risk_level, predicted_value)
                    }
                    
                    prediction_data["predictions"].append(prediction)
                
                # Calculate overall service health prediction
                avg_risk = sum(p["risk_score"] for p in prediction_data["predictions"]) / len(prediction_data["predictions"])
                overall_status = "healthy"
                if avg_risk > 0.8:
                    overall_status = "critical"
                elif avg_risk > 0.6:
                    overall_status = "warning"
                elif avg_risk > 0.3:
                    overall_status = "attention"
                
                prediction_data["overall_risk_score"] = avg_risk
                prediction_data["overall_status"] = overall_status
                
                # Store predictive analysis
                await self._store_memory(
                    content=f"Predictive analysis for {service_name}: {overall_status} status predicted",
                    category="monitoring",
                    context=f"Analyzed {len(metrics)} metrics, overall risk: {avg_risk:.2f}",
                    metadata=prediction_data,
                    tags=["prediction", "ml", "monitoring", service_name, overall_status]
                )
                
                # Format output
                status_icons = {
                    "healthy": "âœ…",
                    "attention": "âš ï¸", 
                    "warning": "ðŸŸ¡",
                    "critical": "ðŸ”´"
                }
                
                risk_icons = {
                    "low": "ðŸ’š",
                    "medium": "ðŸŸ¡", 
                    "high": "ðŸŸ ",
                    "critical": "ðŸ”´"
                }
                
                output = f"""ðŸ”® **Predictive Analysis for {service_name}**

ðŸ“Š **Overall Prediction** ({prediction_horizon} ahead):
{status_icons.get(overall_status, 'â“')} **Status**: {overall_status.title()}
ðŸ“ˆ **Risk Score**: {avg_risk:.2f}/1.0
ðŸŽ¯ **Model Type**: {model_type}
ðŸ”¬ **Metrics Analyzed**: {len(metrics)}

ðŸ“ˆ **Metric Predictions:**"""
                
                for pred in prediction_data["predictions"]:
                    risk_icon = risk_icons.get(pred["risk_level"], "â“")
                    trend_arrow = "ðŸ“ˆ" if pred["trend"] == "increasing" else "ðŸ“‰" if pred["trend"] == "decreasing" else "âž¡ï¸"
                    
                    output += f"""
{risk_icon} **{pred['metric']}**:
   Current: {pred['current_value']:.1f} â†’ Predicted: {pred['predicted_value']:.1f} {trend_arrow}
   Risk: {pred['risk_level'].upper()} ({pred['risk_score']:.2f}) | Confidence: {pred['confidence']:.0%}"""
                    
                    if pred["recommended_actions"]:
                        output += f"\n   ðŸ’¡ Actions: {', '.join(pred['recommended_actions'][:2])}"
                
                # Add overall recommendations
                if avg_risk > 0.6:
                    output += f"\n\nðŸš¨ **Immediate Actions Recommended:**"
                    if avg_risk > 0.8:
                        output += f"\nâ€¢ Consider immediate scaling or resource allocation"
                        output += f"\nâ€¢ Enable enhanced monitoring and alerting"
                        output += f"\nâ€¢ Prepare rollback procedures"
                    else:
                        output += f"\nâ€¢ Monitor closely over next hour"
                        output += f"\nâ€¢ Review resource utilization patterns"
                        output += f"\nâ€¢ Consider proactive scaling"
                
                output += f"\n\nðŸ·ï¸ Predictive analysis stored in hAIveMind memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Predictive analysis failed: {str(e)}"
        
        def _get_prediction_recommendations(self, metric: str, risk_level: str, predicted_value: float) -> List[str]:
            """Get recommended actions based on metric predictions"""
            recommendations = []
            
            if metric == "cpu_usage_percent":
                if risk_level in ["high", "critical"]:
                    recommendations.extend(["Scale horizontally", "Optimize CPU-intensive processes", "Add CPU limits"])
                elif risk_level == "medium":
                    recommendations.extend(["Monitor CPU patterns", "Review recent deployments"])
            
            elif metric == "memory_usage_percent":
                if risk_level in ["high", "critical"]:
                    recommendations.extend(["Increase memory allocation", "Check for memory leaks", "Restart services"])
                elif risk_level == "medium":
                    recommendations.extend(["Monitor memory growth", "Analyze heap dumps"])
            
            elif metric == "response_time_seconds":
                if risk_level in ["high", "critical"]:
                    recommendations.extend(["Scale backend services", "Add caching layer", "Optimize database queries"])
                elif risk_level == "medium":
                    recommendations.extend(["Profile slow endpoints", "Check network latency"])
            
            elif metric == "error_rate":
                if risk_level in ["high", "critical"]:
                    recommendations.extend(["Check recent deployments", "Review error logs", "Enable circuit breakers"])
                elif risk_level == "medium":
                    recommendations.extend(["Analyze error patterns", "Increase log verbosity"])
            
            return recommendations[:3]  # Limit to top 3 recommendations

        # # @self.mcp.tool()
        async def backup_monitoring_rules(
            backup_name: Optional[str] = None,
            include_prometheus: bool = True,
            include_grafana: bool = True,
            include_alertmanager: bool = True,
            compression_enabled: bool = True,
            encryption_enabled: bool = True
        ) -> str:
            """Backup alert rules and dashboards for disaster recovery"""
            try:
                backup_id = backup_name or f"monitoring_backup_{int(time.time())}"
                
                backup_data = {
                    "backup_id": backup_id,
                    "timestamp": time.time(),
                    "machine_id": self.machine_id,
                    "components": [],
                    "files_backed_up": 0,
                    "total_size_bytes": 0
                }
                
                # Backup Prometheus rules
                if include_prometheus:
                    try:
                        # In real implementation, read actual Prometheus rules
                        prometheus_rules = {
                            "groups": [
                                {
                                    "name": "service_alerts",
                                    "rules": [
                                        {
                                            "alert": "HighCPUUsage",
                                            "expr": "cpu_usage_percent > 80",
                                            "for": "5m",
                                            "labels": {"severity": "warning"},
                                            "annotations": {"summary": "High CPU usage detected"}
                                        }
                                    ]
                                }
                            ]
                        }
                        
                        backup_data["components"].append({
                            "type": "prometheus",
                            "rules_count": len(prometheus_rules["groups"][0]["rules"]),
                            "groups_count": len(prometheus_rules["groups"]),
                            "data": prometheus_rules
                        })
                        backup_data["files_backed_up"] += 1
                        
                    except Exception as pe:
                        backup_data["prometheus_error"] = str(pe)
                
                # Backup Grafana dashboards
                if include_grafana:
                    try:
                        # Mock Grafana dashboard backup
                        grafana_dashboards = [
                            {
                                "dashboard": {
                                    "id": 1,
                                    "title": "Service Overview",
                                    "tags": ["monitoring", "overview"],
                                    "panels": [
                                        {"title": "CPU Usage", "type": "graph"},
                                        {"title": "Memory Usage", "type": "graph"},
                                        {"title": "Response Time", "type": "graph"}
                                    ]
                                }
                            }
                        ]
                        
                        backup_data["components"].append({
                            "type": "grafana",
                            "dashboards_count": len(grafana_dashboards),
                            "data": grafana_dashboards
                        })
                        backup_data["files_backed_up"] += len(grafana_dashboards)
                        
                    except Exception as ge:
                        backup_data["grafana_error"] = str(ge)
                
                # Backup AlertManager config
                if include_alertmanager:
                    try:
                        alertmanager_config = {
                            "global": {
                                "smtp_smarthost": "localhost:587"
                            },
                            "route": {
                                "group_by": ["alertname"],
                                "receiver": "default"
                            },
                            "receivers": [
                                {
                                    "name": "default",
                                    "email_configs": [
                                        {"to": "admin@example.com"}
                                    ]
                                }
                            ]
                        }
                        
                        backup_data["components"].append({
                            "type": "alertmanager",
                            "receivers_count": len(alertmanager_config["receivers"]),
                            "data": alertmanager_config
                        })
                        backup_data["files_backed_up"] += 1
                        
                    except Exception as ae:
                        backup_data["alertmanager_error"] = str(ae)
                
                # Calculate backup size (mock)
                backup_data["total_size_bytes"] = len(json.dumps(backup_data)) * 2  # Rough estimate
                
                # Apply compression and encryption (mock)
                if compression_enabled:
                    backup_data["compressed"] = True
                    backup_data["compression_ratio"] = 0.3  # 70% size reduction
                    backup_data["total_size_bytes"] = int(backup_data["total_size_bytes"] * 0.3)
                
                if encryption_enabled:
                    backup_data["encrypted"] = True
                    backup_data["encryption_method"] = "AES-256"
                
                # Store backup record
                await self._store_memory(
                    content=f"Monitoring configuration backup: {backup_id}",
                    category="monitoring",
                    context=f"Backed up {len(backup_data['components'])} components, {backup_data['files_backed_up']} files",
                    metadata=backup_data,
                    tags=["backup", "monitoring", "disaster_recovery", backup_id]
                )
                
                # Format output
                output = f"""ðŸ’¾ **Monitoring Configuration Backup Complete**

ðŸ“¦ **Backup Details:**
â€¢ **Backup ID**: {backup_id}
â€¢ **Timestamp**: {datetime.fromtimestamp(backup_data['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}
â€¢ **Components**: {len(backup_data['components'])}
â€¢ **Files Backed Up**: {backup_data['files_backed_up']}
â€¢ **Total Size**: {backup_data['total_size_bytes']/1024:.1f} KB

ðŸ”§ **Components Backed Up:**"""
                
                for component in backup_data["components"]:
                    if component["type"] == "prometheus":
                        output += f"""
âœ… **Prometheus Rules**:
   â€¢ Rules: {component['rules_count']}
   â€¢ Groups: {component['groups_count']}"""
                    
                    elif component["type"] == "grafana":
                        output += f"""
âœ… **Grafana Dashboards**:
   â€¢ Dashboards: {component['dashboards_count']}"""
                    
                    elif component["type"] == "alertmanager":
                        output += f"""
âœ… **AlertManager Config**:
   â€¢ Receivers: {component['receivers_count']}"""
                
                # Show processing options
                processing = []
                if backup_data.get("compressed"):
                    processing.append(f"Compressed (70% reduction)")
                if backup_data.get("encrypted"):
                    processing.append(f"Encrypted ({backup_data['encryption_method']})")
                
                if processing:
                    output += f"\n\nðŸ”’ **Processing Applied**: {', '.join(processing)}"
                
                output += f"\n\nðŸ“ **Backup Location**: data/backups/monitoring/{backup_id}"
                output += f"\nðŸ·ï¸ Backup record stored in hAIveMind memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Monitoring backup failed: {str(e)}"

        logger.info("ðŸ“Š Monitoring integration tools registered - Prometheus/Grafana integration enabled")

    def _register_deployment_pipeline_tools(self):
        """Register comprehensive deployment pipeline and automation tools"""
        import json
        import yaml
        from datetime import datetime
        import subprocess
        import tempfile
        import hashlib
        from pathlib import Path
        
        # # @self.mcp.tool()
        async def create_deployment_pipeline(
            pipeline_name: str,
            service_name: str,
            stages: Optional[List[str]] = None,
            deployment_strategy: str = "rolling",  # rolling, blue_green, canary
            approval_required: bool = True,
            auto_rollback: bool = True,
            health_checks: Optional[List[str]] = None
        ) -> str:
            """Define multi-stage deployment pipelines with approval workflows"""
            try:
                pipeline_id = f"pipeline_{pipeline_name}_{int(time.time())}"
                
                # Default stages if not provided
                if not stages:
                    stages = [
                        "pre_validation",
                        "backup_creation",
                        "deployment",
                        "health_check",
                        "post_validation"
                    ]
                
                # Default health checks
                if not health_checks:
                    health_checks = [
                        "http_endpoint",
                        "service_startup",
                        "dependency_check"
                    ]
                
                # Build pipeline configuration
                pipeline_config = {
                    "pipeline_id": pipeline_id,
                    "pipeline_name": pipeline_name,
                    "service_name": service_name,
                    "deployment_strategy": deployment_strategy,
                    "stages": [],
                    "approval_settings": {
                        "required": approval_required,
                        "approvers": ["admin", "team_lead"],
                        "timeout_minutes": 60
                    },
                    "rollback_settings": {
                        "auto_enabled": auto_rollback,
                        "health_check_failures": 3,
                        "timeout_minutes": 15
                    },
                    "health_checks": health_checks,
                    "created_at": time.time(),
                    "machine_id": self.machine_id
                }
                
                # Configure each stage
                for i, stage_name in enumerate(stages):
                    stage_config = {
                        "name": stage_name,
                        "order": i + 1,
                        "parallel": False,
                        "timeout_minutes": 30,
                        "retry_count": 2,
                        "required": True
                    }
                    
                    # Add stage-specific configurations
                    if stage_name == "pre_validation":
                        stage_config.update({
                            "actions": [
                                "validate_config",
                                "check_dependencies", 
                                "verify_resources"
                            ],
                            "timeout_minutes": 10
                        })
                    
                    elif stage_name == "backup_creation":
                        stage_config.update({
                            "actions": [
                                "create_service_backup",
                                "backup_database",
                                "snapshot_configs"
                            ],
                            "timeout_minutes": 15
                        })
                    
                    elif stage_name == "deployment":
                        stage_config.update({
                            "actions": [
                                f"deploy_{deployment_strategy}",
                                "update_load_balancer",
                                "migrate_traffic"
                            ],
                            "timeout_minutes": 45,
                            "approval_required": approval_required
                        })
                    
                    elif stage_name == "health_check":
                        stage_config.update({
                            "actions": health_checks,
                            "timeout_minutes": 10,
                            "failure_threshold": 2
                        })
                    
                    elif stage_name == "post_validation":
                        stage_config.update({
                            "actions": [
                                "verify_functionality",
                                "performance_check",
                                "cleanup_old_versions"
                            ],
                            "timeout_minutes": 15
                        })
                    
                    pipeline_config["stages"].append(stage_config)
                
                # Store pipeline configuration
                await self._store_memory(
                    content=f"Deployment pipeline created: {pipeline_name} for {service_name}",
                    category="deployments",
                    context=f"{len(stages)} stages, {deployment_strategy} strategy",
                    metadata=pipeline_config,
                    tags=["deployment", "pipeline", "automation", service_name]
                )
                
                # Generate pipeline YAML for CI/CD systems
                pipeline_yaml = yaml.dump(pipeline_config, default_flow_style=False)
                
                return f"""ðŸš€ **Deployment Pipeline Created Successfully**

ðŸ“‹ **Pipeline Details:**
â€¢ **Pipeline ID**: {pipeline_id}
â€¢ **Service**: {service_name}
â€¢ **Strategy**: {deployment_strategy}
â€¢ **Stages**: {len(stages)}
â€¢ **Approval Required**: {'Yes' if approval_required else 'No'}
â€¢ **Auto Rollback**: {'Yes' if auto_rollback else 'No'}

ðŸ”„ **Pipeline Stages:**"""
                
                for stage in pipeline_config["stages"]:
                    approval_icon = "ðŸ”’" if stage.get("approval_required") else ""
                    output += f"""
{stage['order']}. **{stage['name'].replace('_', ' ').title()}** {approval_icon}
   â±ï¸ Timeout: {stage['timeout_minutes']}min | ðŸ”„ Retries: {stage['retry_count']}
   ðŸŽ¯ Actions: {', '.join(stage['actions'][:3])}"""
                
                output += f"""

âš™ï¸ **Health Checks:**
{chr(10).join([f'â€¢ {check.replace("_", " ").title()}' for check in health_checks])}

ðŸ“„ **Pipeline Configuration:**
```yaml
# Save to .ci/pipeline.yml
{pipeline_yaml[:500]}{'...' if len(pipeline_yaml) > 500 else ''}
```

ðŸ·ï¸ Pipeline configuration stored in hAIveMind memory."""
                
                return output
                
            except Exception as e:
                return f"âŒ Pipeline creation failed: {str(e)}"

        # # @self.mcp.tool()
        async def execute_deployment(
            pipeline_id: str,
            version: str,
            environment: str = "production",
            force_approval: bool = False,
            dry_run: bool = False,
            notification_channels: Optional[List[str]] = None
        ) -> str:
            """Execute deployment with safeguards and monitoring"""
            try:
                execution_id = f"deploy_{pipeline_id}_{int(time.time())}"
                
                # Retrieve pipeline configuration
                pipeline_search = await self._search_memories(
                    query=f"pipeline {pipeline_id}",
                    category="deployments",
                    limit=1
                )
                
                if not pipeline_search.get('memories'):
                    return f"âŒ Pipeline {pipeline_id} not found. Create pipeline first."
                
                pipeline_config = pipeline_search['memories'][0].get('metadata', {})
                service_name = pipeline_config.get('service_name', 'unknown')
                
                deployment_data = {
                    "execution_id": execution_id,
                    "pipeline_id": pipeline_id,
                    "service_name": service_name,
                    "version": version,
                    "environment": environment,
                    "start_time": time.time(),
                    "status": "running",
                    "dry_run": dry_run,
                    "stages_completed": 0,
                    "total_stages": len(pipeline_config.get('stages', [])),
                    "current_stage": None,
                    "execution_log": []
                }
                
                # Execute pipeline stages
                for stage in pipeline_config.get('stages', []):
                    stage_name = stage['name']
                    deployment_data["current_stage"] = stage_name
                    
                    stage_start = time.time()
                    stage_result = {
                        "stage": stage_name,
                        "start_time": stage_start,
                        "status": "running",
                        "actions_completed": []
                    }
                    
                    # Check if approval is required
                    if stage.get('approval_required') and not force_approval and not dry_run:
                        stage_result.update({
                            "status": "pending_approval",
                            "approval_required": True,
                            "message": f"Deployment waiting for approval at {stage_name} stage"
                        })
                        deployment_data["execution_log"].append(stage_result)
                        deployment_data["status"] = "pending_approval"
                        break
                    
                    # Execute stage actions
                    for action in stage.get('actions', []):
                        try:
                            if dry_run:
                                action_result = f"DRY RUN: Would execute {action}"
                            else:
                                # Mock action execution
                                if action == "validate_config":
                                    action_result = f"Configuration validated for {service_name}"
                                elif action == "create_service_backup":
                                    backup_id = f"backup_{service_name}_{int(time.time())}"
                                    action_result = f"Backup created: {backup_id}"
                                elif action.startswith("deploy_"):
                                    strategy = action.replace("deploy_", "")
                                    action_result = f"Deployed {version} using {strategy} strategy"
                                elif "health_check" in action:
                                    action_result = f"Health check passed for {action}"
                                else:
                                    action_result = f"Executed {action} successfully"
                            
                            stage_result["actions_completed"].append({
                                "action": action,
                                "result": action_result,
                                "success": True
                            })
                            
                        except Exception as ae:
                            stage_result["actions_completed"].append({
                                "action": action,
                                "error": str(ae),
                                "success": False
                            })
                            
                            # Handle stage failure
                            stage_result["status"] = "failed"
                            deployment_data["status"] = "failed"
                            deployment_data["failure_stage"] = stage_name
                            break
                    
                    # Complete stage if no failures
                    if stage_result["status"] != "failed":
                        stage_result["status"] = "completed"
                        stage_result["duration"] = time.time() - stage_start
                        deployment_data["stages_completed"] += 1
                    
                    deployment_data["execution_log"].append(stage_result)
                    
                    # Exit on failure
                    if stage_result["status"] == "failed":
                        break
                
                # Update final status
                if deployment_data["status"] == "running":
                    if deployment_data["stages_completed"] == deployment_data["total_stages"]:
                        deployment_data["status"] = "completed"
                    else:
                        deployment_data["status"] = "partial"
                
                deployment_data["end_time"] = time.time()
                deployment_data["total_duration"] = deployment_data["end_time"] - deployment_data["start_time"]
                
                # Store deployment execution
                await self._store_memory(
                    content=f"Deployment executed: {service_name} v{version} to {environment}",
                    category="deployments",
                    context=f"Status: {deployment_data['status']}, {deployment_data['stages_completed']}/{deployment_data['total_stages']} stages",
                    metadata=deployment_data,
                    tags=["deployment", "execution", service_name, environment, deployment_data["status"]]
                )
                
                # Format output
                status_icons = {
                    "completed": "âœ…",
                    "failed": "âŒ",
                    "pending_approval": "â³",
                    "partial": "âš ï¸",
                    "running": "ðŸ”„"
                }
                
                output = f"""ðŸš€ **Deployment Execution Report**

ðŸ“¦ **Deployment Details:**
{status_icons.get(deployment_data['status'], 'â“')} **Status**: {deployment_data['status'].replace('_', ' ').title()}
â€¢ **Execution ID**: {execution_id}
â€¢ **Service**: {service_name} v{version}
â€¢ **Environment**: {environment}
â€¢ **Duration**: {deployment_data.get('total_duration', 0):.1f}s
â€¢ **Stages**: {deployment_data['stages_completed']}/{deployment_data['total_stages']}

ðŸ“‹ **Execution Log:**"""
                
                for log_entry in deployment_data["execution_log"]:
                    stage_icon = "âœ…" if log_entry["status"] == "completed" else "âŒ" if log_entry["status"] == "failed" else "â³"
                    output += f"""
{stage_icon} **{log_entry['stage'].replace('_', ' ').title()}**"""
                    
                    if log_entry["status"] == "completed":
                        output += f" ({log_entry.get('duration', 0):.1f}s)"
                        output += f"\n   ðŸ“ Actions: {len(log_entry['actions_completed'])} completed"
                    elif log_entry["status"] == "failed":
                        failed_actions = [a for a in log_entry['actions_completed'] if not a['success']]
                        output += f"\n   âŒ Failed at: {failed_actions[0]['action'] if failed_actions else 'Unknown'}"
                    elif log_entry["status"] == "pending_approval":
                        output += f"\n   â³ {log_entry.get('message', 'Waiting for approval')}"
                
                if deployment_data["status"] == "pending_approval":
                    output += f"\n\nðŸ”’ **Next Steps:**"
                    output += f"\nâ€¢ Use `deployment_approval_workflow` to approve/reject"
                    output += f"\nâ€¢ Deployment will resume after approval"
                elif deployment_data["status"] == "failed":
                    output += f"\n\nðŸ”„ **Rollback Options:**"
                    output += f"\nâ€¢ Use `rollback_deployment` to restore previous version"
                    output += f"\nâ€¢ Check logs and fix issues before retrying"
                
                output += f"\n\nðŸ·ï¸ Deployment execution logged in hAIveMind memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Deployment execution failed: {str(e)}"

        # # @self.mcp.tool()
        async def rollback_deployment(
            service_name: str,
            target_version: Optional[str] = None,
            backup_id: Optional[str] = None,
            reason: str = "Manual rollback",
            fast_rollback: bool = False
        ) -> str:
            """Automated rollback with state preservation and validation"""
            try:
                rollback_id = f"rollback_{service_name}_{int(time.time())}"
                
                # Search for recent deployment to rollback from
                recent_deployments = await self._search_memories(
                    query=f"deployment {service_name}",
                    category="deployments",
                    limit=5
                )
                
                current_deployment = None
                if recent_deployments.get('memories'):
                    for memory in recent_deployments['memories']:
                        if memory.get('metadata', {}).get('status') == 'completed':
                            current_deployment = memory.get('metadata', {})
                            break
                
                rollback_data = {
                    "rollback_id": rollback_id,
                    "service_name": service_name,
                    "reason": reason,
                    "fast_rollback": fast_rollback,
                    "start_time": time.time(),
                    "status": "running",
                    "rollback_steps": []
                }
                
                if current_deployment:
                    rollback_data["from_version"] = current_deployment.get('version', 'unknown')
                    rollback_data["current_execution_id"] = current_deployment.get('execution_id')
                
                # Determine target version
                if not target_version and current_deployment:
                    # Find previous successful deployment
                    for memory in recent_deployments['memories'][1:]:
                        if memory.get('metadata', {}).get('status') == 'completed':
                            target_version = memory.get('metadata', {}).get('version', 'previous')
                            break
                
                rollback_data["target_version"] = target_version or "previous"
                
                # Execute rollback steps
                rollback_steps = [
                    "validate_rollback_target",
                    "create_rollback_backup", 
                    "stop_current_version",
                    "restore_previous_version",
                    "verify_rollback_health",
                    "update_load_balancer",
                    "cleanup_failed_deployment"
                ]
                
                if fast_rollback:
                    rollback_steps = ["stop_current_version", "restore_previous_version", "verify_rollback_health"]
                
                for step in rollback_steps:
                    step_start = time.time()
                    step_result = {
                        "step": step,
                        "start_time": step_start,
                        "status": "running"
                    }
                    
                    try:
                        # Mock rollback step execution
                        if step == "validate_rollback_target":
                            step_result["message"] = f"Validated rollback target: {target_version}"
                        elif step == "create_rollback_backup":
                            backup_id = f"rollback_backup_{service_name}_{int(time.time())}"
                            step_result["message"] = f"Created rollback backup: {backup_id}"
                        elif step == "stop_current_version":
                            step_result["message"] = f"Stopped current version gracefully"
                        elif step == "restore_previous_version":
                            step_result["message"] = f"Restored to version: {target_version}"
                        elif step == "verify_rollback_health":
                            step_result["message"] = f"Health checks passed for rolled-back version"
                        elif step == "update_load_balancer":
                            step_result["message"] = f"Load balancer updated to route to {target_version}"
                        elif step == "cleanup_failed_deployment":
                            step_result["message"] = f"Cleaned up failed deployment artifacts"
                        
                        step_result["status"] = "completed"
                        step_result["duration"] = time.time() - step_start
                        
                    except Exception as se:
                        step_result["status"] = "failed"
                        step_result["error"] = str(se)
                        rollback_data["status"] = "failed"
                        rollback_data["failed_step"] = step
                        break
                    
                    rollback_data["rollback_steps"].append(step_result)
                
                # Update final status
                if rollback_data["status"] == "running":
                    rollback_data["status"] = "completed"
                
                rollback_data["end_time"] = time.time()
                rollback_data["total_duration"] = rollback_data["end_time"] - rollback_data["start_time"]
                
                # Store rollback execution
                await self._store_memory(
                    content=f"Rollback executed: {service_name} to {target_version}",
                    category="deployments", 
                    context=f"Status: {rollback_data['status']}, reason: {reason}",
                    metadata=rollback_data,
                    tags=["rollback", "deployment", service_name, rollback_data["status"]]
                )
                
                # Format output
                status_icon = "âœ…" if rollback_data["status"] == "completed" else "âŒ" if rollback_data["status"] == "failed" else "ðŸ”„"
                
                output = f"""ðŸ”„ **Deployment Rollback Report**

{status_icon} **Rollback Status**: {rollback_data['status'].title()}
â€¢ **Rollback ID**: {rollback_id}
â€¢ **Service**: {service_name}
â€¢ **From Version**: {rollback_data.get('from_version', 'unknown')}
â€¢ **To Version**: {target_version}
â€¢ **Duration**: {rollback_data.get('total_duration', 0):.1f}s
â€¢ **Reason**: {reason}

ðŸ”„ **Rollback Steps:**"""
                
                for step in rollback_data["rollback_steps"]:
                    step_icon = "âœ…" if step["status"] == "completed" else "âŒ" if step["status"] == "failed" else "ðŸ”„"
                    output += f"""
{step_icon} **{step['step'].replace('_', ' ').title()}**"""
                    
                    if step["status"] == "completed":
                        output += f" ({step.get('duration', 0):.1f}s)"
                        if step.get('message'):
                            output += f"\n   ðŸ“ {step['message']}"
                    elif step["status"] == "failed":
                        output += f"\n   âŒ Error: {step.get('error', 'Unknown error')}"
                
                if rollback_data["status"] == "completed":
                    output += f"\n\nâœ… **Rollback Successful:**"
                    output += f"\nâ€¢ Service is now running version {target_version}"
                    output += f"\nâ€¢ All health checks passed"
                    output += f"\nâ€¢ Traffic routing updated"
                elif rollback_data["status"] == "failed":
                    output += f"\n\nâŒ **Rollback Failed:**"
                    output += f"\nâ€¢ Failed at step: {rollback_data.get('failed_step', 'unknown')}"
                    output += f"\nâ€¢ Manual intervention may be required"
                    output += f"\nâ€¢ Check service status and logs"
                
                output += f"\n\nðŸ·ï¸ Rollback execution logged in hAIveMind memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Rollback execution failed: {str(e)}"

        # # @self.mcp.tool()
        async def deployment_approval_workflow(
            execution_id: str,
            action: str = "approve",  # approve, reject
            approver: str = "admin",
            comments: Optional[str] = None
        ) -> str:
            """Multi-stage approval workflow for deployments"""
            try:
                # Search for pending deployment
                deployment_search = await self._search_memories(
                    query=f"execution {execution_id}",
                    category="deployments",
                    limit=1
                )
                
                if not deployment_search.get('memories'):
                    return f"âŒ Deployment execution {execution_id} not found."
                
                deployment_data = deployment_search['memories'][0].get('metadata', {})
                
                if deployment_data.get('status') != 'pending_approval':
                    return f"âŒ Deployment {execution_id} is not pending approval. Status: {deployment_data.get('status')}"
                
                approval_data = {
                    "execution_id": execution_id,
                    "action": action,
                    "approver": approver,
                    "comments": comments,
                    "timestamp": time.time(),
                    "service_name": deployment_data.get('service_name'),
                    "version": deployment_data.get('version'),
                    "current_stage": deployment_data.get('current_stage')
                }
                
                if action == "approve":
                    # Continue deployment from where it left off
                    approval_data["result"] = "approved"
                    approval_data["next_action"] = "continue_deployment"
                    
                    # Update deployment status
                    deployment_data["status"] = "running"
                    deployment_data["approvals"] = deployment_data.get('approvals', [])
                    deployment_data["approvals"].append(approval_data)
                    
                    status_message = f"âœ… **Deployment Approved**\n\nâ€¢ **Service**: {deployment_data['service_name']} v{deployment_data['version']}\nâ€¢ **Stage**: {deployment_data['current_stage']}\nâ€¢ **Approver**: {approver}\nâ€¢ **Comments**: {comments or 'No comments'}\n\nðŸš€ **Next Steps**:\nâ€¢ Deployment will continue automatically\nâ€¢ Monitor progress with deployment status tools\nâ€¢ Rollback available if issues occur"
                
                elif action == "reject":
                    approval_data["result"] = "rejected" 
                    approval_data["next_action"] = "cancel_deployment"
                    
                    # Update deployment status
                    deployment_data["status"] = "cancelled"
                    deployment_data["cancellation_reason"] = comments or "Deployment rejected by approver"
                    deployment_data["approvals"] = deployment_data.get('approvals', [])
                    deployment_data["approvals"].append(approval_data)
                    
                    status_message = f"âŒ **Deployment Rejected**\n\nâ€¢ **Service**: {deployment_data['service_name']} v{deployment_data['version']}\nâ€¢ **Stage**: {deployment_data['current_stage']}\nâ€¢ **Approver**: {approver}\nâ€¢ **Reason**: {comments or 'No reason provided'}\n\nðŸ”„ **Next Steps**:\nâ€¢ Deployment has been cancelled\nâ€¢ Address concerns and resubmit if needed\nâ€¢ Previous version remains active"
                
                else:
                    return f"âŒ Invalid action: {action}. Use 'approve' or 'reject'."
                
                # Store approval decision
                await self._store_memory(
                    content=f"Deployment {action}d: {deployment_data['service_name']} v{deployment_data['version']}",
                    category="deployments",
                    context=f"Approver: {approver}, Stage: {deployment_data['current_stage']}",
                    metadata=approval_data,
                    tags=["approval", "deployment", action, approver]
                )
                
                # Update original deployment memory
                await self._store_memory(
                    content=f"Deployment status updated: {deployment_data['service_name']} - {deployment_data['status']}",
                    category="deployments",
                    context=f"Updated after {action} by {approver}",
                    metadata=deployment_data,
                    tags=["deployment", "status_update", deployment_data['status']]
                )
                
                return status_message + f"\n\nðŸ·ï¸ Approval decision logged in hAIveMind memory."
                
            except Exception as e:
                return f"âŒ Approval workflow failed: {str(e)}"

        # # @self.mcp.tool()
        async def backup_before_deployment(
            service_name: str,
            deployment_id: str,
            backup_type: str = "full",  # full, incremental, config_only
            include_database: bool = True,
            include_files: bool = True,
            verification_enabled: bool = True
        ) -> str:
            """Automated backup creation before deployments"""
            try:
                backup_id = f"pre_deploy_{service_name}_{int(time.time())}"
                
                backup_data = {
                    "backup_id": backup_id,
                    "service_name": service_name,
                    "deployment_id": deployment_id,
                    "backup_type": backup_type,
                    "timestamp": time.time(),
                    "machine_id": self.machine_id,
                    "components": [],
                    "total_size_bytes": 0,
                    "status": "running"
                }
                
                # Service configuration backup
                try:
                    config_backup = {
                        "type": "configuration",
                        "files_backed_up": 5,  # Mock count
                        "size_bytes": 1024 * 50,  # 50KB
                        "paths": [
                            f"/etc/{service_name}/",
                            f"/opt/{service_name}/config/",
                            "/etc/nginx/sites-available/",
                            "/etc/systemd/system/",
                            "/var/log/rotation.conf"
                        ]
                    }
                    backup_data["components"].append(config_backup)
                    backup_data["total_size_bytes"] += config_backup["size_bytes"]
                    
                except Exception as ce:
                    backup_data["config_error"] = str(ce)
                
                # Database backup
                if include_database:
                    try:
                        db_backup = {
                            "type": "database",
                            "databases": [f"{service_name}_db", f"{service_name}_cache"],
                            "size_bytes": 1024 * 1024 * 100,  # 100MB
                            "dump_format": "sql",
                            "compression": "gzip"
                        }
                        backup_data["components"].append(db_backup)
                        backup_data["total_size_bytes"] += db_backup["size_bytes"]
                        
                    except Exception as de:
                        backup_data["database_error"] = str(de)
                
                # Application files backup
                if include_files:
                    try:
                        files_backup = {
                            "type": "application_files",
                            "directories": [
                                f"/opt/{service_name}/",
                                f"/var/www/{service_name}/",
                                f"/var/lib/{service_name}/"
                            ],
                            "files_count": 1543,  # Mock count
                            "size_bytes": 1024 * 1024 * 250,  # 250MB
                            "compression": "tar.gz"
                        }
                        backup_data["components"].append(files_backup)
                        backup_data["total_size_bytes"] += files_backup["size_bytes"]
                        
                    except Exception as fe:
                        backup_data["files_error"] = str(fe)
                
                # Container images backup (if applicable)
                try:
                    image_backup = {
                        "type": "container_images",
                        "images": [f"{service_name}:current", f"{service_name}:latest"],
                        "size_bytes": 1024 * 1024 * 500,  # 500MB
                        "format": "docker_export"
                    }
                    backup_data["components"].append(image_backup)
                    backup_data["total_size_bytes"] += image_backup["size_bytes"]
                    
                except Exception as ie:
                    backup_data["image_error"] = str(ie)
                
                # Verification
                if verification_enabled:
                    verification_results = {
                        "backup_integrity": "passed",
                        "file_count_match": True,
                        "checksum_verification": "passed",
                        "restoration_test": "passed"
                    }
                    backup_data["verification"] = verification_results
                
                backup_data["status"] = "completed"
                backup_data["duration"] = time.time() - backup_data["timestamp"]
                
                # Store backup record
                await self._store_memory(
                    content=f"Pre-deployment backup created: {service_name}",
                    category="deployments",
                    context=f"Backup ID: {backup_id}, Size: {backup_data['total_size_bytes']//1024//1024}MB",
                    metadata=backup_data,
                    tags=["backup", "pre_deployment", service_name, backup_type]
                )
                
                # Format output
                total_size_mb = backup_data["total_size_bytes"] // 1024 // 1024
                
                output = f"""ðŸ’¾ **Pre-Deployment Backup Complete**

ðŸ“¦ **Backup Details:**
â€¢ **Backup ID**: {backup_id}
â€¢ **Service**: {service_name}
â€¢ **Type**: {backup_type}
â€¢ **Total Size**: {total_size_mb} MB
â€¢ **Duration**: {backup_data['duration']:.1f}s
â€¢ **Components**: {len(backup_data['components'])}

ðŸ“‹ **Backup Components:**"""
                
                for component in backup_data["components"]:
                    comp_size_mb = component["size_bytes"] // 1024 // 1024
                    output += f"""
âœ… **{component['type'].replace('_', ' ').title()}**: {comp_size_mb} MB"""
                    
                    if component["type"] == "configuration":
                        output += f"\n   ðŸ“ Files: {component['files_backed_up']}"
                    elif component["type"] == "database":
                        output += f"\n   ðŸ—„ï¸ Databases: {', '.join(component['databases'])}"
                    elif component["type"] == "application_files":
                        output += f"\n   ðŸ“„ Files: {component['files_count']:,}"
                    elif component["type"] == "container_images":
                        output += f"\n   ðŸ³ Images: {len(component['images'])}"
                
                if verification_enabled:
                    output += f"\n\nâœ… **Verification Results:**"
                    for check, result in backup_data["verification"].items():
                        check_icon = "âœ…" if result in ["passed", True] else "âŒ"
                        output += f"\n{check_icon} {check.replace('_', ' ').title()}: {result}"
                
                output += f"\n\nðŸ“ **Backup Location**: data/backups/pre_deployment/{backup_id}"
                output += f"\nâ±ï¸ **Retention**: 30 days (configurable)"
                output += f"\nðŸ·ï¸ Backup record stored in hAIveMind memory."
                
                return output
                
            except Exception as e:
                return f"âŒ Pre-deployment backup failed: {str(e)}"

        logger.info("ðŸš€ Deployment pipeline tools registered - CI/CD automation enabled")

    def _register_config_backup_system_tools(self):
        """Register comprehensive configuration backup and tracking tools"""
        from config_backup_system import ConfigBackupSystem, ConfigSnapshot
        import json
        
        # Initialize config backup system
        self.config_backup = ConfigBackupSystem()
        
        # # @self.mcp.tool()
        async def register_config_system(
            system_id: str,
            system_name: str,
            system_type: str,
            agent_id: str = "",
            description: str = "",
            backup_frequency: int = 3600,
            metadata: Optional[Dict[str, Any]] = None
        ) -> str:
            """Register a new system for configuration tracking and backup"""
            try:
                if metadata is None:
                    metadata = {}
                    
                success = self.config_backup.register_system(
                    system_id=system_id,
                    system_name=system_name,
                    system_type=system_type,
                    agent_id=agent_id,
                    description=description,
                    backup_frequency=backup_frequency,
                    metadata=metadata
                )
                
                if success:
                    # Store in hAIveMind memory
                    await self.storage.store_memory(
                        category='infrastructure',
                        content=f"Registered config system {system_name} ({system_type})",
                        metadata={
                            'system_id': system_id,
                            'system_type': system_type,
                            'agent_id': agent_id,
                            'backup_frequency': backup_frequency
                        }
                    )
                    
                    return f"âœ… Successfully registered config system: {system_name} ({system_type})\nSystem ID: {system_id}\nBackup frequency: {backup_frequency}s"
                else:
                    return f"âŒ Failed to register config system: {system_name}"
                    
            except Exception as e:
                return f"âŒ Error registering config system: {str(e)}"
        
        # # @self.mcp.tool()
        async def create_config_snapshot(
            system_id: str,
            config_content: str,
            config_type: str = "config",
            file_path: Optional[str] = None,
            agent_id: str = "",
            metadata: Optional[Dict[str, Any]] = None
        ) -> str:
            """Create a configuration snapshot with automatic deduplication"""
            try:
                if metadata is None:
                    metadata = {}
                    
                snapshot = ConfigSnapshot(
                    system_id=system_id,
                    config_type=config_type,
                    config_content=config_content,
                    file_path=file_path,
                    agent_id=agent_id,
                    metadata=metadata
                )
                
                snapshot_id = self.config_backup.create_snapshot(snapshot)
                
                if snapshot_id:
                    # Store in hAIveMind memory
                    await self.storage.store_memory(
                        category='infrastructure',
                        content=f"Created config snapshot for {system_id}",
                        metadata={
                            'snapshot_id': snapshot_id,
                            'system_id': system_id,
                            'config_type': config_type,
                            'config_size': len(config_content),
                            'config_hash': snapshot.config_hash
                        }
                    )
                    
                    return f"ðŸ“¸ Configuration snapshot created successfully\nSnapshot ID: {snapshot_id}\nSystem: {system_id}\nType: {config_type}\nSize: {len(config_content)} bytes\nHash: {snapshot.config_hash[:12]}..."
                else:
                    return f"âŒ Failed to create configuration snapshot for {system_id}"
                    
            except Exception as e:
                return f"âŒ Error creating config snapshot: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_config_history(
            system_id: str,
            limit: int = 20
        ) -> str:
            """Get configuration history for a system with change tracking"""
            try:
                history = self.config_backup.get_system_history(system_id, limit)
                
                if not history:
                    return f"ðŸ“‹ No configuration history found for system: {system_id}"
                
                result = [f"ðŸ“‹ Configuration History for {system_id}"]
                result.append(f"{'='*60}")
                
                for i, entry in enumerate(history):
                    timestamp = entry.get('timestamp', 'Unknown')
                    config_type = entry.get('config_type', 'config')
                    size = entry.get('size', 0)
                    snapshot_id = entry.get('id', 'N/A')
                    
                    # Change information
                    change_type = entry.get('change_type')
                    risk_score = entry.get('risk_score', 0.0)
                    lines_added = entry.get('lines_added', 0)
                    lines_removed = entry.get('lines_removed', 0)
                    
                    result.append(f"\n{i+1}. Snapshot ID: {snapshot_id}")
                    result.append(f"   ðŸ“… Time: {timestamp}")
                    result.append(f"   ðŸ“„ Type: {config_type}")
                    result.append(f"   ðŸ“Š Size: {size} bytes")
                    
                    if change_type:
                        risk_emoji = "ðŸ”´" if risk_score > 0.7 else "ðŸŸ¡" if risk_score > 0.3 else "ðŸŸ¢"
                        result.append(f"   {risk_emoji} Change: {change_type} (risk: {risk_score:.2f})")
                        if lines_added or lines_removed:
                            result.append(f"   ðŸ“ˆ Lines: +{lines_added}/-{lines_removed}")
                
                # Store query in hAIveMind memory  
                await self.storage.store_memory(
                    category='infrastructure',
                    content=f"Retrieved config history for {system_id}",
                    metadata={
                        'system_id': system_id,
                        'history_count': len(history),
                        'query_limit': limit
                    }
                )
                
                return "\n".join(result)
                
            except Exception as e:
                return f"âŒ Error retrieving config history: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_current_config(
            system_id: str
        ) -> str:
            """Get the current configuration for a system"""
            try:
                current_config = self.config_backup.get_current_config(system_id)
                
                if not current_config:
                    return f"ðŸ“‹ No current configuration found for system: {system_id}"
                
                config_content = current_config.get('config_content', '')
                config_type = current_config.get('config_type', 'config')
                timestamp = current_config.get('timestamp', 'Unknown')
                config_hash = current_config.get('config_hash', '')
                size = current_config.get('size', len(config_content))
                
                result = [
                    f"ðŸ“„ Current Configuration for {system_id}",
                    f"{'='*50}",
                    f"ðŸ“… Last Updated: {timestamp}",
                    f"ðŸ“„ Type: {config_type}",
                    f"ðŸ“Š Size: {size} bytes",
                    f"ðŸ” Hash: {config_hash[:16]}...",
                    f"",
                    f"ðŸ“ Configuration Content:",
                    f"{'â”€'*30}",
                    config_content[:2000] + ("..." if len(config_content) > 2000 else "")
                ]
                
                return "\n".join(result)
                
            except Exception as e:
                return f"âŒ Error retrieving current config: {str(e)}"
        
        # # @self.mcp.tool()
        async def detect_config_drift(
            system_id: Optional[str] = None,
            hours_back: int = 24
        ) -> str:
            """Detect configuration drift and analyze changes"""
            try:
                drift_issues = self.config_backup.detect_drift(system_id, hours_back)
                
                if not drift_issues:
                    scope = f"system {system_id}" if system_id else "all systems"
                    return f"âœ… No configuration drift detected in {scope} (last {hours_back}h)"
                
                result = [f"ðŸ” Configuration Drift Analysis"]
                if system_id:
                    result.append(f"ðŸŽ¯ System: {system_id}")
                result.append(f"â° Time Range: Last {hours_back} hours")
                result.append(f"{'='*60}")
                
                # Group by severity
                critical_issues = [i for i in drift_issues if i.get('analysis', {}).get('severity') == 'critical']
                high_issues = [i for i in drift_issues if i.get('analysis', {}).get('severity') == 'high']  
                medium_issues = [i for i in drift_issues if i.get('analysis', {}).get('severity') == 'medium']
                low_issues = [i for i in drift_issues if i.get('analysis', {}).get('severity') == 'low']
                
                if critical_issues:
                    result.append(f"\nðŸ”´ CRITICAL ISSUES ({len(critical_issues)}):")
                    for issue in critical_issues[:5]:  # Show top 5
                        self._format_drift_issue(result, issue)
                
                if high_issues:
                    result.append(f"\nðŸŸ¡ HIGH PRIORITY ({len(high_issues)}):")
                    for issue in high_issues[:3]:  # Show top 3
                        self._format_drift_issue(result, issue)
                
                if medium_issues:
                    result.append(f"\nðŸŸ  MEDIUM PRIORITY ({len(medium_issues)}):")
                    for issue in medium_issues[:2]:  # Show top 2
                        self._format_drift_issue(result, issue)
                        
                if low_issues:
                    result.append(f"\nðŸŸ¢ LOW PRIORITY: {len(low_issues)} items")
                
                # Store drift analysis in hAIveMind memory
                await self.storage.store_memory(
                    category='infrastructure',
                    content=f"Config drift analysis: {len(drift_issues)} issues detected",
                    metadata={
                        'system_id': system_id,
                        'hours_back': hours_back,
                        'total_issues': len(drift_issues),
                        'critical': len(critical_issues),
                        'high': len(high_issues),
                        'medium': len(medium_issues),
                        'low': len(low_issues)
                    }
                )
                
                return "\n".join(result)
                
            except Exception as e:
                return f"âŒ Error detecting config drift: {str(e)}"
        
        def _format_drift_issue(self, result: List[str], issue: Dict):
            """Format a drift issue for display"""
            system_id = issue.get('system_id', 'Unknown')
            system_name = issue.get('system_name', system_id)
            change_type = issue.get('change_type', 'unknown')
            risk_score = issue.get('risk_score', 0.0)
            lines_added = issue.get('lines_added', 0)
            lines_removed = issue.get('lines_removed', 0)
            timestamp = issue.get('timestamp', 'Unknown')
            analysis = issue.get('analysis', {})
            
            result.append(f"  ðŸ“ {system_name} ({system_id})")
            result.append(f"     ðŸ”„ Change: {change_type} | Risk: {risk_score:.2f}")
            result.append(f"     ðŸ“ˆ Lines: +{lines_added}/-{lines_removed}")
            result.append(f"     ðŸ“… Time: {timestamp}")
            
            if analysis.get('recommendations'):
                result.append(f"     ðŸ’¡ Actions: {', '.join(analysis['recommendations'][:2])}")
        
        # # @self.mcp.tool()
        async def list_config_systems(self) -> str:
            """List all registered configuration systems"""
            try:
                systems = self.config_backup.get_systems()
                
                if not systems:
                    return "ðŸ“‹ No configuration systems registered yet"
                
                result = [
                    f"ðŸ“‹ Registered Configuration Systems ({len(systems)})",
                    f"{'='*60}"
                ]
                
                for system in systems:
                    system_id = system.get('system_id', 'Unknown')
                    system_name = system.get('system_name', 'Unnamed')
                    system_type = system.get('system_type', 'unknown')
                    snapshot_count = system.get('snapshot_count', 0)
                    last_snapshot = system.get('last_snapshot', 'Never')
                    backup_frequency = system.get('backup_frequency', 3600)
                    
                    result.append(f"\nðŸ–¥ï¸  {system_name} ({system_type})")
                    result.append(f"    ðŸ†” ID: {system_id}")
                    result.append(f"    ðŸ“¸ Snapshots: {snapshot_count}")
                    result.append(f"    ðŸ“… Last Backup: {last_snapshot}")
                    result.append(f"    â° Frequency: {backup_frequency}s")
                
                return "\n".join(result)
                
            except Exception as e:
                return f"âŒ Error listing config systems: {str(e)}"
        
        # # @self.mcp.tool() 
        async def get_config_alerts(
            system_id: Optional[str] = None
        ) -> str:
            """Get active configuration drift alerts"""
            try:
                alerts = self.config_backup.get_active_alerts(system_id)
                
                if not alerts:
                    scope = f"system {system_id}" if system_id else "all systems"
                    return f"âœ… No active configuration alerts for {scope}"
                
                result = [f"ðŸš¨ Active Configuration Alerts ({len(alerts)})"]
                if system_id:
                    result.append(f"ðŸŽ¯ System: {system_id}")
                result.append(f"{'='*60}")
                
                # Group by severity
                critical_alerts = [a for a in alerts if a.get('severity') == 'critical']
                high_alerts = [a for a in alerts if a.get('severity') == 'high']
                medium_alerts = [a for a in alerts if a.get('severity') == 'medium']
                low_alerts = [a for a in alerts if a.get('severity') == 'low']
                
                for severity, alert_list, emoji in [
                    ('CRITICAL', critical_alerts, 'ðŸ”´'),
                    ('HIGH', high_alerts, 'ðŸŸ¡'),
                    ('MEDIUM', medium_alerts, 'ðŸŸ '),
                    ('LOW', low_alerts, 'ðŸŸ¢')
                ]:
                    if alert_list:
                        result.append(f"\n{emoji} {severity} ALERTS ({len(alert_list)}):")
                        for alert in alert_list:
                            system_name = alert.get('system_name', alert.get('system_id', 'Unknown'))
                            drift_type = alert.get('drift_type', 'unknown')
                            description = alert.get('description', 'No description')
                            timestamp = alert.get('timestamp', 'Unknown')
                            
                            result.append(f"  ðŸ“ {system_name}")
                            result.append(f"     ðŸ” Type: {drift_type}")
                            result.append(f"     ðŸ“ {description}")
                            result.append(f"     ðŸ“… {timestamp}")
                
                return "\n".join(result)
                
            except Exception as e:
                return f"âŒ Error retrieving config alerts: {str(e)}"

        # # @self.mcp.tool()
        async def analyze_config_drift_patterns(
            system_id: Optional[str] = None,
            hours_back: int = 24,
            include_trends: bool = True
        ) -> str:
            """Advanced configuration drift analysis with pattern recognition and ML insights"""
            try:
                drift_issues = self.config_backup.detect_drift(system_id, hours_back)
                
                if not drift_issues:
                    scope = f"system {system_id}" if system_id else "all systems"
                    return f"âœ… No configuration drift detected in {scope} (last {hours_back}h)"
                
                result = [f"ðŸ” Advanced Configuration Drift Analysis"]
                if system_id:
                    result.append(f"ðŸŽ¯ System: {system_id}")
                result.append(f"â° Analysis Period: Last {hours_back} hours")
                result.append(f"ðŸ§  Pattern Recognition: ENABLED")
                result.append(f"{'='*70}")
                
                # Enhanced analysis with patterns
                total_patterns = 0
                security_issues = 0
                service_issues = 0
                network_issues = 0
                confidence_scores = []
                
                for issue in drift_issues:
                    analysis = self.config_backup._analyze_drift(issue)
                    issue['analysis'] = analysis
                    
                    patterns = analysis.get('patterns_detected', [])
                    total_patterns += len(patterns)
                    confidence_scores.append(analysis.get('confidence', 0.0))
                    
                    for pattern in patterns:
                        if pattern['type'] == 'security_change':
                            security_issues += 1
                        elif pattern['type'] in ['service_disable', 'service_change']:
                            service_issues += 1
                        elif pattern['type'] in ['port_change', 'network_change']:
                            network_issues += 1
                
                # Summary statistics
                avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
                result.append(f"\nðŸ“Š ANALYSIS SUMMARY:")
                result.append(f"   ðŸ”¢ Total Issues: {len(drift_issues)}")
                result.append(f"   ðŸ§© Patterns Detected: {total_patterns}")
                result.append(f"   ðŸŽ¯ Analysis Confidence: {avg_confidence:.1%}")
                result.append(f"   ðŸ”’ Security Issues: {security_issues}")
                result.append(f"   âš™ï¸  Service Issues: {service_issues}")
                result.append(f"   ðŸŒ Network Issues: {network_issues}")
                
                # Categorized issues with enhanced details
                critical_issues = [i for i in drift_issues if i.get('analysis', {}).get('severity') == 'critical']
                high_issues = [i for i in drift_issues if i.get('analysis', {}).get('severity') == 'high']
                
                if critical_issues:
                    result.append(f"\nðŸ”´ CRITICAL ISSUES ({len(critical_issues)}) - IMMEDIATE ACTION REQUIRED:")
                    for issue in critical_issues[:3]:  # Show top 3
                        self._format_enhanced_drift_issue(result, issue)
                
                if high_issues:
                    result.append(f"\nðŸŸ¡ HIGH PRIORITY ISSUES ({len(high_issues)}):")
                    for issue in high_issues[:3]:  # Show top 3
                        self._format_enhanced_drift_issue(result, issue)
                
                # Trend analysis
                if include_trends:
                    trends = self.config_backup.get_drift_trends(system_id, days_back=7)
                    if 'error' not in trends:
                        result.append(f"\nðŸ“ˆ DRIFT TRENDS (7 Days):")
                        result.append(f"   ðŸ“Š Total Changes: {trends['total_changes']}")
                        
                        if trends.get('most_active_systems'):
                            result.append(f"   ðŸ”¥ Most Active Systems:")
                            for sys_name, count in trends['most_active_systems'][:3]:
                                result.append(f"      â€¢ {sys_name}: {count} changes")
                        
                        if trends.get('highest_risk_systems'):
                            result.append(f"   âš ï¸  Highest Risk Systems:")
                            for sys_name, risk in trends['highest_risk_systems'][:3]:
                                result.append(f"      â€¢ {sys_name}: {risk:.2f} avg risk")
                
                # Store enhanced analysis in hAIveMind memory
                await self.storage.store_memory(
                    category='infrastructure',
                    content=f"Advanced config drift analysis: {len(drift_issues)} issues, {total_patterns} patterns detected",
                    metadata={
                        'system_id': system_id,
                        'hours_back': hours_back,
                        'total_issues': len(drift_issues),
                        'patterns_detected': total_patterns,
                        'confidence': avg_confidence,
                        'security_issues': security_issues,
                        'service_issues': service_issues,
                        'network_issues': network_issues
                    }
                )
                
                return "\n".join(result)
                
            except Exception as e:
                return f"âŒ Error in advanced drift analysis: {str(e)}"
        
        def _format_enhanced_drift_issue(self, result: List[str], issue: Dict):
            """Format drift issue with enhanced pattern information"""
            system_id = issue.get('system_id', 'Unknown')
            system_name = issue.get('system_name', system_id)
            analysis = issue.get('analysis', {})
            patterns = analysis.get('patterns_detected', [])
            confidence = analysis.get('confidence', 0.0)
            
            result.append(f"  ðŸ“ {system_name} ({system_id})")
            result.append(f"     ðŸŽ¯ Confidence: {confidence:.1%}")
            
            if patterns:
                result.append(f"     ðŸ§© Patterns:")
                for pattern in patterns[:2]:  # Show top 2 patterns
                    result.append(f"        â€¢ {pattern['detail']}")
            
            recommendations = analysis.get('recommendations', [])
            if recommendations:
                result.append(f"     ðŸ’¡ Actions: {recommendations[0]}")
        
        # # @self.mcp.tool()
        async def create_intelligent_config_alert(
            system_id: str,
            snapshot_id: str,
            auto_analyze: bool = True
        ) -> str:
            """Create intelligent configuration alert with automated analysis"""
            try:
                # Get snapshot and diff data
                with sqlite3.connect(self.config_backup.db_path) as conn:
                    conn.row_factory = sqlite3.Row
                    
                    # Get snapshot
                    snapshot = conn.execute("""
                        SELECT s.*, sys.system_name, sys.system_type
                        FROM config_snapshots s
                        JOIN config_systems sys ON s.system_id = sys.system_id
                        WHERE s.id = ?
                    """, (snapshot_id,)).fetchone()
                    
                    if not snapshot:
                        return f"âŒ Snapshot {snapshot_id} not found"
                    
                    # Get associated diff
                    diff = conn.execute("""
                        SELECT * FROM config_diffs WHERE snapshot_id_after = ?
                    """, (snapshot_id,)).fetchone()
                    
                    if not diff:
                        return f"â„¹ï¸ No drift detected for snapshot {snapshot_id}"
                
                # Convert to dict and add system info
                drift_data = dict(diff)
                drift_data.update({
                    'system_id': snapshot['system_id'],
                    'system_name': snapshot['system_name'],
                    'system_type': snapshot['system_type']
                })
                
                # Create intelligent alert
                if auto_analyze:
                    alert_created = self.config_backup.create_drift_alert_with_analysis(system_id, drift_data)
                else:
                    alert_created = self.config_backup.create_drift_alert(
                        system_id=system_id,
                        drift_type='configuration_change',
                        severity='medium',
                        description=f"Configuration change detected in {snapshot['system_name']}",
                        snapshot_id=snapshot_id
                    )
                
                if alert_created:
                    # Get analysis for display
                    analysis = self.config_backup._analyze_drift(drift_data) if auto_analyze else {}
                    
                    result = [
                        f"ðŸš¨ Intelligent Configuration Alert Created",
                        f"{'='*50}",
                        f"ðŸŽ¯ System: {snapshot['system_name']} ({system_id})",
                        f"ðŸ“¸ Snapshot: {snapshot_id}",
                        f"ðŸ“… Timestamp: {snapshot['timestamp']}"
                    ]
                    
                    if auto_analyze:
                        severity = analysis.get('severity', 'unknown')
                        confidence = analysis.get('confidence', 0.0)
                        patterns = analysis.get('patterns_detected', [])
                        
                        severity_emoji = {"critical": "ðŸ”´", "high": "ðŸŸ¡", "medium": "ðŸŸ ", "low": "ðŸŸ¢"}.get(severity, "âšª")
                        result.extend([
                            f"{severity_emoji} Severity: {severity.upper()}",
                            f"ðŸŽ¯ Confidence: {confidence:.1%}",
                            f"ðŸ§© Patterns Detected: {len(patterns)}"
                        ])
                        
                        if patterns:
                            result.append(f"ðŸ“‹ Key Patterns:")
                            for pattern in patterns[:3]:
                                result.append(f"   â€¢ {pattern['detail']}")
                        
                        recommendations = analysis.get('recommendations', [])
                        if recommendations:
                            result.append(f"ðŸ’¡ Recommendations:")
                            for rec in recommendations[:3]:
                                result.append(f"   â€¢ {rec}")
                    
                    # Store alert in hAIveMind memory
                    await self.storage.store_memory(
                        category='infrastructure',
                        content=f"Created intelligent config alert for {snapshot['system_name']}",
                        metadata={
                            'system_id': system_id,
                            'snapshot_id': snapshot_id,
                            'alert_type': 'intelligent_config_drift',
                            'auto_analyzed': auto_analyze,
                            'severity': analysis.get('severity', 'unknown') if auto_analyze else 'medium'
                        }
                    )
                    
                    return "\n".join(result)
                else:
                    return f"âŒ Failed to create alert for system {system_id}"
                    
            except Exception as e:
                return f"âŒ Error creating intelligent alert: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_drift_trend_analysis(
            system_id: Optional[str] = None,
            days_back: int = 7
        ) -> str:
            """Get comprehensive drift trend analysis with predictive insights"""
            try:
                trends = self.config_backup.get_drift_trends(system_id, days_back)
                
                if 'error' in trends:
                    return f"âŒ Error analyzing trends: {trends['error']}"
                
                if trends['total_changes'] == 0:
                    scope = f"system {system_id}" if system_id else "all systems"
                    return f"ðŸ“Š No configuration changes detected in {scope} over the last {days_back} days"
                
                result = [
                    f"ðŸ“ˆ Configuration Drift Trend Analysis",
                    f"{'='*50}",
                    f"ðŸ“Š Analysis Period: {days_back} days"
                ]
                
                if system_id:
                    result.append(f"ðŸŽ¯ System: {system_id}")
                
                result.extend([
                    f"ðŸ”¢ Total Changes: {trends['total_changes']}",
                    f"ðŸ“… Daily Average: {trends['total_changes'] / days_back:.1f} changes/day"
                ])
                
                # Daily activity pattern
                daily_drift = trends.get('daily_drift', {})
                if daily_drift:
                    result.append(f"\nðŸ“… DAILY ACTIVITY PATTERN:")
                    for date, count in sorted(daily_drift.items())[-7:]:  # Last 7 days
                        activity_bar = "â–ˆ" * min(count, 20)  # Visual bar
                        result.append(f"   {date}: {count:2d} changes {activity_bar}")
                
                # Severity distribution
                severity_dist = trends.get('severity_distribution', {})
                if severity_dist:
                    result.append(f"\nðŸš¨ SEVERITY BREAKDOWN:")
                    total = sum(severity_dist.values())
                    for severity in ['critical', 'high', 'medium', 'low']:
                        count = severity_dist.get(severity, 0)
                        percentage = (count / total * 100) if total > 0 else 0
                        emoji = {"critical": "ðŸ”´", "high": "ðŸŸ¡", "medium": "ðŸŸ ", "low": "ðŸŸ¢"}.get(severity, "âšª")
                        result.append(f"   {emoji} {severity.title()}: {count} ({percentage:.1f}%)")
                
                # Most active systems
                most_active = trends.get('most_active_systems', [])
                if most_active:
                    result.append(f"\nðŸ”¥ MOST ACTIVE SYSTEMS:")
                    for i, (sys_name, count) in enumerate(most_active[:5], 1):
                        result.append(f"   {i}. {sys_name}: {count} changes")
                
                # Highest risk systems
                highest_risk = trends.get('highest_risk_systems', [])
                if highest_risk:
                    result.append(f"\nâš ï¸  HIGHEST RISK SYSTEMS:")
                    for i, (sys_name, risk) in enumerate(highest_risk[:5], 1):
                        risk_level = "CRITICAL" if risk > 0.7 else "HIGH" if risk > 0.4 else "MEDIUM" if risk > 0.2 else "LOW"
                        result.append(f"   {i}. {sys_name}: {risk:.2f} ({risk_level})")
                
                # Predictive insights
                result.append(f"\nðŸ”® PREDICTIVE INSIGHTS:")
                
                # Calculate trend direction
                daily_values = list(daily_drift.values()) if daily_drift else []
                if len(daily_values) >= 3:
                    recent_avg = sum(daily_values[-3:]) / 3
                    older_avg = sum(daily_values[:-3]) / len(daily_values[:-3]) if len(daily_values) > 3 else recent_avg
                    
                    if recent_avg > older_avg * 1.2:
                        result.append("   ðŸ“ˆ Drift activity is INCREASING")
                        result.append("   ðŸ’¡ Recommendation: Investigate recent infrastructure changes")
                    elif recent_avg < older_avg * 0.8:
                        result.append("   ðŸ“‰ Drift activity is DECREASING")
                        result.append("   âœ… Configuration stability is improving")
                    else:
                        result.append("   âž¡ï¸  Drift activity is STABLE")
                
                # Risk assessment
                total_critical = severity_dist.get('critical', 0)
                total_high = severity_dist.get('high', 0)
                high_risk_percentage = ((total_critical + total_high) / trends['total_changes'] * 100) if trends['total_changes'] > 0 else 0
                
                if high_risk_percentage > 30:
                    result.append("   ðŸš¨ HIGH RISK: >30% of changes are critical/high severity")
                    result.append("   ðŸ’¡ Recommendation: Implement stricter change controls")
                elif high_risk_percentage > 15:
                    result.append("   âš ï¸  MODERATE RISK: 15-30% of changes are critical/high severity")
                    result.append("   ðŸ’¡ Recommendation: Increase configuration monitoring")
                else:
                    result.append("   âœ… LOW RISK: <15% of changes are critical/high severity")
                
                # Store trend analysis in hAIveMind memory
                await self.storage.store_memory(
                    category='infrastructure',
                    content=f"Config drift trend analysis: {trends['total_changes']} changes over {days_back} days",
                    metadata={
                        'system_id': system_id,
                        'days_analyzed': days_back,
                        'total_changes': trends['total_changes'],
                        'daily_average': trends['total_changes'] / days_back,
                        'high_risk_percentage': high_risk_percentage,
                        'most_active_system': most_active[0][0] if most_active else None,
                        'severity_distribution': severity_dist
                    }
                )
                
                return "\n".join(result)
                
            except Exception as e:
                return f"âŒ Error analyzing drift trends: {str(e)}"

        logger.info("ðŸ“Š Config backup system tools registered - comprehensive configuration tracking enabled")

    def _add_admin_routes(self):
        """Add admin web interface routes"""
        from starlette.responses import HTMLResponse, JSONResponse, FileResponse
        from starlette.staticfiles import StaticFiles
        import os
        import re
        
        admin_dir = str(Path(__file__).parent.parent / "admin")
        
        # Serve static files (with path traversal protection)
        @self.mcp.custom_route("/admin/static/{path:path}", methods=["GET"])
        async def admin_static(request):
            try:
                # Base static directory
                base_dir = (Path(admin_dir) / "static").resolve()
                # Requested path (may include nested paths)
                requested = request.path_params.get("path", "")

                # Normalize and resolve against base directory
                candidate = (base_dir / requested).resolve()

                # Ensure the resolved path is within the static directory
                if not str(candidate).startswith(str(base_dir) + os.sep) and candidate != base_dir:
                    return JSONResponse({"error": "Invalid path"}, status_code=400)

                if candidate.is_file():
                    return FileResponse(str(candidate))
                return JSONResponse({"error": "File not found"}, status_code=404)
            except Exception:
                # On any resolution error, return 404 to avoid information leaks
                return JSONResponse({"error": "File not found"}, status_code=404)
        
        # Serve assets files (with path traversal protection)
        @self.mcp.custom_route("/assets/{path:path}", methods=["GET"])
        async def assets_static(request):
            try:
                # Base assets directory
                base_dir = (Path(__file__).parent.parent / "assets").resolve()
                # Requested path (may include nested paths)
                requested = request.path_params.get("path", "")

                # Normalize and resolve against base directory
                candidate = (base_dir / requested).resolve()

                # Ensure the resolved path is within the assets directory
                if not str(candidate).startswith(str(base_dir) + os.sep) and candidate != base_dir:
                    return JSONResponse({"error": "Invalid path"}, status_code=400)

                if candidate.is_file():
                    return FileResponse(str(candidate))
                return JSONResponse({"error": "File not found"}, status_code=404)
            except Exception:
                # On any resolution error, return 404 to avoid information leaks
                return JSONResponse({"error": "File not found"}, status_code=404)
        
        # ===== SPECIFIC DASHBOARD ROUTES (must be before catch-all route) =====
        
        # Confluence integration dashboard endpoint
        @self.mcp.custom_route("/admin/confluence", methods=["GET"])
        async def confluence_dashboard(request):
            """Serve the Confluence integration dashboard interface"""
            return HTMLResponse("""
            <!DOCTYPE html>
            <html>
            <head>
                <title>Confluence Integration - hAIveMind</title>
                <link rel="stylesheet" href="/admin/static/admin.css">
                <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
            </head>
            <body class="dashboard">
                <nav class="nav-header">
                    <div class="nav-brand">
                        <img src="/assets/logo.png" alt="hAIveMind" class="nav-logo">
                        <h1>hAIveMind Confluence Integration</h1>
                    </div>
                    <div class="nav-links">
                        <a href="/admin/dashboard.html">Dashboard</a>
                        <a href="/admin/memory.html">Memory Browser</a>
                        <a href="/admin/mcp_servers.html">MCP Servers</a>
                        <a href="/admin/vault">Vault Management</a>
                        <a href="/admin/rules-dashboard">Rules & Governance</a>
                        <a href="/admin/playbooks">Playbook Management</a>
                        <a href="/admin/executions">Execution Monitoring</a>
                        <a href="/admin/confluence" class="active">Confluence Integration</a>
                        <a href="/admin/help-dashboard">Help System</a>
                    </div>
                    <button class="logout-btn" onclick="logout()">Logout</button>
                </nav>
                <main class="main-content">
                    <div class="card">
                        <h2><i class="fab fa-confluence"></i> Confluence Integration</h2>
                        <p>Sync documentation and runbooks from Confluence spaces into hAIveMind knowledge base.</p>
                        
                        <div class="feature-grid">
                            <div class="feature-card">
                                <h3><i class="fas fa-cog"></i> Configuration</h3>
                                <p>Configure Confluence API connection and authentication</p>
                                <button class="btn btn-primary" onclick="showConfig()">Configure Connection</button>
                            </div>
                            <div class="feature-card">
                                <h3><i class="fas fa-sync"></i> Sync Management</h3>
                                <p>Manage automatic sync schedules and manual imports</p>
                                <button class="btn btn-secondary" onclick="showSyncManager()">Manage Sync</button>
                            </div>
                            <div class="feature-card">
                                <h3><i class="fas fa-sitemap"></i> Space Mapping</h3>
                                <p>Map Confluence spaces to hAIveMind categories</p>
                                <button class="btn btn-info" onclick="showSpaceMapping()">Configure Spaces</button>
                            </div>
                            <div class="feature-card">
                                <h3><i class="fas fa-history"></i> Import History</h3>
                                <p>View sync history and resolve conflicts</p>
                                <button class="btn btn-warning" onclick="showImportHistory()">View History</button>
                            </div>
                        </div>

                        <div class="sync-status" id="syncStatus">
                            <h3>Sync Status</h3>
                            <div class="status-grid">
                                <div class="status-item">
                                    <span class="status-label">Connection Status</span>
                                    <span class="status-value" id="connectionStatus">Loading...</span>
                                </div>
                                <div class="status-item">
                                    <span class="status-label">Last Sync</span>
                                    <span class="status-value" id="lastSync">Loading...</span>
                                </div>
                                <div class="status-item">
                                    <span class="status-label">Monitored Spaces</span>
                                    <span class="status-value" id="monitoredSpaces">Loading...</span>
                                </div>
                                <div class="status-item">
                                    <span class="status-label">Synced Pages</span>
                                    <span class="status-value" id="syncedPages">Loading...</span>
                                </div>
                            </div>
                        </div>
                        
                        <div class="quick-actions">
                            <h3>Quick Actions</h3>
                            <button class="btn btn-success" onclick="triggerManualSync()">
                                <i class="fas fa-download"></i> Manual Sync Now
                            </button>
                            <button class="btn btn-info" onclick="testConnection()">
                                <i class="fas fa-plug"></i> Test Connection
                            </button>
                            <button class="btn btn-secondary" onclick="viewLogs()">
                                <i class="fas fa-file-text"></i> View Sync Logs
                            </button>
                        </div>
                    </div>
                </main>
                <script src="/admin/static/admin.js"></script>
                <script src="/admin/static/confluence.js"></script>
            </body>
            </html>
            """)
        
        # ===== COMET BROWSER INTEGRATION ROUTES =====
        
        # Comet main portal
        @self.mcp.custom_route("/comet", methods=["GET"])
        async def comet_portal(request):
            """Redirect to AI-optimized interface"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            # Redirect to the AI-optimized single-page playground
            return RedirectResponse(url="/comet-ai", status_code=302)
            
        
        # ===== COMET API ENDPOINTS =====
        
        # Comet authentication endpoint
        @self.mcp.custom_route("/comet/api/auth", methods=["POST"])
        async def comet_auth(request):
            """Authenticate Comet browser and return session token"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            try:
                body = await request.json()
                password = body.get('password')
                
                if not password:
                    return JSONResponse({"error": "Password required"}, status_code=400)
                
                session_token = self.comet_system.authenticate_comet(password)
                
                if session_token:
                    return JSONResponse({
                        "success": True,
                        "session_token": session_token,
                        "message": "Authentication successful"
                    })
                else:
                    return JSONResponse({"error": "Invalid password"}, status_code=401)
                    
            except Exception as e:
                logger.error(f"Comet auth error: {e}")
                return JSONResponse({"error": "Authentication failed"}, status_code=500)
        
        # Comet session validation endpoint
        @self.mcp.custom_route("/comet/api/validate", methods=["GET"])
        async def comet_validate(request):
            """Validate Comet session token"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            auth_header = request.headers.get('authorization')
            if not auth_header or not auth_header.startswith('Bearer '):
                return JSONResponse({"error": "Authorization header required"}, status_code=401)
            
            token = auth_header.split(' ')[1]
            
            if self.comet_system.validate_session(token):
                return JSONResponse({"valid": True})
            else:
                return JSONResponse({"valid": False}, status_code=401)
        
        # Comet directives endpoint
        @self.mcp.custom_route("/comet/directives", methods=["GET"])
        async def comet_directives(request):
            """Get active directives for Comet to process"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            directives = self.comet_system.get_active_directives()
            
            # Return HTML format optimized for AI reading
            return HTMLResponse(f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>hAIveMind Active Directives</title>
                <meta name="ai-optimized" content="true">
                <meta name="refresh-interval" content="{self.comet_system.refresh_interval}">
            </head>
            <body style="font-family: monospace; padding: 20px; background: #0a0e1a; color: #e1e5e9; line-height: 1.6;">
                <h1>ðŸ“‹ Active Directives for Comet Assistant</h1>
                <p><strong>Directive Count:</strong> {len(directives)}</p>
                <p><strong>Last Updated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                
                {"".join([f'''
                <div style="border: 1px solid #2d3748; border-left: 4px solid #00d4aa; margin: 15px 0; padding: 15px; background: #162032;">
                    <h3>Directive ID: {directive['id']}</h3>
                    <p><strong>Type:</strong> {directive['type']}</p>
                    <p><strong>Priority:</strong> {directive['priority']}</p>
                    <p><strong>Status:</strong> {directive['status']}</p>
                    <p><strong>Created:</strong> {directive['created_at']}</p>
                    <div style="background: #0a0e1a; padding: 10px; margin: 10px 0; border-radius: 4px;">
                        <pre>{json.dumps(directive['content'], indent=2)}</pre>
                    </div>
                    <p><strong>Instructions:</strong> {directive['content'].get('instruction', 'No specific instructions')}</p>
                    <p><strong>Expected Format:</strong> {directive['content'].get('expected_format', 'Not specified')}</p>
                </div>
                ''' for directive in directives])}
                
                {'''<div style="border: 1px solid #4a5568; padding: 15px; margin: 15px 0; background: #1a1e30; text-align: center;">
                    <p>No active directives at this time.</p>
                    <p>Check back in {self.comet_system.refresh_interval} seconds for new tasks.</p>
                </div>''' if not directives else ''}
                
                <hr style="border: 1px solid #2d3748; margin: 30px 0;">
                <p><strong>Next Check:</strong> Refresh this page in {self.comet_system.refresh_interval} seconds</p>
                <p><strong>API Endpoint:</strong> <code>/comet/api/directives</code> for JSON format</p>
            </body>
            </html>
            """)
        
        # Comet memory search endpoint
        @self.mcp.custom_route("/comet/api/memory/search", methods=["GET"])  
        async def comet_memory_search(request):
            """Search hAIveMind collective memory"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            query = request.query_params.get('q', '')
            category = request.query_params.get('category')
            limit = int(request.query_params.get('limit', '5'))
            
            if not query:
                return JSONResponse({"error": "Query parameter 'q' is required"}, status_code=400)
            
            try:
                # Search memories using the storage system
                results = await self.storage.search_memories(
                    query=query,
                    category=category,
                    limit=limit,
                    semantic=True
                )
                
                # Sanitize output to prevent prompt injection
                sanitized_results = self.comet_system.sanitize_output(results)
                
                return JSONResponse({
                    "query": query,
                    "category": category,
                    "results": sanitized_results[:limit],
                    "total_found": len(sanitized_results),
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Comet memory search error: {e}")
                return JSONResponse({"error": "Search failed"}, status_code=500)
        
        # Comet API directives (JSON format)
        @self.mcp.custom_route("/comet/api/directives", methods=["GET"])
        async def comet_api_directives(request):
            """Get active directives in JSON format"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            # Validate session
            auth_header = request.headers.get('authorization')
            if not auth_header or not auth_header.startswith('Bearer '):
                return JSONResponse({"error": "Authorization required"}, status_code=401)
            
            token = auth_header.split(' ')[1]
            if not self.comet_system.validate_session(token):
                return JSONResponse({"error": "Invalid or expired session"}, status_code=401)
            
            directives = self.comet_system.get_active_directives()
            
            return JSONResponse({
                "directives": directives,
                "count": len(directives),
                "refresh_interval": self.comet_system.refresh_interval,
                "timestamp": datetime.now().isoformat()
            })
        
        # Comet API status (JSON format) - Fast version
        @self.mcp.custom_route("/comet/api/status", methods=["GET"])
        async def comet_api_status(request):
            """Get system status in JSON format - Fast mode"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            # Fast status check without database queries
            try:
                active_sessions = len([s for s in self.comet_system.sessions.values() if s.get('active', False)])
                active_directives = len(self.comet_system.get_active_directives())
                
                status = {
                    'status': 'healthy',
                    'timestamp': datetime.now().isoformat(),
                    'system': {
                        'active_sessions': active_sessions,
                        'active_directives': active_directives,
                        'response_time_ms': '<50',
                        'mode': 'fast'
                    },
                    'comet': {
                        'enabled': True,
                        'portal_url': '/comet',
                        'features': ['data_submission', 'directive_creation', 'memory_search']
                    }
                }
            except Exception as e:
                status = {
                    'status': 'error',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
            
            return JSONResponse(status)
        
        # Comet status HTML page
        @self.mcp.custom_route("/comet/status", methods=["GET"])
        async def comet_status_html(request):
            """Get system status in HTML format for Comet"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            # Fast status with minimal queries
            try:
                active_sessions = len([s for s in self.comet_system.sessions.values() if s.get('active', False)])
                active_directives = len(self.comet_system.get_active_directives())
                
                status = {
                    'status': 'healthy',
                    'timestamp': datetime.now().isoformat(),
                    'system': {
                        'active_sessions': active_sessions,
                        'active_directives': active_directives,
                        'agent_count': 'N/A (fast mode)',
                        'memory_system': 'operational'
                    },
                    'recent_activity': [
                        {'category': 'system', 'timestamp': datetime.now().isoformat(), 'content': 'System status check - fast mode'},
                        {'category': 'comet', 'timestamp': datetime.now().isoformat(), 'content': 'Portal active and receiving requests'}
                    ]
                }
            except Exception as e:
                status = {
                    'status': 'error',
                    'timestamp': datetime.now().isoformat(),
                    'error': str(e),
                    'system': {'message': 'Status check failed'}
                }
            
            # Convert to HTML
            recent_activity_html = ""
            for activity in status.get('recent_activity', []):
                recent_activity_html += f'''
                <div style="background: #1a1e30; padding: 10px; margin: 10px 0; border-radius: 4px; border-left: 3px solid #00d4aa;">
                    <strong>{activity.get('category', 'unknown').upper()}</strong> - {activity.get('timestamp', '')}
                    <br>{activity.get('content', '')[:100]}...
                </div>'''
            
            # Simplified fast HTML response
            return HTMLResponse(f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>hAIveMind System Status - Fast Mode</title>
                <meta name="ai-optimized" content="true">
                <meta name="refresh-interval" content="5">
            </head>
            <body style="font-family: monospace; padding: 20px; background: #0a0e1a; color: #e1e5e9; line-height: 1.6;">
                <h1>âš¡ hAIveMind System Status (Fast Mode)</h1>
                <p><a href="/comet">â† Back to Portal</a> | <a href="javascript:location.reload()">ðŸ”„ Refresh</a></p>
                
                <div style="border: 1px solid #00d4aa; padding: 15px; margin: 15px 0; background: #162032; border-radius: 4px;">
                    <h2>System Health: âœ… {status.get('status', 'Unknown').title()}</h2>
                    <p>ðŸŸ¢ <strong>Active Sessions:</strong> {status.get('system', {}).get('active_sessions', 0)}</p>
                    <p>ðŸŽ¯ <strong>Active Directives:</strong> {status.get('system', {}).get('active_directives', 0)}</p>
                    <p>ðŸ§  <strong>Memory System:</strong> {status.get('system', {}).get('memory_system', 'operational')}</p>
                    <p>â±ï¸ <strong>Response Time:</strong> <50ms (optimized)</p>
                    <p>ðŸ• <strong>Last Updated:</strong> {datetime.now().strftime('%H:%M:%S')}</p>
                </div>
                
                <div style="border: 1px solid #2d3748; padding: 15px; margin: 15px 0; background: #162032; border-radius: 4px;">
                    <h2>Quick Status</h2>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
                        <div>âœ… Comet Portal: Active</div>
                        <div>âœ… Data Submission: Working</div>
                        <div>âœ… Directive Creation: Working</div>
                        <div>âœ… Memory Search: Working</div>
                        <div>âœ… Agent Network: Connected</div>
                        <div>âœ… Tailscale: Secured</div>
                    </div>
                </div>
                
                <p style="font-size: 0.9em; color: #8892b0;">
                    <strong>Note:</strong> Fast mode enabled for quick loading. 
                    API: <code><a href="/comet/api/status" style="color: #00d4aa;">/comet/api/status</a></code>
                </p>
            </body>
            </html>
            """)
        
        # Comet API get available categories
        @self.mcp.custom_route("/comet/api/categories", methods=["GET"])
        async def comet_get_categories(request):
            """Get all available memory categories for Comet"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            try:
                # Return default categories for now - can be extended later
                category_options = [
                    {"value": "comet_findings", "label": "Comet Findings", "description": "Findings from Comet browser"},
                    {"value": "research", "label": "Research", "description": "Research data and findings"},
                    {"value": "intelligence", "label": "Intelligence", "description": "Threat intelligence data"},
                    {"value": "observations", "label": "Observations", "description": "General observations"},
                    {"value": "analysis", "label": "Analysis", "description": "Analysis results"},
                    {"value": "incident", "label": "Incident", "description": "Security incidents"},
                    {"value": "security", "label": "Security", "description": "Security-related information"},
                    {"value": "other", "label": "Other", "description": "Miscellaneous category"}
                ]
                
                return JSONResponse({
                    "categories": category_options,
                    "total": len(category_options),
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Comet get categories error: {e}")
                return JSONResponse({"error": "Failed to get categories"}, status_code=500)
        
        # Comet API create new category (simplified for now)
        @self.mcp.custom_route("/comet/api/categories", methods=["POST"])
        async def comet_create_category(request):
            """Create a new memory category from Comet"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            try:
                data = await request.json()
                category_name = data.get('name', '').strip()
                description = data.get('description', '').strip()
                
                if not category_name:
                    return JSONResponse({"error": "Category name is required"}, status_code=400)
                
                # For now, just acknowledge the creation
                # Later this can be expanded to actually create categories in the system
                return JSONResponse({
                    "success": True,
                    "message": f"Category '{category_name}' noted for creation",
                    "category": {
                        "name": category_name,
                        "description": description
                    },
                    "timestamp": datetime.now().isoformat()
                })
                    
            except Exception as e:
                logger.error(f"Comet create category error: {e}")
                return JSONResponse({"error": "Failed to create category"}, status_code=500)

        # Comet API create directive (JSON format)
        @self.mcp.custom_route("/comet/api/directive", methods=["POST"])
        async def comet_api_create_directive(request):
            """Create a new directive for Comet"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            try:
                data = await request.json()
                title = data.get('title', '').strip()
                content = data.get('content', '').strip()
                directive_type = data.get('type', 'task')
                priority = data.get('priority', 'normal')
                deadline = data.get('deadline')
                tags = data.get('tags', [])
                required_capabilities = data.get('required_capabilities', [])
                metadata = data.get('metadata', {})
                
                if not title or not content:
                    return JSONResponse({"error": "Title and content are required"}, status_code=400)
                
                # Enhanced directive data structure
                directive_content = {
                    "title": title,
                    "content": content,
                    "type": directive_type,
                    "priority": priority,
                    "deadline": deadline,
                    "tags": tags,
                    "required_capabilities": required_capabilities,
                    "metadata": {
                        **metadata,
                        "created_by": "comet_browser",
                        "created_at": datetime.now().isoformat(),
                        "network": "tailscale_secured"
                    }
                }
                
                directive_id = self.comet_system.create_directive(directive_type, directive_content, priority)
                
                # Store directive in hAIveMind memory for agent coordination
                await self.storage.store_memory(
                    content=f"DIRECTIVE: {title}\n\n{content}",
                    category="directives",
                    tags=["directive", directive_type, priority] + tags,
                    metadata={
                        "directive_id": directive_id,
                        "directive_type": directive_type,
                        "priority": priority,
                        "required_capabilities": ",".join(required_capabilities) if required_capabilities else "",
                        "deadline": deadline or "",
                        "created_by": metadata.get("created_by", "comet_browser"),
                        "created_at": datetime.now().isoformat(),
                        "network": "tailscale_secured"
                    }
                )
                
                logger.info(f"ðŸŽ¯ Comet created directive: {directive_type} - {directive_id} - {title}")
                
                return JSONResponse({
                    "success": True,
                    "directive_id": directive_id,
                    "message": f"Directive '{title}' created successfully and broadcast to hAIveMind network",
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Comet create directive error: {e}")
                return JSONResponse({"error": "Failed to create directive"}, status_code=500)
        
        # Comet data submission endpoints for bidirectional communication  
        @self.mcp.custom_route("/comet/api/submit", methods=["POST"])
        async def comet_submit_data(request):
            """Accept data submissions from Comet browser"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            try:
                data = await request.json()
                submission_type = data.get('type', 'general')
                content = data.get('content', '')
                category = data.get('category', 'comet_findings')
                tags = data.get('tags', [])
                metadata = data.get('metadata', {})
                
                if not content:
                    return JSONResponse({"error": "Content is required"}, status_code=400)
                
                # Add Comet-specific metadata
                enhanced_metadata = {
                    **metadata,
                    "source": "comet_browser",
                    "submission_type": submission_type,
                    "submitted_at": datetime.now().isoformat(),
                    "network": "tailscale_secured",
                }
                
                # Store in hAIveMind memory system
                memory_id = await self.storage.store_memory(
                    content=content,
                    category=category,
                    tags=tags + ["comet-submission", submission_type],
                    metadata=enhanced_metadata
                )
                
                logger.info(f"ðŸš€ Comet submitted data: {submission_type} - {memory_id}")
                
                return JSONResponse({
                    "success": True,
                    "memory_id": memory_id,
                    "message": "Data successfully submitted to hAIveMind",
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Comet data submission error: {e}")
                return JSONResponse({"error": "Failed to submit data"}, status_code=500)
        
        # Comet results submission (for task completion)
        @self.mcp.custom_route("/comet/api/results", methods=["POST"])
        async def comet_submit_results(request):
            """Accept task completion results from Comet"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            # Validate session
            auth_header = request.headers.get('authorization')
            if not auth_header or not auth_header.startswith('Bearer '):
                return JSONResponse({"error": "Authorization required"}, status_code=401)
            
            token = auth_header.split(' ')[1]
            if not self.comet_system.validate_session(token):
                return JSONResponse({"error": "Invalid or expired session"}, status_code=401)
            
            try:
                data = await request.json()
                directive_id = data.get('directive_id')
                results = data.get('results', {})
                status = data.get('status', 'completed')
                notes = data.get('notes', '')
                
                if not directive_id:
                    return JSONResponse({"error": "directive_id is required"}, status_code=400)
                
                # Update directive status
                updated = self.comet_system.update_directive_status(
                    directive_id, 
                    status, 
                    {
                        "results": results,
                        "notes": notes,
                        "completed_at": datetime.now().isoformat(),
                        "completion_source": "comet_browser"
                    }
                )
                
                if not updated:
                    return JSONResponse({"error": "Directive not found or already completed"}, status_code=404)
                
                # Store results in memory for future reference
                result_content = f"Comet Task Completion - Directive {directive_id}\\n\\n"
                result_content += f"Status: {status}\\n"
                result_content += f"Results: {results}\\n"
                if notes:
                    result_content += f"Notes: {notes}\\n"
                
                memory_id = await self.storage.store_memory(
                    content=result_content,
                    category="comet_results",
                    tags=["comet-completion", "task-results", directive_id],
                    metadata={
                        "source": "comet_browser",
                        "directive_id": directive_id,
                        "completion_status": status,
                        "completed_at": datetime.now().isoformat()
                    }
                )
                
                logger.info(f"ðŸš€ Comet completed directive {directive_id} - {status}")
                
                return JSONResponse({
                    "success": True,
                    "directive_id": directive_id,
                    "memory_id": memory_id,
                    "message": "Results successfully submitted",
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Comet results submission error: {e}")
                return JSONResponse({"error": "Failed to submit results"}, status_code=500)
        
        # Comet feedback/questions endpoint
        @self.mcp.custom_route("/comet/api/feedback", methods=["POST"])
        async def comet_submit_feedback(request):
            """Accept feedback or questions from Comet"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            # Validate session
            auth_header = request.headers.get('authorization')
            if not auth_header or not auth_header.startswith('Bearer '):
                return JSONResponse({"error": "Authorization required"}, status_code=401)
            
            token = auth_header.split(' ')[1]
            if not self.comet_system.validate_session(token):
                return JSONResponse({"error": "Invalid or expired session"}, status_code=401)
            
            try:
                data = await request.json()
                feedback_type = data.get('type', 'general')  # general, question, issue, suggestion
                message = data.get('message', '')
                priority = data.get('priority', 'normal')
                context = data.get('context', {})
                
                if not message:
                    return JSONResponse({"error": "Message is required"}, status_code=400)
                
                # Store feedback in memory
                feedback_content = f"Comet Browser Feedback ({feedback_type})\\n\\n{message}"
                if context:
                    feedback_content += f"\\n\\nContext: {context}"
                
                memory_id = await self.storage.store_memory(
                    content=feedback_content,
                    category="comet_feedback",
                    tags=["comet-feedback", feedback_type, priority],
                    metadata={
                        "source": "comet_browser",
                        "feedback_type": feedback_type,
                        "priority": priority,
                        "context": context,
                        "submitted_at": datetime.now().isoformat()
                    }
                )
                
                # If it's a question or issue, create a directive for human review
                if feedback_type in ['question', 'issue'] and priority in ['high', 'urgent']:
                    self.comet_system.create_directive(
                        'comet_feedback_review',
                        {
                            "feedback_id": memory_id,
                            "type": feedback_type,
                            "message": message,
                            "priority": priority,
                            "requires_response": True
                        },
                        priority
                    )
                
                logger.info(f"ðŸš€ Comet feedback received: {feedback_type} - {priority}")
                
                return JSONResponse({
                    "success": True,
                    "feedback_id": memory_id,
                    "message": "Feedback received and stored",
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Comet feedback submission error: {e}")
                return JSONResponse({"error": "Failed to submit feedback"}, status_code=500)

        # Comet memory retrieval endpoints for enhanced comment system
        @self.mcp.custom_route("/comet/api/memory/{memory_id}", methods=["GET"])
        async def comet_get_memory_by_id(request):
            """Get a specific memory by ID for browser display"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            memory_id = request.path_params.get('memory_id')
            if not memory_id:
                return JSONResponse({"error": "Memory ID is required"}, status_code=400)
            
            try:
                memory = await self.comet_system.get_memory_by_id(memory_id)
                if memory:
                    return JSONResponse({
                        "success": True,
                        "memory": memory,
                        "timestamp": datetime.now().isoformat()
                    })
                else:
                    return JSONResponse({"error": "Memory not found"}, status_code=404)
                    
            except Exception as e:
                logger.error(f"Comet get memory error: {e}")
                return JSONResponse({"error": "Failed to retrieve memory"}, status_code=500)

        @self.mcp.custom_route("/comet/api/tickets/{ticket_id}/comments", methods=["GET"])
        async def comet_get_ticket_comments(request):
            """Get all comments for a ticket with memory context for browser display"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            ticket_id = request.path_params.get('ticket_id')
            if not ticket_id:
                return JSONResponse({"error": "Ticket ID is required"}, status_code=400)
            
            try:
                comments = await self.comet_system.get_comment_memories(ticket_id)
                return JSONResponse({
                    "success": True,
                    "ticket_id": ticket_id,
                    "comments": comments,
                    "total_comments": len(comments),
                    "browser_optimized": True,
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Comet get ticket comments error: {e}")
                return JSONResponse({"error": "Failed to retrieve comments"}, status_code=500)

        @self.mcp.custom_route("/comet/api/memory/search-browser", methods=["GET"])
        async def comet_search_memories_browser(request):
            """Search memories with browser-optimized formatting"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled"}, status_code=404)
            
            query = request.query_params.get('q', '')
            category = request.query_params.get('category')
            limit = int(request.query_params.get('limit', 20))
            
            if not query:
                return JSONResponse({"error": "Query parameter 'q' is required"}, status_code=400)
            
            try:
                results = await self.comet_system.search_memories_for_browser(query, category, limit)
                return JSONResponse(results)
                
            except Exception as e:
                logger.error(f"Comet memory search error: {e}")
                return JSONResponse({"error": "Search failed"}, status_code=500)

        logger.info("ðŸš€ Comet memory retrieval endpoints registered - enhanced comment system integration enabled")
        
        # ===== COMET# - AI-OPTIMIZED SIMPLIFIED PORTAL =====
        
        @self.mcp.custom_route("/comet-ai", methods=["GET"])
        async def comet_ai_portal(request):
            """Ultra-simplified AI-first portal for Comet Browser"""
            if not self.comet_system.enabled:
                return JSONResponse({"error": "Comet integration is disabled", "comet_meta": {"source": "haivemind", "error": True}}, status_code=404)
            
            return HTMLResponse("""<!DOCTYPE html>
<html><head>
<title>Comet Handoff | Simple</title>
<style>
body { font-family: monospace; margin: 20px; background: #0a0e1a; color: #e1e5e9; }
.container { display: flex; gap: 20px; }
.column { flex: 1; }
.input-box { width: 100%; padding: 8px; background: #2d3748; color: #e1e5e9; border: 1px solid #4a5568; margin-bottom: 10px; }
.paste-area { width: 100%; height: 300px; background: #2d3748; color: #e1e5e9; border: 1px solid #4a5568; padding: 10px; }
.btn { background: #00d4aa; color: #0a0e1a; border: none; padding: 8px 16px; margin-right: 10px; cursor: pointer; }
</style>
</head><body>

<div class="container">
  <div class="column">
    <h3>ðŸ“ Paste Work</h3>
    <input type="text" id="subject" class="input-box" placeholder="Subject">
    <textarea id="content" class="paste-area" placeholder="Content"></textarea>
    <button class="btn" onclick="save()">Save</button>
    <button class="btn" onclick="clearForm()">Clear</button>
  </div>
  <div class="column">
    <h3>ðŸ“¤ Recent (click items to load)</h3>
    <div style="margin-bottom: 10px;">
      <input type="text" id="searchInput" placeholder="Search memories..."
             style="background:#2d3748;color:#e1e5e9;border:1px solid #4a5568;padding:4px;width:200px;margin-right:10px;">
      <button class="btn" onclick="loadRecent()" style="margin-right:5px;">Search</button>
      <button class="btn" onclick="document.getElementById('searchInput').value='';loadRecent()" style="margin-right:10px;">Clear</button>

      Show: <select id="limitSelect" onchange="loadRecent()" style="background:#2d3748;color:#e1e5e9;border:1px solid #4a5568;padding:4px;">
        <option value="5">5</option>
        <option value="10" selected>10</option>
        <option value="25">25</option>
        <option value="50">50</option>
        <option value="100">100</option>
      </select> items
    </div>
    <div id="recent">Loading...</div>
    <button class="btn" onclick="location.reload()">Refresh</button>
  </div>
</div>

<script>
function save() {
  const subject = document.getElementById('subject').value;
  const content = document.getElementById('content').value;

  if (!subject && !content) {
    // Show warning in recent div (visible to browser AI)
    const div = document.getElementById('recent');
    div.innerHTML = '<div style="background:#ffaa00;color:#0a0e1a;padding:20px;font-size:18px;font-weight:bold;text-align:center;border-radius:5px;margin:10px 0;">âš ï¸ NOTHING TO SAVE - Please enter content</div>';
    return;
  }

  const data = JSON.stringify({
    subject: subject || 'Untitled',
    content: content,
    timestamp: new Date().toISOString()
  });

  fetch('/comet-ai/paste', {
    method: 'POST',
    headers: {'Content-Type': 'application/json'},
    body: JSON.stringify({content: data, category: 'comet_handoff'})
  })
  .then(r => r.json())
  .then(result => {
    if (result.success) {
      // Show success with memory ID
      const memId = result.memory_id ? result.memory_id.substring(0, 8) : 'unknown';
      const successMsg = 'âœ… SAVED! ID: ' + memId;

      // Show in recent div (visible to browser AI)
      const div = document.getElementById('recent');
      div.innerHTML = '<div style="background:#00d4aa;color:#0a0e1a;padding:20px;font-size:18px;font-weight:bold;text-align:center;border-radius:5px;margin:10px 0;">' + successMsg + '</div>';

      // Clear form
      document.getElementById('subject').value = '';
      document.getElementById('content').value = '';

      // Wait 1 second before reloading to ensure indexing is complete
      setTimeout(() => {
        loadRecent();
      }, 1000);
    } else {
      // Show error in recent div (visible to browser AI)
      const div = document.getElementById('recent');
      div.innerHTML = '<div style="background:#ff5555;color:#fff;padding:20px;font-size:18px;font-weight:bold;text-align:center;border-radius:5px;margin:10px 0;">âŒ SAVE FAILED</div>';
    }
  })
  .catch(e => {
    // Show error in recent div (visible to browser AI)
    const div = document.getElementById('recent');
    div.innerHTML = '<div style="background:#ff5555;color:#fff;padding:20px;font-size:18px;font-weight:bold;text-align:center;border-radius:5px;margin:10px 0;">âŒ ERROR: ' + e.message + '</div>';
  });
}

function clearForm() {
  console.log('Clearing form');
  document.getElementById('subject').value = '';
  document.getElementById('content').value = '';
}

// Load recent function
function loadRecent() {
  console.log('loadRecent() called');
  const limit = document.getElementById('limitSelect') ? document.getElementById('limitSelect').value : 10;
  const query = document.getElementById('searchInput') ? document.getElementById('searchInput').value : '';
  console.log('Search query:', query, 'Limit:', limit);
  const url = '/comet-ai/recent?limit=' + limit + (query ? '&query=' + encodeURIComponent(query) : '');

  // Show searching indicator
  const div = document.getElementById('recent');
  div.innerHTML = query ? 'Searching for "' + query + '"...' : 'Loading recent items...';

  fetch(url)
  .then(r => r.json())
  .then(data => {
    const div = document.getElementById('recent');
    if (data.memories && data.memories.length > 0) {
      div.innerHTML = data.memories.map(m => {
        const source = m.metadata?.processed_by === 'simple-comet' ? 'ðŸŒ Web' :
                      m.metadata?.source === 'simple-paste' ? 'ðŸŒ Web' :
                      m.metadata?.processed_by ? 'âš¡ MCP' : 'ðŸ“ Other';
        const sourceColor = source.includes('Web') ? '#00d4aa' :
                           source.includes('MCP') ? '#4fc3f7' : '#9ca3af';
        return `<div style="border:1px solid #4a5568;padding:10px;margin:10px 0;cursor:pointer;background:#1a202c;" onclick="load('${m.id}')" onmouseover="this.style.background='#2d3748'" onmouseout="this.style.background='#1a202c'">
          <div style="display:flex;justify-content:space-between;align-items:center;">
            <strong>ðŸ”— #${m.id.substring(0,8)}</strong>
            <span style="color:${sourceColor};font-size:12px;">${source}</span>
          </div>
          <em>(click to load)</em><br>
          ${m.content.substring(0,100)}...
        </div>`;
      }).join('');
    } else {
      div.innerHTML = 'No recent work found';
    }
  })
  .catch(e => document.getElementById('recent').innerHTML = 'Load error');
}

// Load recent on page load
window.onload = loadRecent;

function load(id) {
  fetch('/comet-ai/get/' + id)
  .then(r => r.json())
  .then(result => {
    if (result.success) {
      try {
        const parsed = JSON.parse(result.content);
        document.getElementById('subject').value = parsed.subject || '';
        document.getElementById('content').value = parsed.content || '';
      } catch (e) {
        document.getElementById('content').value = result.content;
      }
      // Show success in recent div (visible to browser AI)
      const div = document.getElementById('recent');
      div.innerHTML = '<div style="background:#00d4aa;color:#0a0e1a;padding:20px;font-size:18px;font-weight:bold;text-align:center;border-radius:5px;margin:10px 0;">âœ… LOADED! ID: ' + id.substring(0,8) + '</div>';
      setTimeout(loadRecent, 1000);
    }
  })
  .catch(e => {
    // Show error in recent div (visible to browser AI)
    const div = document.getElementById('recent');
    div.innerHTML = '<div style="background:#ff5555;color:#fff;padding:20px;font-size:18px;font-weight:bold;text-align:center;border-radius:5px;margin:10px 0;">âŒ LOAD FAILED</div>';
  });
}
</script>
</body></html>""")

        @self.mcp.custom_route("/comet-ai/context", methods=["GET"])
        async def comet_ai_context_DISABLED(request):
            return JSONResponse({"error": "Simplified portal - context disabled"})

        # Cleaned up - old HTML removed
        async def comet_ai_paste(request):
            """Quick paste endpoint for pastebin functionality"""
            try:
                data = await request.json()
                content = data.get('content', '').strip()

                if not content:
                    return JSONResponse({"error": "No content provided"}, status_code=400)

                # Generate unique short ID
                import hashlib
                short_id = hashlib.md5(content.encode()).hexdigest()[:8]

                # Store with pastebin metadata
                if hasattr(self.storage, 'store_memory'):
                    memory_id = await self.storage.store_memory(
                        content=content,
                        category='comet_handoff',
                        metadata={
                            "processed_by": "comet-pastebin",
                            "processed_at": datetime.now().isoformat(),
                            "tags": "#comet-handoff,#pastebin",
                            "source": "comet-paste",
                            "short_id": short_id,
                            "paste_type": data.get('type', 'text')
                        }
                    )

                    return JSONResponse({
                        "success": True,
                        "memory_id": memory_id,
                        "short_id": short_id,
                        "content_length": len(content)
                    })
                else:
                    return JSONResponse({"error": "Storage unavailable"}, status_code=500)

            except Exception as e:
                logger.error(f"Paste error: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)

        @self.mcp.custom_route("/comet-ai/get/{item_id}", methods=["GET"])
        async def comet_ai_get(request):
            """Get specific paste by ID"""
            try:
                item_id = request.path_params['item_id']

                if hasattr(self.storage, 'retrieve_memory'):
                    memory = await self.storage.retrieve_memory(item_id)
                    if memory:
                        return JSONResponse({
                            "success": True,
                            "memory": memory,
                            "content": memory.get('content', ''),
                            "created_at": memory.get('created_at', ''),
                            "short_id": memory.get('metadata', {}).get('short_id', item_id[:8])
                        })
                    else:
                        return JSONResponse({"error": "Paste not found"}, status_code=404)
                else:
                    return JSONResponse({"error": "Retrieval unavailable"}, status_code=500)

            except Exception as e:
                logger.error(f"Get paste error: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)

        @self.mcp.custom_route("/comet-ai/paste", methods=["POST"])
        async def comet_ai_paste(request):
            """Simple paste endpoint"""
            try:
                data = await request.json()
                content = data.get('content', '').strip()
                category = data.get('category', 'comet_handoff')

                if not content:
                    return JSONResponse({"success": False, "error": "No content provided"}, status_code=400)

                # Store with simple metadata
                if hasattr(self.storage, 'store_memory'):
                    memory_id = await self.storage.store_memory(
                        content=content,
                        category=category,
                        metadata={
                            "processed_by": "simple-comet",
                            "processed_at": datetime.now().isoformat(),
                            "source": "simple-paste"
                        }
                    )
                    return JSONResponse({
                        "success": True,
                        "memory_id": memory_id
                    })
                else:
                    return JSONResponse({"success": False, "error": "Storage unavailable"}, status_code=500)

            except Exception as e:
                logger.error(f"Paste error: {e}")
                return JSONResponse({"success": False, "error": str(e)}, status_code=500)

        @self.mcp.custom_route("/comet-ai/recent", methods=["GET"])
        async def comet_ai_recent(request):
            """Get recent pastes from all memories"""
            try:
                limit = int(request.query_params.get('limit', 10))
                query = request.query_params.get('query', '').strip()

                # Try different approaches to get recent memories
                memories = []

                if hasattr(self.storage, 'search_memories'):
                    # Use search to get all recent memories
                    try:
                        all_memories = await self.storage.search_memories(
                            query=query,  # Use actual search query
                            limit=500,  # Get more to find recent ones
                            category=None
                        )

                        # Sort by created_at and limit
                        if all_memories:
                            sorted_memories = sorted(all_memories,
                                                   key=lambda x: x.get('created_at', ''),
                                                   reverse=True)
                            memories = sorted_memories[:limit]

                    except Exception as e:
                        logger.warning(f"Search fallback failed: {e}")

                if not memories and hasattr(self.storage, 'get_recent_memories'):
                    memories = await self.storage.get_recent_memories(limit=limit, hours=24*7)  # 7 days

                return JSONResponse({
                    "success": True,
                    "memories": memories,
                    "count": len(memories)
                })

            except Exception as e:
                logger.error(f"Recent pastes error: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)

        @self.mcp.custom_route("/comet-ai/exchange", methods=["POST"])
        async def comet_ai_exchange(request):
            """Unified data exchange endpoint for AI agents"""
            # No auth required - secured by Tailscale network
                
            try:
                data = await request.json()
                intent = data.get('intent', 'query')
                payload = data.get('payload', {})
                comet_meta = data.get('comet_meta', {})
                
                # Add automatic tagging (ChromaDB requires simple values, not lists)
                enhanced_meta = {
                    "processed_by": "haivemind",
                    "processed_at": datetime.now().isoformat(),
                    "tags": "#comet-ai,#ai-exchange",
                    "source": comet_meta.get("source", "unknown"),
                    "capabilities": ",".join(comet_meta.get("capabilities", [])) if comet_meta.get("capabilities") else ""
                }
                
                result = None
                
                if intent == "store":
                    # Store memory with Comet tagging
                    content = payload.get('content', '')
                    category = payload.get('category', 'comet_findings')
                    
                    if hasattr(self.storage, 'store_memory'):
                        memory_id = await self.storage.store_memory(
                            content=content,
                            category=category,
                            metadata=enhanced_meta
                        )
                        result = {"stored": True, "memory_id": memory_id}
                    else:
                        result = {"stored": False, "error": "Storage unavailable"}
                        
                elif intent == "query":
                    # Search memories or get recent messages
                    query = payload.get('content', '')
                    limit = payload.get('limit', 5)

                    if hasattr(self.storage, 'search_memories'):
                        # Check if this is a "get last messages" request or recent work request
                        if (query.lower().startswith('get last') and 'messages' in query.lower()) or 'recent work' in query.lower():
                            # Get recent memories from comet category first, then fall back to general
                            try:
                                memories = await self.storage.get_recent_memories(category='comet_handoff', limit=limit)
                                if not memories:
                                    # Fallback to global category
                                    memories = await self.storage.get_recent_memories(category='global', limit=limit)
                            except:
                                # Final fallback - get any recent memories
                                memories = await self.storage.get_recent_memories(limit=limit)
                        else:
                            # Normal search - search in comet categories first
                            try:
                                memories = await self.storage.search_memories(query, category='comet_handoff', limit=limit)
                                if not memories:
                                    # Also search in related comet categories
                                    comet_memories = []
                                    for cat in ['comet_findings', 'comet_results', 'comet_feedback']:
                                        try:
                                            cat_memories = await self.storage.search_memories(query, category=cat, limit=limit//3)
                                            comet_memories.extend(cat_memories)
                                        except:
                                            continue
                                    memories = comet_memories[:limit] if comet_memories else []
                                if not memories:
                                    # Final fallback - search all categories
                                    memories = await self.storage.search_memories(query, limit=limit)
                            except:
                                # Final fallback - search all categories
                                memories = await self.storage.search_memories(query, limit=limit)
                        result = {"memories": memories}
                    else:
                        result = {"memories": [], "error": "Search unavailable"}
                        
                elif intent == "execute":
                    # Execute task or delegate
                    task_type = payload.get('type', 'general')
                    content = payload.get('content', '')
                    
                    if task_type == 'task_delegation':
                        directive_id = self.comet_system.create_directive(
                            'comet_task', 
                            {"task": content, "source": "comet-browser"}, 
                            "normal"
                        )
                        result = {"executed": True, "directive_id": directive_id}
                    else:
                        result = {"executed": False, "error": "Unknown task type"}
                        
                else:
                    result = {"error": f"Unknown intent: {intent}"}

                response = {
                    "comet_meta": enhanced_meta,
                    "result": result,
                    "status": "processed"
                }
                
                return JSONResponse(response)
                
            except Exception as e:
                logger.error(f"Comet exchange error: {e}")
                return JSONResponse({
                    "error": str(e),
                    "comet_meta": {"source": "haivemind", "error": True}
                }, status_code=500)

        @self.mcp.custom_route("/comet-ai/stream", methods=["GET"])
        async def comet_ai_stream(request):
            """SSE stream for real-time bidirectional communication"""
            # No auth required - secured by Tailscale network
            
            async def generate_stream():
                yield "data: " + json.dumps({
                    "comet_meta": {
                        "source": "haivemind",
                        "stream": "connected",
                        "timestamp": datetime.now().isoformat()
                    },
                    "message": "Stream connected"
                }) + "\n\n"
                
                # Send periodic updates
                import asyncio
                try:
                    while True:
                        await asyncio.sleep(10)
                        
                        # Get system status updates
                        status_update = {
                            "comet_meta": {
                                "source": "haivemind",
                                "type": "status_update",
                                "timestamp": datetime.now().isoformat()
                            },
                            "active_directives": len(self.comet_system.active_directives),
                            "system_healthy": True
                        }
                        
                        yield "data: " + json.dumps(status_update) + "\n\n"
                        
                except Exception as e:
                    yield "data: " + json.dumps({
                        "comet_meta": {"source": "haivemind", "error": True},
                        "error": str(e)
                    }) + "\n\n"
            
            from starlette.responses import StreamingResponse
            return StreamingResponse(
                generate_stream(),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/event-stream"
                }
            )

        @self.mcp.custom_route("/comet-ai/sync", methods=["GET", "POST"])
        async def comet_ai_sync(request):
            """State synchronization for multi-agent handoff"""
            # No auth required - secured by Tailscale network
            
            try:
                if request.method == "GET":
                    # Get current state for handoff
                    sync_state = {
                        "comet_meta": {
                            "source": "haivemind",
                            "sync_type": "state_export",
                            "timestamp": datetime.now().isoformat()
                        },
                        "session_context": {
                            "created_at": datetime.now().isoformat(),
                            "last_activity": datetime.now().isoformat()
                        },
                        "active_directives": self.comet_system.active_directives,
                        "recent_memories": self.comet_system.get_session_memories(token, limit=5),
                        "context_preservation": {
                            "format_version": "1.0",
                            "compatible_agents": ["comet-browser", "claude-desktop", "martin-ai"],
                            "handoff_ready": True
                        }
                    }
                    
                    return JSONResponse(sync_state)
                    
                else:  # POST - restore state from another agent
                    data = await request.json()
                    imported_state = data.get('state', {})
                    source_agent = data.get('source_agent', 'unknown')
                    
                    # Process imported state
                    restored_items = []
                    
                    if 'context' in imported_state:
                        # Store context as memory
                        if hasattr(self.storage, 'store_memory'):
                            memory_id = await self.storage.store_memory(
                                content=f"Imported context from {source_agent}: {imported_state['context']}",
                                category="agent_handoff",
                                metadata={
                                    "source_agent": source_agent,
                                    "handoff_timestamp": datetime.now().isoformat(),
                                    "tags": "#agent-handoff,#comet-sync"  # Comma-separated string
                                }
                            )
                            restored_items.append({"type": "context", "memory_id": memory_id})
                    
                    if 'tasks' in imported_state:
                        # Create directives from imported tasks
                        for task in imported_state.get('tasks', []):
                            directive_id = self.comet_system.create_directive(
                                'imported_task',
                                {
                                    "task": task,
                                    "source_agent": source_agent,
                                    "imported_at": datetime.now().isoformat()
                                },
                                "normal"
                            )
                            restored_items.append({"type": "task", "directive_id": directive_id})
                    
                    response = {
                        "comet_meta": {
                            "source": "haivemind",
                            "sync_type": "state_import",
                            "timestamp": datetime.now().isoformat()
                        },
                        "import_successful": True,
                        "restored_items": restored_items,
                        "source_agent": source_agent
                    }
                    
                    return JSONResponse(response)
                    
            except Exception as e:
                logger.error(f"Comet sync error: {e}")
                return JSONResponse({
                    "error": str(e),
                    "comet_meta": {"source": "haivemind", "error": True}
                }, status_code=500)

        logger.info("ðŸš€ Comet# AI-optimized portal registered - simplified interface active")
        
        # Playbook management dashboard endpoint
        @self.mcp.custom_route("/admin/playbooks", methods=["GET"])
        async def playbooks_dashboard(request):
            """Serve the playbook management dashboard interface"""
            try:
                return HTMLResponse("""
                <!DOCTYPE html>
                <html>
                <head>
                    <title>Playbook Management - hAIveMind</title>
                    <link rel="stylesheet" href="/admin/static/admin.css">
                    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
                </head>
                <body class="dashboard">
                    <nav class="nav-header">
                        <div class="nav-brand">
                            <img src="/assets/logo.png" alt="hAIveMind" class="nav-logo">
                            <h1>hAIveMind Playbook Management</h1>
                        </div>
                        <div class="nav-links">
                            <a href="/admin/dashboard.html">Dashboard</a>
                            <a href="/admin/memory.html">Memory Browser</a>
                            <a href="/admin/mcp_servers.html">MCP Servers</a>
                            <a href="/admin/vault">Vault Management</a>
                            <a href="/admin/rules-dashboard">Rules & Governance</a>
                            <a href="/admin/playbooks" class="active">Playbook Management</a>
                            <a href="/admin/executions">Execution Monitoring</a>
                            <a href="/admin/confluence">Confluence Integration</a>
                            <a href="/admin/help-dashboard">Help System</a>
                        </div>
                        <button class="logout-btn" onclick="logout()">Logout</button>
                    </nav>
                    <main class="main-content">
                        <div class="card">
                            <h2><i class="fas fa-book"></i> Playbook Management</h2>
                            <p>Create, manage, and execute automated playbooks for DevOps operations.</p>
                            
                            <!-- Playbook interface will be dynamically loaded by playbooks.js -->
                            <div id="playbook-loading" class="loading" style="text-align: center; padding: 2rem;">
                                <i class="fas fa-spinner fa-spin fa-2x"></i>
                                <p>Loading playbook management interface...</p>
                            </div>
                        </div>
                    </main>
                    <script src="/admin/static/admin.js"></script>
                    <script src="/admin/static/playbooks.js"></script>
                </body>
                </html>
                """)
            except Exception as e:
                logger.error(f"Error serving playbook dashboard: {e}")
                return JSONResponse({"error": "Playbook dashboard unavailable"}, status_code=500)
        
        # Rules dashboard endpoint (must come before catch-all admin route)
        @self.mcp.custom_route("/admin/rules-dashboard", methods=["GET"])
        async def rules_dashboard(request):
            """Serve the rules dashboard interface"""
            try:
                rules_dashboard_path = Path(__file__).parent.parent / "templates" / "rules_dashboard.html"
                if rules_dashboard_path.exists():
                    return FileResponse(str(rules_dashboard_path), media_type="text/html")
                else:
                    # Fallback to basic rules interface
                    return HTMLResponse("""
                    <!DOCTYPE html>
                    <html>
                    <head>
                        <title>Rules & Governance - hAIveMind</title>
                        <link rel="stylesheet" href="/admin/static/admin.css">
                    </head>
                    <body class="dashboard">
                        <nav class="nav-header">
                            <div class="nav-brand">
                                <img src="/assets/logo.png" alt="hAIveMind" class="nav-logo">
                                <h1>hAIveMind Rules & Governance</h1>
                            </div>
                            <div class="nav-links">
                                <a href="/admin/dashboard.html">Dashboard</a>
                                <a href="/admin/memory.html">Memory Browser</a>
                                <a href="/admin/mcp_servers.html">MCP Servers</a>
                                <a href="/admin/vault">Vault Management</a>
                                <a href="/admin/rules-dashboard" class="active">Rules & Governance</a>
                                <a href="/admin/playbooks">Playbook Management</a>
                                <a href="/admin/executions">Execution Monitoring</a>
                                <a href="/admin/confluence">Confluence Integration</a>
                                <a href="/admin/help-dashboard">Help System</a>
                            </div>
                            <button class="logout-btn" onclick="logout()">Logout</button>
                        </nav>
                        <main class="main-content">
                            <div class="card">
                                <h2>ðŸŽ¯ Rules & Governance System</h2>
                                <p>Comprehensive rule management for consistent agent behavior across the hAIveMind network.</p>
                                
                                <div class="feature-grid">
                                    <div class="feature-card">
                                        <h3><i class="fas fa-plus"></i> Create Rules</h3>
                                        <p>Visual rule builder with templates and validation</p>
                                        <button class="btn btn-primary" onclick="alert('Rule builder coming soon!')">Create New Rule</button>
                                    </div>
                                    <div class="feature-card">
                                        <h3><i class="fas fa-list"></i> Rule Catalog</h3>
                                        <p>Browse and manage all rules with filtering and search</p>
                                        <button class="btn btn-secondary" onclick="alert('Rule catalog coming soon!')">Browse Rules</button>
                                    </div>
                                    <div class="feature-card">
                                        <h3><i class="fas fa-chart-line"></i> Analytics</h3>
                                        <p>Rule performance analytics and optimization insights</p>
                                        <button class="btn btn-info" onclick="alert('Analytics coming soon!')">View Analytics</button>
                                    </div>
                                    <div class="feature-card">
                                        <h3><i class="fas fa-shield-alt"></i> Compliance</h3>
                                        <p>Compliance monitoring and audit trails</p>
                                        <button class="btn btn-warning" onclick="alert('Compliance tracking coming soon!')">View Compliance</button>
                                    </div>
                                </div>
                            </div>
                        </main>
                        <script src="/admin/static/admin.js"></script>
                    </body>
                    </html>
                    """)
            except Exception as e:
                logger.error(f"Error serving rules dashboard: {e}")
                return JSONResponse({"error": "Rules dashboard unavailable"}, status_code=500)
        
        # Serve admin HTML pages (with validation)
        @self.mcp.custom_route("/admin/{page}", methods=["GET"])
        async def admin_pages(request):
            try:
                page = request.path_params.get("page", "")
                
                # Handle root admin access
                if page == "" or page == "/":
                    page = "dashboard.html"


                # Only allow simple filenames without path separators
                if not re.fullmatch(r"[A-Za-z0-9_-]+(?:\.html)?", page):
                    return JSONResponse({"error": "Invalid page"}, status_code=400)

                # Enforce .html extension
                if not page.endswith(".html"):
                    page = f"{page}.html"

                base_dir = Path(admin_dir).resolve()
                candidate = (base_dir / page).resolve()

                # Ensure the resolved path is within the admin directory
                if not str(candidate).startswith(str(base_dir) + os.sep) and candidate != base_dir:
                    return JSONResponse({"error": "Invalid page"}, status_code=400)

                if candidate.is_file():
                    return FileResponse(str(candidate))
                return JSONResponse({"error": "Page not found"}, status_code=404)
            except Exception:
                return JSONResponse({"error": "Page not found"}, status_code=404)
        
        # Root admin route - redirect to dashboard
        @self.mcp.custom_route("/admin/", methods=["GET"])
        @self.mcp.custom_route("/admin", methods=["GET"])
        async def admin_root(request):
            return FileResponse(str(Path(admin_dir) / "dashboard.html"))
        
        # Admin login endpoint
        @self.mcp.custom_route("/admin/api/login", methods=["POST"])
        async def admin_login(request):
            try:
                data = await request.json()
                username = data.get('username')
                password = data.get('password')
                
                if self.auth.validate_admin_login(username, password):
                    token = self.auth.generate_jwt_token({
                        'username': username,
                        'role': 'admin'
                    })
                    return JSONResponse({"token": token, "status": "success"})
                else:
                    return JSONResponse({"error": "Invalid credentials"}, status_code=401)
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # API v1 auth login endpoint (used by login.html)
        @self.mcp.custom_route("/api/v1/auth/login", methods=["POST"])
        async def api_v1_login(request):
            try:
                data = await request.json()
                username = data.get('username')
                password = data.get('password')
                
                if self.auth.validate_admin_login(username, password):
                    token = self.auth.generate_jwt_token({
                        'username': username,
                        'role': 'admin'
                    })
                    return JSONResponse({"token": token, "status": "success"})
                else:
                    return JSONResponse({"error": "Invalid credentials"}, status_code=401)
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Token verification
        @self.mcp.custom_route("/admin/api/verify", methods=["GET"])
        async def verify_token(request):
            auth_header = request.headers.get('Authorization')
            if not auth_header or not auth_header.startswith('Bearer '):
                return JSONResponse({"error": "No token provided"}, status_code=401)
            
            token = auth_header.split(' ')[1]
            valid, payload = self.auth.validate_jwt_token(token)
            
            if valid:
                return JSONResponse({"status": "valid", "user": payload})
            else:
                return JSONResponse({"error": "Invalid token"}, status_code=401)
        
        # System stats
        @self.mcp.custom_route("/admin/api/stats", methods=["GET"])
        async def admin_stats(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Get agent roster
                agents = await self.storage.get_agent_roster()
                active_agents = len(agents.get('agents', []))
                
                # Get memory stats
                memory_stats = self.storage.get_collection_info()
                total_memories = sum(collection.get('count', 0) for collection in memory_stats.values())
                
                return JSONResponse({
                    "active_agents": active_agents,
                    "total_memories": total_memories,
                    "uptime": "Running",
                    "network_status": "Connected"
                })
            except Exception as e:
                logger.error(f"Error getting stats: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Agent management
        @self.mcp.custom_route("/admin/api/agents", methods=["GET"])
        async def get_agents(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                roster = await self.storage.get_agent_roster()
                return JSONResponse(roster.get('agents', []))
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Memory search
        @self.mcp.custom_route("/admin/api/memory/search", methods=["GET"])
        async def memory_search(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                query = request.query_params.get('query', '')
                category = request.query_params.get('category')
                limit = int(request.query_params.get('limit', 20))
                semantic = request.query_params.get('semantic', 'true').lower() == 'true'
                
                memories = await self.storage.search_memories(
                    query=query,
                    category=category,
                    limit=limit,
                    semantic=semantic
                )
                return JSONResponse(memories)
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Add memory
        @self.mcp.custom_route("/admin/api/memory", methods=["POST"])
        async def add_memory(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                memory_id = await self.storage.store_memory(
                    content=data['content'],
                    category=data.get('category', 'global'),
                    context=data.get('context'),
                    tags=data.get('tags')
                )
                return JSONResponse({"memory_id": memory_id, "status": "success"})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        logger.info("ðŸ”§ Admin web interface routes registered")
        
        # Initialize Vault Dashboard API
        self._init_vault_api()
        
        # Initialize Help System API
        self._init_help_api()
        
        # Initialize Config Backup & Agent Kanban API
        self._init_config_backup_api()
    
    def _init_vault_api(self):
        """Initialize Vault Dashboard API endpoints"""
        try:
            import sqlite3
            import os
            
            # Initialize simple vault storage
            vault_db_path = os.path.join(os.path.dirname(__file__), "..", "data", "vault.db")
            os.makedirs(os.path.dirname(vault_db_path), exist_ok=True)
            
            # Create simple vault storage
            self.vault_db = vault_db_path
            self._init_vault_db()
            self._init_config_backup_db()
            self.vault_unlocked = False
            self.vault_session_expires = None
            
            # Add vault routes with /admin prefix
            @self.mcp.custom_route("/admin/api/vault/status", methods=["GET"])
            async def vault_status(request):
                from starlette.responses import JSONResponse
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    stats = self._get_vault_stats()
                    return JSONResponse(stats)
                except Exception as e:
                    logger.error(f"Error getting vault status: {e}")
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            @self.mcp.custom_route("/admin/api/vault/unlock", methods=["POST"])
            async def vault_unlock(request):
                from starlette.responses import JSONResponse
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    data = await request.json()
                    master_password = data.get('master_password')
                    remember_session = data.get('remember_session', False)
                    
                    # Simple vault unlock - in production this would verify against encrypted master key
                    if master_password == "admin123":  # Demo password - should be encrypted in production
                        self.vault_unlocked = True
                        session_duration = 3600 if remember_session else 1800  # 1 hour or 30 min
                        self.vault_session_expires = datetime.now() + timedelta(seconds=session_duration)
                        
                        return JSONResponse({
                            "success": True,
                            "message": "Vault unlocked successfully",
                            "session_expires": self.vault_session_expires.isoformat()
                        })
                    else:
                        return JSONResponse({"error": "Invalid master password"}, status_code=401)
                    
                except Exception as e:
                    logger.error(f"Error unlocking vault: {e}")
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            @self.mcp.custom_route("/admin/api/vault/credentials", methods=["GET"])
            async def list_credentials(request):
                from starlette.responses import JSONResponse
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    if not self._check_vault_unlocked():
                        return JSONResponse({"error": "Vault is locked"}, status_code=403)
                    
                    credentials = self._list_credentials()
                    return JSONResponse({
                        "credentials": credentials,
                        "total": len(credentials)
                    })
                    
                except Exception as e:
                    logger.error(f"Error listing credentials: {e}")
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            @self.mcp.custom_route("/admin/api/vault/credentials", methods=["POST"])
            async def create_credential(request):
                from starlette.responses import JSONResponse
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    if not self._check_vault_unlocked():
                        return JSONResponse({"error": "Vault is locked"}, status_code=403)
                    
                    data = await request.json()
                    credential_id = self._create_credential(data)
                    
                    # Store in hAIveMind memory
                    await self.storage.store_memory(
                        content=f"Vault credential created: {data.get('name')} ({data.get('type')})",
                        category="security",
                        context="Vault credential management",
                        tags=["vault", "credential", "security"]
                    )
                    
                    return JSONResponse({
                        "success": True,
                        "credential_id": credential_id,
                        "message": "Credential created successfully"
                    })
                    
                except Exception as e:
                    logger.error(f"Error creating credential: {e}")
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            @self.mcp.custom_route("/admin/api/vault/credentials/{credential_id}", methods=["GET"])
            async def get_credential(request):
                from starlette.responses import JSONResponse
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    if not self._check_vault_unlocked():
                        return JSONResponse({"error": "Vault is locked"}, status_code=403)
                    
                    credential_id = request.path_params.get('credential_id')
                    credential = self._get_credential(credential_id)
                    if credential:
                        return JSONResponse(credential)
                    else:
                        return JSONResponse({"error": "Credential not found"}, status_code=404)
                    
                except Exception as e:
                    logger.error(f"Error getting credential: {e}")
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            @self.mcp.custom_route("/admin/api/vault/audit", methods=["GET"])
            async def vault_audit_log(request):
                from starlette.responses import JSONResponse
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    if not self._check_vault_unlocked():
                        return JSONResponse({"error": "Vault is locked"}, status_code=403)
                    
                    audit_entries = self._get_audit_log()
                    return JSONResponse({
                        "audit_log": audit_entries,
                        "total": len(audit_entries)
                    })
                    
                except Exception as e:
                    logger.error(f"Error getting audit log: {e}")
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            logger.info("ðŸ” Vault Dashboard API endpoints registered")
            
        except Exception as e:
            logger.warning(f"Vault API not available: {e}")
    
    def _init_vault_db(self):
        """Initialize the vault SQLite database"""
        import sqlite3
        conn = sqlite3.connect(self.vault_db)
        cursor = conn.cursor()
        
        # Create credentials table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS credentials (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                type TEXT NOT NULL,
                environment TEXT,
                service TEXT,
                description TEXT,
                encrypted_data TEXT NOT NULL,
                tags TEXT,
                project TEXT,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                expires_at TEXT,
                status TEXT DEFAULT 'active'
            )
        ''')
        
        # Create audit log table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS audit_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                action TEXT NOT NULL,
                credential_id TEXT,
                user_id TEXT,
                details TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _init_config_backup_db(self):
        """Initialize the config backup SQLite database"""
        import sqlite3
        import os
        
        # Use same database for simplicity
        conn = sqlite3.connect(self.vault_db)
        cursor = conn.cursor()
        
        # Create config systems table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS config_systems (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                type TEXT NOT NULL,
                description TEXT,
                machine_id TEXT,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                status TEXT DEFAULT 'active'
            )
        ''')
        
        # Create config snapshots table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS config_snapshots (
                id TEXT PRIMARY KEY,
                system_id TEXT NOT NULL,
                config_type TEXT NOT NULL,
                config_content TEXT NOT NULL,
                config_hash TEXT NOT NULL,
                agent_id TEXT,
                timestamp TEXT NOT NULL,
                tags TEXT,
                description TEXT,
                size INTEGER,
                FOREIGN KEY (system_id) REFERENCES config_systems (id)
            )
        ''')
        
        # Create config diffs table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS config_diffs (
                id TEXT PRIMARY KEY,
                from_snapshot_id TEXT NOT NULL,
                to_snapshot_id TEXT NOT NULL,
                diff_content TEXT NOT NULL,
                change_type TEXT NOT NULL,
                lines_added INTEGER DEFAULT 0,
                lines_removed INTEGER DEFAULT 0,
                risk_score INTEGER DEFAULT 0,
                timestamp TEXT NOT NULL,
                FOREIGN KEY (from_snapshot_id) REFERENCES config_snapshots (id),
                FOREIGN KEY (to_snapshot_id) REFERENCES config_snapshots (id)
            )
        ''')
        
        # Create agent tasks table for kanban system
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS agent_tasks (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                description TEXT,
                assigned_agent TEXT,
                status TEXT DEFAULT 'backlog',
                priority INTEGER DEFAULT 3,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                due_date TEXT,
                estimated_hours REAL,
                actual_hours REAL,
                tags TEXT,
                board_id TEXT DEFAULT 'default'
            )
        ''')
        
        # Create agent registry table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS agent_registry (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                role TEXT NOT NULL,
                capabilities TEXT,
                machine_id TEXT,
                status TEXT DEFAULT 'active',
                current_workload INTEGER DEFAULT 0,
                max_workload INTEGER DEFAULT 5,
                last_seen TEXT,
                registered_at TEXT NOT NULL,
                metadata TEXT
            )
        ''')
        
        # Create task dependencies table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS task_dependencies (
                id TEXT PRIMARY KEY,
                task_id TEXT NOT NULL,
                depends_on_task_id TEXT NOT NULL,
                dependency_type TEXT DEFAULT 'blocks',
                created_at TEXT NOT NULL,
                FOREIGN KEY (task_id) REFERENCES agent_tasks (id),
                FOREIGN KEY (depends_on_task_id) REFERENCES agent_tasks (id)
            )
        ''')
        
        # Create indexes for performance
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_config_snapshots_system_id ON config_snapshots (system_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_config_snapshots_timestamp ON config_snapshots (timestamp)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_config_snapshots_hash ON config_snapshots (config_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_agent_tasks_status ON agent_tasks (status)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_agent_tasks_assigned ON agent_tasks (assigned_agent)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_task_dependencies_task_id ON task_dependencies (task_id)')
        
        conn.commit()
        conn.close()
    
    def _check_vault_unlocked(self):
        """Check if vault is unlocked and session is valid"""
        if not self.vault_unlocked:
            return False
        
        if self.vault_session_expires and datetime.now() > self.vault_session_expires:
            self.vault_unlocked = False
            return False
            
        return True
    
    def _get_vault_stats(self):
        """Get vault statistics"""
        import sqlite3
        conn = sqlite3.connect(self.vault_db)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM credentials WHERE status = 'active'")
        total_credentials = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM credentials WHERE expires_at < datetime('now', '+30 days') AND status = 'active'")
        expiring_credentials = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            "vault_status": "unlocked" if self.vault_unlocked else "locked",
            "total_credentials": total_credentials,
            "expiring_credentials": expiring_credentials,
            "session_expires": self.vault_session_expires.isoformat() if self.vault_session_expires else None
        }
    
    def _list_credentials(self):
        """List all credentials"""
        import sqlite3
        conn = sqlite3.connect(self.vault_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT id, name, type, environment, service, description, tags, project, 
                   created_at, updated_at, expires_at, status 
            FROM credentials WHERE status = 'active' ORDER BY name
        ''')
        
        columns = ['id', 'name', 'type', 'environment', 'service', 'description', 
                  'tags', 'project', 'created_at', 'updated_at', 'expires_at', 'status']
        
        credentials = []
        for row in cursor.fetchall():
            credential = dict(zip(columns, row))
            # Don't include encrypted data in list view
            credentials.append(credential)
        
        conn.close()
        return credentials
    
    def _create_credential(self, data):
        """Create a new credential"""
        import sqlite3
        import uuid
        import json
        from datetime import datetime
        
        credential_id = str(uuid.uuid4())
        now = datetime.now().isoformat()
        
        # Simple encryption (in production, use proper encryption)
        encrypted_data = json.dumps(data.get('credentials', {}))
        
        conn = sqlite3.connect(self.vault_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO credentials (id, name, type, environment, service, description, 
                                   encrypted_data, tags, project, created_at, updated_at, expires_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            credential_id, data.get('name'), data.get('type'), data.get('environment'),
            data.get('service'), data.get('description'), encrypted_data,
            data.get('tags'), data.get('project'), now, now, data.get('expires_at')
        ))
        
        # Add audit log entry
        cursor.execute('''
            INSERT INTO audit_log (timestamp, action, credential_id, user_id, details)
            VALUES (?, ?, ?, ?, ?)
        ''', (now, 'create', credential_id, 'admin', f"Created credential: {data.get('name')}"))
        
        conn.commit()
        conn.close()
        
        return credential_id
    
    def _get_credential(self, credential_id):
        """Get a specific credential"""
        import sqlite3
        import json
        
        conn = sqlite3.connect(self.vault_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT id, name, type, environment, service, description, encrypted_data,
                   tags, project, created_at, updated_at, expires_at, status
            FROM credentials WHERE id = ? AND status = 'active'
        ''', (credential_id,))
        
        row = cursor.fetchone()
        if row:
            columns = ['id', 'name', 'type', 'environment', 'service', 'description', 
                      'encrypted_data', 'tags', 'project', 'created_at', 'updated_at', 
                      'expires_at', 'status']
            credential = dict(zip(columns, row))
            
            # Decrypt data (in production, use proper decryption)
            try:
                credential['credentials'] = json.loads(credential['encrypted_data'])
            except:
                credential['credentials'] = {}
            
            # Don't send encrypted_data in response
            del credential['encrypted_data']
            
            # Add audit log entry for access
            now = datetime.now().isoformat()
            cursor.execute('''
                INSERT INTO audit_log (timestamp, action, credential_id, user_id, details)
                VALUES (?, ?, ?, ?, ?)
            ''', (now, 'read', credential_id, 'admin', f"Accessed credential: {credential['name']}"))
            
            conn.commit()
            conn.close()
            return credential
        
        conn.close()
        return None
    
    def _get_audit_log(self):
        """Get audit log entries"""
        import sqlite3
        
        conn = sqlite3.connect(self.vault_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT timestamp, action, credential_id, user_id, details
            FROM audit_log ORDER BY timestamp DESC LIMIT 100
        ''')
        
        columns = ['timestamp', 'action', 'credential_id', 'user_id', 'details']
        audit_entries = []
        for row in cursor.fetchall():
            audit_entries.append(dict(zip(columns, row)))
        
        conn.close()
        return audit_entries
    
    def _init_help_api(self):
        """Initialize Help System API endpoints"""
        from starlette.responses import JSONResponse
        try:
            import sys
            import os
            sys.path.insert(0, os.path.dirname(__file__))
            from help_dashboard import HelpSystemDashboard
            from interactive_help_system import InteractiveHelpSystem
            
            # Initialize help system components
            self.help_system = InteractiveHelpSystem(self.storage, self.config)
            self.help_dashboard = HelpSystemDashboard(self.storage, self.config)
            
            # Help system status endpoint
            @self.mcp.custom_route("/admin/api/help/status", methods=["GET"])
            async def help_status(request):
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    await self.help_system.initialize()
                    analytics = await self.help_system.get_help_analytics()
                    return JSONResponse({
                        "status": "operational",
                        "analytics": analytics,
                        "commands_loaded": len(self.help_system._command_cache),
                        "examples_loaded": len(self.help_system._examples_cache)
                    })
                except Exception as e:
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            # Search help content endpoint
            @self.mcp.custom_route("/admin/api/help/search", methods=["GET"])
            async def help_search(request):
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    query = request.query_params.get('q', '')
                    if not query:
                        return JSONResponse({"error": "Query parameter 'q' is required"}, status_code=400)
                    
                    await self.help_system.initialize()
                    results = await self.help_system.search_help_content(query)
                    return JSONResponse({"results": results})
                except Exception as e:
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            # Get help categories endpoint
            @self.mcp.custom_route("/admin/api/help/categories", methods=["GET"])
            async def help_categories(request):
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    await self.help_system.initialize()
                    categories = await self.help_system.get_help_categories()
                    return JSONResponse({"categories": categories})
                except Exception as e:
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            # Get specific help article endpoint
            @self.mcp.custom_route("/admin/api/help/article/{article_id}", methods=["GET"])
            async def help_article(request):
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    article_id = request.path_params.get('article_id')
                    await self.help_system.initialize()
                    article = await self.help_system.get_help_article(article_id)
                    if article:
                        return JSONResponse({"article": article})
                    else:
                        return JSONResponse({"error": "Article not found"}, status_code=404)
                except Exception as e:
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            # Get contextual help for specific page endpoint
            @self.mcp.custom_route("/admin/api/help/context/{page}", methods=["GET"])
            async def help_context(request):
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    page = request.path_params.get('page')
                    await self.help_system.initialize()
                    context_help = await self.help_system.get_contextual_help(page)
                    return JSONResponse({"context_help": context_help})
                except Exception as e:
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            # Submit help feedback endpoint
            @self.mcp.custom_route("/admin/api/help/feedback", methods=["POST"])
            async def help_feedback(request):
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    data = await request.json()
                    feedback_stored = await self.help_system.store_feedback(data)
                    if feedback_stored:
                        return JSONResponse({"success": True, "message": "Feedback stored successfully"})
                    else:
                        return JSONResponse({"error": "Failed to store feedback"}, status_code=500)
                except Exception as e:
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            # Help analytics dashboard data endpoint
            @self.mcp.custom_route("/admin/api/help/analytics", methods=["GET"])
            async def help_analytics_data(request):
                if not await self._check_admin_auth(request):
                    return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
                try:
                    dashboard_data = await self.help_dashboard.get_dashboard_data()
                    return JSONResponse(dashboard_data)
                except Exception as e:
                    return JSONResponse({"error": str(e)}, status_code=500)
            
            logger.info("ðŸ“š Help System API endpoints registered")
            
        except ImportError as e:
            logger.warning(f"Help system dependencies not available: {e}")
        except Exception as e:
            logger.error(f"Failed to initialize help API: {e}")
    
    def _init_config_backup_api(self):
        """Initialize Config Backup & Agent Kanban API endpoints"""
        from starlette.responses import JSONResponse
        import sqlite3
        import json
        import hashlib
        import difflib
        from datetime import datetime
        import uuid
        
        # Config backup endpoints
        @self.mcp.custom_route("/admin/api/configs/backup", methods=["POST"])
        async def create_config_backup(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                system_id = data.get('system_id')
                config_type = data.get('config_type')
                config_content = data.get('config_content')
                agent_id = data.get('agent_id')
                description = data.get('description', '')
                tags = json.dumps(data.get('tags', []))
                
                if not all([system_id, config_type, config_content]):
                    return JSONResponse({"error": "Missing required fields"}, status_code=400)
                
                # Calculate hash for deduplication
                config_hash = hashlib.sha256(config_content.encode()).hexdigest()
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                # Check if identical config already exists
                cursor.execute('''
                    SELECT id FROM config_snapshots 
                    WHERE system_id = ? AND config_hash = ?
                    ORDER BY timestamp DESC LIMIT 1
                ''', (system_id, config_hash))
                
                existing = cursor.fetchone()
                if existing:
                    conn.close()
                    return JSONResponse({
                        "message": "Identical config already exists",
                        "snapshot_id": existing[0],
                        "deduplicated": True
                    })
                
                # Create new snapshot
                snapshot_id = str(uuid.uuid4())
                timestamp = datetime.utcnow().isoformat()
                
                cursor.execute('''
                    INSERT INTO config_snapshots 
                    (id, system_id, config_type, config_content, config_hash, 
                     agent_id, timestamp, tags, description, size)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (snapshot_id, system_id, config_type, config_content, 
                     config_hash, agent_id, timestamp, tags, description, len(config_content)))
                
                conn.commit()
                conn.close()
                
                return JSONResponse({
                    "snapshot_id": snapshot_id,
                    "message": "Config backup created successfully",
                    "timestamp": timestamp,
                    "size": len(config_content)
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/configs/{system_id}/history", methods=["GET"])
        async def get_config_history(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                system_id = request.path_params.get('system_id')
                limit = int(request.query_params.get('limit', 50))
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT id, config_type, config_hash, agent_id, timestamp, 
                           description, size, tags
                    FROM config_snapshots 
                    WHERE system_id = ? 
                    ORDER BY timestamp DESC 
                    LIMIT ?
                ''', (system_id, limit))
                
                columns = ['id', 'config_type', 'config_hash', 'agent_id', 
                          'timestamp', 'description', 'size', 'tags']
                history = []
                for row in cursor.fetchall():
                    entry = dict(zip(columns, row))
                    entry['tags'] = json.loads(entry['tags']) if entry['tags'] else []
                    history.append(entry)
                
                conn.close()
                return JSONResponse({"history": history, "total": len(history)})
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/configs/{system_id}/current", methods=["GET"])
        async def get_current_config(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                system_id = request.path_params.get('system_id')
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT id, config_type, config_content, config_hash, agent_id, 
                           timestamp, description, size, tags
                    FROM config_snapshots 
                    WHERE system_id = ? 
                    ORDER BY timestamp DESC 
                    LIMIT 1
                ''', (system_id,))
                
                row = cursor.fetchone()
                if not row:
                    conn.close()
                    return JSONResponse({"error": "No config found for system"}, status_code=404)
                
                columns = ['id', 'config_type', 'config_content', 'config_hash', 
                          'agent_id', 'timestamp', 'description', 'size', 'tags']
                current_config = dict(zip(columns, row))
                current_config['tags'] = json.loads(current_config['tags']) if current_config['tags'] else []
                
                conn.close()
                return JSONResponse({"config": current_config})
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/configs/restore", methods=["POST"])
        async def restore_config(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                snapshot_id = data.get('snapshot_id')
                agent_id = data.get('agent_id')
                
                if not snapshot_id:
                    return JSONResponse({"error": "snapshot_id required"}, status_code=400)
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                # Get snapshot to restore
                cursor.execute('''
                    SELECT system_id, config_type, config_content
                    FROM config_snapshots WHERE id = ?
                ''', (snapshot_id,))
                
                row = cursor.fetchone()
                if not row:
                    conn.close()
                    return JSONResponse({"error": "Snapshot not found"}, status_code=404)
                
                system_id, config_type, config_content = row
                
                # Create new "restore" snapshot 
                restore_snapshot_id = str(uuid.uuid4())
                timestamp = datetime.utcnow().isoformat()
                config_hash = hashlib.sha256(config_content.encode()).hexdigest()
                
                cursor.execute('''
                    INSERT INTO config_snapshots 
                    (id, system_id, config_type, config_content, config_hash,
                     agent_id, timestamp, description, size)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (restore_snapshot_id, system_id, config_type, config_content,
                     config_hash, agent_id, timestamp, f"Restored from {snapshot_id}", 
                     len(config_content)))
                
                conn.commit()
                conn.close()
                
                return JSONResponse({
                    "message": "Config restored successfully",
                    "new_snapshot_id": restore_snapshot_id,
                    "restored_from": snapshot_id,
                    "system_id": system_id,
                    "timestamp": timestamp
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/configs/drift", methods=["GET"])
        async def detect_config_drift(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                system_id = request.query_params.get('system_id')
                hours = int(request.query_params.get('hours', 24))
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                if system_id:
                    # Check specific system
                    cursor.execute('''
                        SELECT id, config_hash, timestamp FROM config_snapshots 
                        WHERE system_id = ? 
                        ORDER BY timestamp DESC LIMIT 2
                    ''', (system_id,))
                    
                    results = cursor.fetchall()
                    if len(results) < 2:
                        return JSONResponse({"drift_detected": False, "message": "Not enough snapshots to compare"})
                    
                    latest_hash = results[0][1]
                    previous_hash = results[1][1]
                    
                    drift_detected = latest_hash != previous_hash
                    
                    return JSONResponse({
                        "drift_detected": drift_detected,
                        "system_id": system_id,
                        "latest_snapshot": results[0][0],
                        "previous_snapshot": results[1][0],
                        "latest_timestamp": results[0][2],
                        "previous_timestamp": results[1][2]
                    })
                else:
                    # Check all systems for drift
                    from datetime import datetime, timedelta
                    cutoff_time = (datetime.utcnow() - timedelta(hours=hours)).isoformat()
                    
                    cursor.execute('''
                        SELECT DISTINCT system_id FROM config_snapshots 
                        WHERE timestamp >= ?
                    ''', (cutoff_time,))
                    
                    drift_systems = []
                    for (sys_id,) in cursor.fetchall():
                        cursor.execute('''
                            SELECT id, config_hash, timestamp FROM config_snapshots 
                            WHERE system_id = ? AND timestamp >= ?
                            ORDER BY timestamp DESC LIMIT 2
                        ''', (sys_id, cutoff_time))
                        
                        snapshots = cursor.fetchall()
                        if len(snapshots) >= 2 and snapshots[0][1] != snapshots[1][1]:
                            drift_systems.append({
                                "system_id": sys_id,
                                "latest_snapshot": snapshots[0][0],
                                "latest_timestamp": snapshots[0][2]
                            })
                    
                    conn.close()
                    return JSONResponse({
                        "drift_detected": len(drift_systems) > 0,
                        "total_systems_with_drift": len(drift_systems),
                        "systems": drift_systems
                    })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/configs/diff/{id1}/{id2}", methods=["GET"])
        async def compare_configs(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                id1 = request.path_params.get('id1')
                id2 = request.path_params.get('id2')
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                # Get both configs
                cursor.execute('''
                    SELECT id, system_id, config_content, timestamp, description
                    FROM config_snapshots WHERE id IN (?, ?)
                ''', (id1, id2))
                
                results = cursor.fetchall()
                if len(results) != 2:
                    conn.close()
                    return JSONResponse({"error": "One or both snapshots not found"}, status_code=404)
                
                config1 = {
                    "id": results[0][0],
                    "system_id": results[0][1], 
                    "content": results[0][2],
                    "timestamp": results[0][3],
                    "description": results[0][4]
                }
                
                config2 = {
                    "id": results[1][0],
                    "system_id": results[1][1],
                    "content": results[1][2], 
                    "timestamp": results[1][3],
                    "description": results[1][4]
                }
                
                # Generate diff
                diff = list(difflib.unified_diff(
                    config1["content"].splitlines(keepends=True),
                    config2["content"].splitlines(keepends=True),
                    fromfile=f"{config1['id']} ({config1['timestamp']})",
                    tofile=f"{config2['id']} ({config2['timestamp']})",
                    n=3
                ))
                
                # Calculate stats
                lines_added = sum(1 for line in diff if line.startswith('+') and not line.startswith('+++'))
                lines_removed = sum(1 for line in diff if line.startswith('-') and not line.startswith('---'))
                
                conn.close()
                return JSONResponse({
                    "config1": config1,
                    "config2": config2,
                    "diff": ''.join(diff),
                    "stats": {
                        "lines_added": lines_added,
                        "lines_removed": lines_removed,
                        "total_changes": lines_added + lines_removed
                    }
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Agent Kanban Task Management endpoints
        @self.mcp.custom_route("/admin/api/agents/tasks", methods=["POST"])
        async def create_agent_task(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                title = data.get('title')
                description = data.get('description', '')
                assigned_agent = data.get('assigned_agent')
                priority = int(data.get('priority', 3))
                estimated_hours = data.get('estimated_hours')
                due_date = data.get('due_date')
                tags = json.dumps(data.get('tags', []))
                board_id = data.get('board_id', 'default')
                
                if not title:
                    return JSONResponse({"error": "Title is required"}, status_code=400)
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                task_id = str(uuid.uuid4())
                timestamp = datetime.utcnow().isoformat()
                
                cursor.execute('''
                    INSERT INTO agent_tasks 
                    (id, title, description, assigned_agent, status, priority,
                     created_at, updated_at, due_date, estimated_hours, tags, board_id)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (task_id, title, description, assigned_agent, 'backlog', priority,
                     timestamp, timestamp, due_date, estimated_hours, tags, board_id))
                
                conn.commit()
                conn.close()
                
                return JSONResponse({
                    "task_id": task_id,
                    "message": "Task created successfully",
                    "status": "backlog",
                    "created_at": timestamp
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/agents/tasks/board", methods=["GET"])
        async def get_kanban_board(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                board_id = request.query_params.get('board_id', 'default')
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT id, title, description, assigned_agent, status, priority,
                           created_at, updated_at, due_date, estimated_hours, actual_hours, tags
                    FROM agent_tasks WHERE board_id = ?
                    ORDER BY priority DESC, created_at DESC
                ''', (board_id,))
                
                columns = ['id', 'title', 'description', 'assigned_agent', 'status', 'priority',
                          'created_at', 'updated_at', 'due_date', 'estimated_hours', 'actual_hours', 'tags']
                
                tasks = []
                for row in cursor.fetchall():
                    task = dict(zip(columns, row))
                    task['tags'] = json.loads(task['tags']) if task['tags'] else []
                    tasks.append(task)
                
                # Organize tasks by status
                board = {
                    'backlog': [t for t in tasks if t['status'] == 'backlog'],
                    'assigned': [t for t in tasks if t['status'] == 'assigned'],
                    'in_progress': [t for t in tasks if t['status'] == 'in_progress'],
                    'review': [t for t in tasks if t['status'] == 'review'],
                    'done': [t for t in tasks if t['status'] == 'done']
                }
                
                conn.close()
                return JSONResponse({"board": board, "total_tasks": len(tasks)})
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/agents/tasks/{task_id}/move", methods=["PUT"])
        async def move_task(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                task_id = request.path_params.get('task_id')
                data = await request.json()
                new_status = data.get('status')
                agent_id = data.get('agent_id')
                
                valid_statuses = ['backlog', 'assigned', 'in_progress', 'review', 'done']
                if new_status not in valid_statuses:
                    return JSONResponse({"error": f"Invalid status. Must be one of: {valid_statuses}"}, status_code=400)
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                # Check if task exists
                cursor.execute('SELECT id, assigned_agent FROM agent_tasks WHERE id = ?', (task_id,))
                task = cursor.fetchone()
                if not task:
                    conn.close()
                    return JSONResponse({"error": "Task not found"}, status_code=404)
                
                # Update task
                timestamp = datetime.utcnow().isoformat()
                cursor.execute('''
                    UPDATE agent_tasks 
                    SET status = ?, updated_at = ?, assigned_agent = COALESCE(?, assigned_agent)
                    WHERE id = ?
                ''', (new_status, timestamp, agent_id, task_id))
                
                conn.commit()
                conn.close()
                
                return JSONResponse({
                    "message": "Task moved successfully",
                    "task_id": task_id,
                    "new_status": new_status,
                    "updated_at": timestamp
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/agents/register", methods=["POST"])
        async def register_agent(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                name = data.get('name')
                role = data.get('role')
                capabilities = json.dumps(data.get('capabilities', []))
                machine_id = data.get('machine_id')
                max_workload = int(data.get('max_workload', 5))
                metadata = json.dumps(data.get('metadata', {}))
                
                if not all([name, role]):
                    return JSONResponse({"error": "Name and role are required"}, status_code=400)
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                agent_id = str(uuid.uuid4())
                timestamp = datetime.utcnow().isoformat()
                
                cursor.execute('''
                    INSERT INTO agent_registry 
                    (id, name, role, capabilities, machine_id, status, current_workload,
                     max_workload, last_seen, registered_at, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (agent_id, name, role, capabilities, machine_id, 'active', 0,
                     max_workload, timestamp, timestamp, metadata))
                
                conn.commit()
                conn.close()
                
                return JSONResponse({
                    "agent_id": agent_id,
                    "message": "Agent registered successfully",
                    "registered_at": timestamp
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/agents/roster", methods=["GET"])
        async def get_agent_roster(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT id, name, role, capabilities, machine_id, status, current_workload,
                           max_workload, last_seen, registered_at, metadata
                    FROM agent_registry
                    ORDER BY last_seen DESC
                ''')
                
                columns = ['id', 'name', 'role', 'capabilities', 'machine_id', 'status', 
                          'current_workload', 'max_workload', 'last_seen', 'registered_at', 'metadata']
                
                agents = []
                for row in cursor.fetchall():
                    agent = dict(zip(columns, row))
                    agent['capabilities'] = json.loads(agent['capabilities']) if agent['capabilities'] else []
                    agent['metadata'] = json.loads(agent['metadata']) if agent['metadata'] else {}
                    agents.append(agent)
                
                conn.close()
                return JSONResponse({"agents": agents, "total": len(agents)})
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/api/agents/{agent_id}/workload", methods=["GET"])
        async def get_agent_workload(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                agent_id = request.path_params.get('agent_id')
                
                conn = sqlite3.connect(self.vault_db)
                cursor = conn.cursor()
                
                # Get agent info
                cursor.execute('''
                    SELECT name, current_workload, max_workload, status
                    FROM agent_registry WHERE id = ?
                ''', (agent_id,))
                
                agent_info = cursor.fetchone()
                if not agent_info:
                    conn.close()
                    return JSONResponse({"error": "Agent not found"}, status_code=404)
                
                # Get current tasks
                cursor.execute('''
                    SELECT id, title, status, priority, created_at
                    FROM agent_tasks 
                    WHERE assigned_agent = ? AND status IN ('assigned', 'in_progress', 'review')
                    ORDER BY priority DESC
                ''', (agent_id,))
                
                current_tasks = []
                for row in cursor.fetchall():
                    current_tasks.append({
                        'id': row[0],
                        'title': row[1], 
                        'status': row[2],
                        'priority': row[3],
                        'created_at': row[4]
                    })
                
                conn.close()
                return JSONResponse({
                    "agent_id": agent_id,
                    "name": agent_info[0],
                    "current_workload": agent_info[1],
                    "max_workload": agent_info[2],
                    "status": agent_info[3],
                    "utilization": (agent_info[1] / agent_info[2]) * 100 if agent_info[2] > 0 else 0,
                    "current_tasks": current_tasks,
                    "available_capacity": agent_info[2] - agent_info[1]
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # ===== CONFIG BACKUP SYSTEM API ENDPOINTS =====
        
        @self.mcp.custom_route("/api/config/backup/systems", methods=["GET"])
        async def list_config_systems(request):
            """List all registered configuration systems"""
            try:
                from config_backup_system import ConfigBackupSystem
                config_backup = ConfigBackupSystem()
                systems = config_backup.list_systems()
                return JSONResponse({"systems": systems})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/config/backup/snapshot", methods=["POST"])
        async def create_config_snapshot(request):
            """Create a configuration snapshot"""
            try:
                from config_backup_system import ConfigBackupSystem
                body = await request.json()
                config_backup = ConfigBackupSystem()
                snapshot_id = config_backup.create_snapshot(
                    system_id=body["system_id"],
                    config_content=body["config_content"],
                    config_type=body.get("config_type", "unknown"),
                    tags=body.get("tags", [])
                )
                return JSONResponse({"snapshot_id": snapshot_id, "status": "created"})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/config/backup/snapshots/{system_id}", methods=["GET"])
        async def get_config_snapshots(request):
            """Get configuration snapshots for a system"""
            try:
                system_id = request.path_params["system_id"]
                query_params = dict(request.query_params)
                limit = int(query_params.get("limit", 20))
                offset = int(query_params.get("offset", 0))
                
                from config_backup_system import ConfigBackupSystem
                config_backup = ConfigBackupSystem()
                snapshots = config_backup.get_snapshots(system_id, limit, offset)
                return JSONResponse({"snapshots": snapshots})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/config/backup/snapshot/{snapshot_id}", methods=["GET"])
        async def get_config_snapshot_content(request):
            """Get specific snapshot content"""
            try:
                snapshot_id = request.path_params["snapshot_id"]
                from config_backup_system import ConfigBackupSystem
                config_backup = ConfigBackupSystem()
                snapshot = config_backup.get_snapshot_by_id(snapshot_id)
                if not snapshot:
                    return JSONResponse({"error": "Snapshot not found"}, status_code=404)
                return JSONResponse({"snapshot": snapshot})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/config/backup/restore", methods=["POST"])
        async def restore_config_snapshot(request):
            """Restore configuration from snapshot"""
            try:
                body = await request.json()
                from config_backup_system import ConfigBackupSystem
                config_backup = ConfigBackupSystem()
                result = config_backup.restore_snapshot(
                    snapshot_id=body["snapshot_id"],
                    target_path=body.get("target_path")
                )
                return JSONResponse({"status": "restored", "result": result})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/config/backup/diff/{snapshot1_id}/{snapshot2_id}", methods=["GET"])
        async def get_config_diff(request):
            """Get difference between two snapshots"""
            try:
                snapshot1_id = request.path_params["snapshot1_id"]
                snapshot2_id = request.path_params["snapshot2_id"]
                from config_backup_system import ConfigBackupSystem
                config_backup = ConfigBackupSystem()
                diff = config_backup.compare_snapshots(snapshot1_id, snapshot2_id)
                return JSONResponse({"diff": diff})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/config/backup/history/{system_id}", methods=["GET"])
        async def get_config_history(request):
            """Get configuration change history for a system"""
            try:
                system_id = request.path_params["system_id"]
                query_params = dict(request.query_params)
                limit = int(query_params.get("limit", 50))
                
                from config_backup_system import ConfigBackupSystem
                config_backup = ConfigBackupSystem()
                history = config_backup.get_change_history(system_id, limit)
                return JSONResponse({"history": history})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/config/backup/drift/{system_id}", methods=["GET"])
        async def detect_config_drift(request):
            """Detect configuration drift for a system"""
            try:
                system_id = request.path_params["system_id"]
                query_params = dict(request.query_params)
                hours = int(query_params.get("hours", 24))
                
                from config_backup_system import ConfigBackupSystem
                config_backup = ConfigBackupSystem()
                drift = config_backup.detect_drift(system_id, hours)
                return JSONResponse({"drift": drift})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/config/backup/alert", methods=["POST"])
        async def create_drift_alert(request):
            """Create configuration drift alert"""
            try:
                body = await request.json()
                from config_backup_system import ConfigBackupSystem
                config_backup = ConfigBackupSystem()
                alert_id = config_backup.create_drift_alert(
                    system_id=body["system_id"],
                    drift_type=body["drift_type"],
                    severity=body.get("severity", "medium"),
                    description=body["description"],
                    affected_keys=body.get("affected_keys", [])
                )
                return JSONResponse({"alert_id": alert_id, "status": "created"})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/config/backup/alerts/{system_id}", methods=["GET"])
        async def get_drift_alerts(request):
            """Get drift alerts for a system"""
            try:
                system_id = request.path_params["system_id"]
                query_params = dict(request.query_params)
                limit = int(query_params.get("limit", 20))
                
                from config_backup_system import ConfigBackupSystem
                config_backup = ConfigBackupSystem()
                alerts = config_backup.get_drift_alerts(system_id, limit)
                return JSONResponse({"alerts": alerts})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # ===== AGENT KANBAN TASK MANAGEMENT API ENDPOINTS =====
        
        @self.mcp.custom_route("/api/kanban/agents/register", methods=["POST"])
        async def register_kanban_agent(request):
            """Register new agent with capabilities"""
            try:
                from agent_kanban_system import AgentKanbanSystem
                body = await request.json()
                kanban = AgentKanbanSystem()
                
                success = kanban.register_agent(
                    agent_id=body["agent_id"],
                    name=body["name"],
                    machine_id=body["machine_id"],
                    capabilities=body["capabilities"],
                    max_workload=body.get("max_workload", 5),
                    metadata=body.get("metadata", {})
                )
                
                return JSONResponse({"success": success, "message": "Agent registered" if success else "Registration failed"})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/kanban/agents", methods=["GET"])
        async def get_available_agents(request):
            """Get available agents with optional capability filtering"""
            try:
                from agent_kanban_system import AgentKanbanSystem
                query_params = dict(request.query_params)
                required_capabilities = query_params.get("capabilities", "").split(",") if query_params.get("capabilities") else None
                min_level = int(query_params.get("min_level", 1))
                
                kanban = AgentKanbanSystem()
                agents = kanban.get_available_agents(required_capabilities, min_level)
                
                return JSONResponse({"agents": agents})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/kanban/agents/{agent_id}/status", methods=["PUT"])
        async def update_agent_status(request):
            """Update agent status and workload"""
            try:
                from agent_kanban_system import AgentKanbanSystem, AgentStatus
                agent_id = request.path_params["agent_id"]
                body = await request.json()
                
                kanban = AgentKanbanSystem()
                success = kanban.update_agent_status(
                    agent_id=agent_id,
                    status=AgentStatus(body["status"]),
                    current_workload=body.get("current_workload")
                )
                
                return JSONResponse({"success": success})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/kanban/tasks", methods=["POST"])
        async def create_kanban_task(request):
            """Create a new kanban task"""
            try:
                from agent_kanban_system import AgentKanbanSystem, TaskPriority
                body = await request.json()
                kanban = AgentKanbanSystem()
                
                task_id = kanban.create_task(
                    title=body["title"],
                    description=body["description"], 
                    created_by=body["created_by"],
                    priority=TaskPriority(body.get("priority", "medium")),
                    board_id=body.get("board_id", "default"),
                    dependencies=body.get("dependencies", []),
                    estimated_hours=body.get("estimated_hours"),
                    due_date=body.get("due_date"),
                    tags=body.get("tags", []),
                    metadata=body.get("metadata", {})
                )
                
                return JSONResponse({"task_id": task_id, "success": True})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/kanban/tasks/{task_id}/assign", methods=["PUT"])
        async def assign_kanban_task(request):
            """Assign task to agent"""
            try:
                from agent_kanban_system import AgentKanbanSystem
                task_id = request.path_params["task_id"]
                body = await request.json()
                
                kanban = AgentKanbanSystem()
                success = kanban.assign_task(
                    task_id=task_id,
                    agent_id=body.get("agent_id"),
                    auto_assign=body.get("auto_assign", True)
                )
                
                return JSONResponse({"success": success})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/kanban/tasks/{task_id}/move", methods=["PUT"])
        async def move_kanban_task(request):
            """Move task between kanban columns"""
            try:
                from agent_kanban_system import AgentKanbanSystem, TaskStatus
                task_id = request.path_params["task_id"]
                body = await request.json()
                
                kanban = AgentKanbanSystem()
                success = kanban.move_task(
                    task_id=task_id,
                    new_status=TaskStatus(body["status"]),
                    moved_by=body["moved_by"]
                )
                
                return JSONResponse({"success": success})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/kanban/boards/{board_id}", methods=["GET"])
        async def get_kanban_board(request):
            """Get complete kanban board state"""
            try:
                from agent_kanban_system import AgentKanbanSystem
                board_id = request.path_params["board_id"]
                query_params = dict(request.query_params)
                include_metrics = query_params.get("metrics", "true").lower() == "true"
                
                kanban = AgentKanbanSystem()
                board_data = kanban.get_kanban_board(board_id, include_metrics)
                
                return JSONResponse({"board": board_data})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/kanban/agents/workload", methods=["GET"])
        async def get_agent_workload_report(request):
            """Get agent workload and performance report"""
            try:
                from agent_kanban_system import AgentKanbanSystem
                kanban = AgentKanbanSystem()
                report = kanban.get_agent_workload_report()
                
                return JSONResponse({"report": report})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/api/kanban/analytics", methods=["GET"])
        async def get_task_analytics(request):
            """Get task analytics for specified time period"""
            try:
                from agent_kanban_system import AgentKanbanSystem
                query_params = dict(request.query_params)
                days = int(query_params.get("days", 30))
                
                kanban = AgentKanbanSystem()
                analytics = kanban.get_task_analytics(days)
                
                return JSONResponse({"analytics": analytics})
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        logger.info("ðŸ’¾ Config Backup API endpoints registered")
        logger.info("ðŸ“‹ Agent Kanban Task Management API endpoints registered")
        
        # Config Backup API - Create snapshot
        @self.mcp.custom_route("/admin/api/configs/backup", methods=["POST"])
        async def api_create_config_backup(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
            try:
                data = await request.json()
                system_id = data.get('system_id')
                config_content = data.get('config_content')
                config_type = data.get('config_type', 'config')
                file_path = data.get('file_path')
                agent_id = data.get('agent_id', '')
                metadata = data.get('metadata', {})
                
                if not system_id or not config_content:
                    return JSONResponse({"error": "system_id and config_content required"}, status_code=400)
                
                from config_backup_system import ConfigSnapshot
                snapshot = ConfigSnapshot(
                    system_id=system_id,
                    config_type=config_type,
                    config_content=config_content,
                    file_path=file_path,
                    agent_id=agent_id,
                    metadata=metadata
                )
                
                snapshot_id = self.config_backup.create_snapshot(snapshot)
                
                if snapshot_id:
                    return JSONResponse({
                        "success": True,
                        "snapshot_id": snapshot_id,
                        "system_id": system_id,
                        "config_hash": snapshot.config_hash,
                        "size": len(config_content),
                        "message": "Configuration snapshot created successfully"
                    })
                else:
                    return JSONResponse({"error": "Failed to create snapshot"}, status_code=500)
                    
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Config History API 
        @self.mcp.custom_route("/admin/api/configs/{system_id}/history", methods=["GET"])
        async def api_get_config_history(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
            try:
                system_id = request.path_params.get('system_id')
                limit = int(request.query_params.get('limit', 20))
                
                history = self.config_backup.get_system_history(system_id, limit)
                
                return JSONResponse({
                    "success": True,
                    "system_id": system_id,
                    "history": history,
                    "count": len(history)
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Current Config API
        @self.mcp.custom_route("/admin/api/configs/{system_id}/current", methods=["GET"])
        async def api_get_current_config(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
            try:
                system_id = request.path_params.get('system_id')
                include_content = request.query_params.get('include_content', 'false').lower() == 'true'
                
                current_config = self.config_backup.get_current_config(system_id)
                
                if not current_config:
                    return JSONResponse({"error": "No configuration found"}, status_code=404)
                
                # Optionally exclude large content from response
                if not include_content:
                    current_config = {k: v for k, v in current_config.items() if k != 'config_content'}
                    current_config['content_size'] = current_config.get('size', 0)
                
                return JSONResponse({
                    "success": True,
                    "system_id": system_id,
                    "config": current_config
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Config Restore API
        @self.mcp.custom_route("/admin/api/configs/restore", methods=["POST"])
        async def api_restore_config(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
            try:
                data = await request.json()
                snapshot_id = data.get('snapshot_id')
                system_id = data.get('system_id')
                restore_to_file = data.get('restore_to_file', False)
                
                if not snapshot_id and not system_id:
                    return JSONResponse({"error": "snapshot_id or system_id required"}, status_code=400)
                
                # Get snapshot to restore
                if snapshot_id:
                    # Restore specific snapshot
                    with sqlite3.connect(self.config_backup.db_path) as conn:
                        conn.row_factory = sqlite3.Row
                        snapshot = conn.execute("""
                            SELECT * FROM config_snapshots WHERE id = ?
                        """, (snapshot_id,)).fetchone()
                else:
                    # Restore latest for system
                    snapshot = self.config_backup.get_current_config(system_id)
                
                if not snapshot:
                    return JSONResponse({"error": "Snapshot not found"}, status_code=404)
                
                snapshot_dict = dict(snapshot)
                config_content = snapshot_dict.get('config_content', '')
                file_path = snapshot_dict.get('file_path')
                
                # If restore_to_file and file_path exists, write to file
                restored_to_file = False
                if restore_to_file and file_path:
                    try:
                        import os
                        os.makedirs(os.path.dirname(file_path), exist_ok=True)
                        with open(file_path, 'w') as f:
                            f.write(config_content)
                        restored_to_file = True
                    except Exception as file_error:
                        logger.warning(f"Failed to restore to file {file_path}: {file_error}")
                
                return JSONResponse({
                    "success": True,
                    "snapshot_id": snapshot_dict.get('id'),
                    "system_id": snapshot_dict.get('system_id'),
                    "config_content": config_content,
                    "file_path": file_path,
                    "restored_to_file": restored_to_file,
                    "timestamp": snapshot_dict.get('timestamp'),
                    "message": "Configuration restored successfully"
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Drift Detection API
        @self.mcp.custom_route("/admin/api/configs/drift", methods=["GET"])
        async def api_detect_drift(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
            try:
                system_id = request.query_params.get('system_id')
                hours_back = int(request.query_params.get('hours_back', 24))
                
                drift_issues = self.config_backup.detect_drift(system_id, hours_back)
                
                # Analyze and categorize issues
                categorized = {
                    'critical': [],
                    'high': [],
                    'medium': [],
                    'low': []
                }
                
                for issue in drift_issues:
                    analysis = self.config_backup._analyze_drift(issue)
                    issue['analysis'] = analysis
                    severity = analysis.get('severity', 'low')
                    categorized[severity].append(issue)
                
                return JSONResponse({
                    "success": True,
                    "system_id": system_id,
                    "hours_back": hours_back,
                    "total_issues": len(drift_issues),
                    "issues": drift_issues,
                    "categorized": categorized,
                    "summary": {
                        "critical": len(categorized['critical']),
                        "high": len(categorized['high']),
                        "medium": len(categorized['medium']),
                        "low": len(categorized['low'])
                    }
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Config Diff API
        @self.mcp.custom_route("/admin/api/configs/diff/{id1}/{id2}", methods=["GET"])
        async def api_compare_configs(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
            try:
                id1 = request.path_params.get('id1')
                id2 = request.path_params.get('id2')
                
                with sqlite3.connect(self.config_backup.db_path) as conn:
                    conn.row_factory = sqlite3.Row
                    
                    # Get both snapshots
                    snapshot1 = conn.execute("SELECT * FROM config_snapshots WHERE id = ?", (id1,)).fetchone()
                    snapshot2 = conn.execute("SELECT * FROM config_snapshots WHERE id = ?", (id2,)).fetchone()
                    
                    if not snapshot1 or not snapshot2:
                        return JSONResponse({"error": "One or both snapshots not found"}, status_code=404)
                    
                    # Check if diff already exists
                    existing_diff = conn.execute("""
                        SELECT * FROM config_diffs 
                        WHERE (snapshot_id_before = ? AND snapshot_id_after = ?) 
                           OR (snapshot_id_before = ? AND snapshot_id_after = ?)
                    """, (id1, id2, id2, id1)).fetchone()
                    
                    if existing_diff:
                        diff_data = dict(existing_diff)
                    else:
                        # Generate diff
                        import difflib
                        content1 = dict(snapshot1)['config_content']
                        content2 = dict(snapshot2)['config_content']
                        
                        diff_lines = list(difflib.unified_diff(
                            content1.splitlines(keepends=True),
                            content2.splitlines(keepends=True),
                            fromfile=f"snapshot_{id1}",
                            tofile=f"snapshot_{id2}",
                            n=3
                        ))
                        
                        diff_content = ''.join(diff_lines)
                        lines_added = sum(1 for line in diff_lines if line.startswith('+') and not line.startswith('+++'))
                        lines_removed = sum(1 for line in diff_lines if line.startswith('-') and not line.startswith('---'))
                        
                        diff_data = {
                            'diff_content': diff_content,
                            'lines_added': lines_added,
                            'lines_removed': lines_removed,
                            'snapshot_id_before': id1,
                            'snapshot_id_after': id2
                        }
                
                return JSONResponse({
                    "success": True,
                    "snapshot1": dict(snapshot1),
                    "snapshot2": dict(snapshot2),
                    "diff": diff_data
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Systems List API
        @self.mcp.custom_route("/admin/api/configs/systems", methods=["GET"])
        async def api_list_config_systems(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
            try:
                systems = self.config_backup.get_systems()
                
                return JSONResponse({
                    "success": True,
                    "systems": systems,
                    "count": len(systems)
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Register System API
        @self.mcp.custom_route("/admin/api/configs/systems", methods=["POST"])
        async def api_register_config_system(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
                
            try:
                data = await request.json()
                system_id = data.get('system_id')
                system_name = data.get('system_name')
                system_type = data.get('system_type')
                
                if not all([system_id, system_name, system_type]):
                    return JSONResponse({"error": "system_id, system_name, and system_type required"}, status_code=400)
                
                success = self.config_backup.register_system(
                    system_id=system_id,
                    system_name=system_name,
                    system_type=system_type,
                    agent_id=data.get('agent_id', ''),
                    description=data.get('description', ''),
                    backup_frequency=data.get('backup_frequency', 3600),
                    metadata=data.get('metadata', {})
                )
                
                if success:
                    return JSONResponse({
                        "success": True,
                        "system_id": system_id,
                        "message": "System registered successfully"
                    })
                else:
                    return JSONResponse({"error": "Failed to register system"}, status_code=500)
                    
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
    
    def _add_session_recovery_middleware(self):
        """Add session recovery logic using custom error handling"""
        from starlette.responses import JSONResponse
        import uuid
        import logging
        
        logger = logging.getLogger(__name__)
        
        # Override the original HTTP server's error handler
        original_http_server_init = None
        
        # Store original error handling for later use
        if hasattr(self.mcp, '_http_server'):
            original_http_server_init = getattr(self.mcp._http_server, '__init__', None)
        
        # Add recovery endpoint that clients can call when they get 404s
        @self.mcp.custom_route("/api/session/recover", methods=["POST"])
        async def session_recovery_endpoint(request):
            """Handle session recovery requests"""
            try:
                data = await request.json() if request.method == "POST" else {}
                old_session_id = data.get('old_session_id', 'unknown')
                
                # Generate new session ID
                new_session_id = uuid.uuid4().hex
                
                logger.info(f"ðŸ”„ Session recovery requested - Old: {old_session_id}, New: {new_session_id}")
                
                return JSONResponse({
                    "status": "recovered", 
                    "old_session_id": old_session_id,
                    "new_session_id": new_session_id,
                    "message": "New session ID generated, please reconnect",
                    "action": "reconnect"
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Add a public recovery info endpoint (no auth required)
        @self.mcp.custom_route("/api/session/info", methods=["GET"])
        async def session_info_endpoint(request):
            """Get session info and recovery instructions"""
            return JSONResponse({
                "server_status": "online",
                "session_management": "enabled",
                "recovery_available": True,
                "recovery_endpoint": "/api/session/recover",
                "message": "Server now uses session mode - sessions enabled for MCP tool functionality",
                "server_time": datetime.now().isoformat(),
                "uptime": str(datetime.now() - self._start_datetime),
                "note": "If you still get session errors, it's likely a client-side caching issue"
            })
        
        logger.info("ðŸ”„ Session recovery system registered - endpoints: /api/session/recover, /api/session/info")

    def _setup_session_error_handling(self):
        """Setup improved session error handling at the FastMCP level"""
        import logging
        
        logger = logging.getLogger(__name__)
        
        # Create a custom handler for /messages/ that handles missing sessions gracefully
        # This route will catch all /messages/ requests including those with query parameters
        @self.mcp.custom_route("/messages/", methods=["POST", "GET"], include_in_schema=False)  
        async def handle_session_messages_root(request):
            """Handle /messages/ endpoint - this should intercept before FastMCP"""
            # This route should run BEFORE FastMCP's built-in /messages/ handler
            pass
            
        # Also create a catch-all for any /messages path variations
        @self.mcp.custom_route("/messages", methods=["POST", "GET"], include_in_schema=False)
        async def handle_session_messages_no_slash(request):
            """Custom handler for /messages/ root endpoint that provides session recovery"""
            from starlette.responses import JSONResponse
            import uuid
            
            try:
                # Get session ID from query parameters
                session_id = request.query_params.get('session_id', '')
                
                # Log the session request
                logger.warning(f"ðŸ” Session request intercepted - ID: {session_id}, Path: {request.url.path}")
                
                # Check if this is the problematic session ID
                if session_id == "2dbfd1c3556642c2ac12c2a0450c0e50":
                    logger.warning(f"ðŸš¨ Detected problematic cached session ID: {session_id} - providing recovery")
                    
                    # Generate new session and redirect to SSE
                    new_session_id = uuid.uuid4().hex
                    return JSONResponse({
                        "error": "session_expired",
                        "message": f"Session {session_id} has expired. Please reconnect with new session ID.",
                        "old_session_id": session_id,
                        "new_session_id": new_session_id,
                        "action": "reconnect",
                        "sse_url": f"/sse?session_id={new_session_id}",
                        "recovery_url": f"/api/session/recover"
                    }, status_code=410)  # Gone status
                
                # If no session ID provided, suggest creating one
                if not session_id:
                    new_session_id = uuid.uuid4().hex
                    return JSONResponse({
                        "error": "no_session_id",
                        "message": "No session ID provided",
                        "suggested_session_id": new_session_id,
                        "instructions": "Connect to /sse with session_id parameter"
                    }, status_code=400)
                
                # For other sessions, return guidance to use SSE endpoint
                return JSONResponse({
                    "error": "use_sse_endpoint", 
                    "message": "For MCP communication, please connect via the SSE endpoint",
                    "session_id": session_id,
                    "sse_url": f"/sse?session_id={session_id}",
                    "instructions": "Use Server-Sent Events endpoint at /sse for MCP protocol"
                }, status_code=200)
                
            except Exception as e:
                logger.error(f"âŒ Error in session message handler: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        logger.info("ðŸ“¡ Enhanced session error handling configured for /messages/ endpoints")

    def _add_session_management(self):
        """Add session management and health endpoints"""
        from starlette.responses import JSONResponse
        from datetime import datetime, timedelta
        import uuid
        
        # Session health check endpoint
        @self.mcp.custom_route("/admin/api/sessions/health", methods=["GET"])
        async def session_health(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Get session manager if available
                session_manager = getattr(self.mcp, '_session_manager', None)
                active_sessions = 0
                session_details = []
                
                if session_manager:
                    active_sessions = len(getattr(session_manager, '_server_instances', {}))
                    for session_id, transport in getattr(session_manager, '_server_instances', {}).items():
                        session_details.append({
                            "session_id": session_id,
                            "is_terminated": getattr(transport, 'is_terminated', False),
                            "created_at": self.session_last_activity.get(session_id, "Unknown")
                        })
                
                return JSONResponse({
                    "status": "healthy",
                    "active_sessions": active_sessions,
                    "session_details": session_details,
                    "server_uptime": str(datetime.now() - self._start_datetime),
                    "mcp_config": {
                        "sse_path": "/sse", 
                        "message_path": "/messages/",
                        "stateless": False,
                        "session_recovery_enabled": True,
                        "note": "Sessions enabled for MCP tool functionality"
                    }
                })
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Session cleanup endpoint
        @self.mcp.custom_route("/admin/api/sessions/cleanup", methods=["POST"])
        async def cleanup_sessions(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                session_manager = getattr(self.mcp, '_session_manager', None)
                cleaned = 0
                
                if session_manager:
                    instances = getattr(session_manager, '_server_instances', {})
                    to_remove = []
                    
                    for session_id, transport in instances.items():
                        if getattr(transport, 'is_terminated', False):
                            to_remove.append(session_id)
                    
                    for session_id in to_remove:
                        del instances[session_id]
                        if session_id in self.session_last_activity:
                            del self.session_last_activity[session_id]
                        cleaned += 1
                
                return JSONResponse({
                    "message": f"Cleaned up {cleaned} terminated sessions",
                    "remaining_sessions": len(getattr(session_manager, '_server_instances', {})) if session_manager else 0
                })
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Session recovery endpoint for manual testing
        @self.mcp.custom_route("/admin/api/sessions/recover", methods=["POST"])
        async def recover_session(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                old_session_id = data.get('old_session_id')
                
                session_manager = getattr(self.mcp, '_session_manager', None)
                
                if session_manager:
                    instances = getattr(session_manager, '_server_instances', {})
                    
                    # Remove old session if it exists
                    if old_session_id and old_session_id in instances:
                        del instances[old_session_id]
                        if old_session_id in self.session_last_activity:
                            del self.session_last_activity[old_session_id]
                    
                    # Generate new session ID for client to use
                    new_session_id = uuid.uuid4().hex
                    
                    return JSONResponse({
                        "message": "Session recovery initiated",
                        "old_session_id": old_session_id,
                        "new_session_id": new_session_id,
                        "instructions": "Use new session ID for next connection"
                    })
                
                return JSONResponse({"error": "Session manager not available"}, status_code=503)
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Force session refresh endpoint
        @self.mcp.custom_route("/admin/api/sessions/refresh", methods=["POST"])
        async def refresh_sessions(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                session_id = data.get('session_id')
                
                if session_id:
                    # Update activity time for specific session
                    self.session_last_activity[session_id] = datetime.now().isoformat()
                    return JSONResponse({
                        "message": f"Session {session_id} activity updated",
                        "updated_at": self.session_last_activity[session_id]
                    })
                else:
                    # Refresh all sessions
                    current_time = datetime.now().isoformat()
                    session_manager = getattr(self.mcp, '_session_manager', None)
                    
                    if session_manager:
                        for session_id in getattr(session_manager, '_server_instances', {}):
                            self.session_last_activity[session_id] = current_time
                    
                    return JSONResponse({
                        "message": "All sessions refreshed",
                        "refreshed_at": current_time,
                        "total_sessions": len(self.session_last_activity)
                    })
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Session debugging endpoint
        @self.mcp.custom_route("/admin/api/sessions/debug", methods=["GET"])
        async def session_debug(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                session_manager = getattr(self.mcp, '_session_manager', None)
                debug_info = {
                    "session_manager_available": session_manager is not None,
                    "mcp_server_available": hasattr(self, 'mcp'),
                    "stored_activities": len(self.session_last_activity),
                    "stored_sessions": len(self.active_sessions),
                    "config": {
                        "host": self.host,
                        "port": self.port,
                        "sse_path": "/sse",
                        "message_path": "/messages/"
                    }
                }
                
                if session_manager:
                    debug_info.update({
                        "active_server_instances": len(getattr(session_manager, '_server_instances', {})),
                        "session_ids": list(getattr(session_manager, '_server_instances', {}).keys()),
                        "task_group_active": getattr(session_manager, '_task_group', None) is not None
                    })
                
                return JSONResponse(debug_info)
            except Exception as e:
                return JSONResponse({"error": str(e), "debug_failed": True}, status_code=500)
        
        logger.info("ðŸ”§ Session management endpoints registered")
    
    def _init_dashboard_functionality(self):
        """Initialize enhanced dashboard functionality from dashboard_server"""
        try:
            # Import dashboard components
            from database import ControlDatabase, UserRole, DeviceStatus, KeyStatus
            from config_generator import ConfigGenerator, ConfigFormat
            
            # Initialize dashboard database
            self.dashboard_db = ControlDatabase("database/haivemind.db")
            
            # Initialize configuration generator
            self._config_generator = None
            try:
                self._config_generator = ConfigGenerator(self.config, self.storage)
            except Exception as e:
                logger.warning(f"Configuration generator not available: {e}")
            
            # Add dashboard-specific routes
            self._add_dashboard_routes()
            
            logger.info("ðŸ“Š Enhanced dashboard functionality initialized")
            
        except Exception as e:
            logger.warning(f"Dashboard functionality not available: {e}")
    
    def _add_dashboard_routes(self):
        """Add enhanced dashboard routes"""
        from starlette.responses import JSONResponse
        import jwt
        from datetime import datetime, timedelta
        
        # Dashboard stats endpoint
        @self.mcp.custom_route("/admin/api/dashboard-stats", methods=["GET"])
        async def dashboard_stats(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Get enhanced stats from dashboard database
                stats = self.dashboard_db.get_dashboard_stats()
                
                # Add MCP server stats
                agent_roster = await self.storage.get_agent_roster()
                memory_stats = self.storage.get_collection_info()
                
                enhanced_stats = {
                    **stats,
                    "mcp_agents": len(agent_roster.get('agents', [])),
                    "memory_collections": len(memory_stats),
                    "total_memories": sum(collection.get('count', 0) for collection in memory_stats.values()),
                    "server_status": "running",
                    "port": self.port
                }
                
                return JSONResponse(enhanced_stats)
            except Exception as e:
                logger.error(f"Error getting dashboard stats: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # General stats endpoint (requested by dashboard)
        @self.mcp.custom_route("/admin/api/stats", methods=["GET"])
        async def general_stats(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Get basic system stats
                agent_roster = await self.storage.get_agent_roster()
                memory_stats = self.storage.get_collection_info()
                
                stats = {
                    "system_health": "healthy",
                    "active_agents": len(agent_roster.get('agents', [])),
                    "total_memories": sum(collection.get('count', 0) for collection in memory_stats.values()),
                    "memory_collections": len(memory_stats),
                    "server_port": self.port,
                    "server_status": "running",
                    "uptime": int(time.time() - getattr(self, '_start_time', time.time())),
                }
                
                return JSONResponse(stats)
            except Exception as e:
                logger.error(f"Error getting general stats: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Advanced memory search with multiple filters
        @self.mcp.custom_route("/admin/api/memory/advanced-search", methods=["GET"])
        async def advanced_memory_search(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Get advanced search parameters
                query = request.query_params.get('query', '')
                category = request.query_params.get('category')
                machine_id = request.query_params.get('machine_id')
                date_from = request.query_params.get('date_from')
                date_to = request.query_params.get('date_to')
                tags = request.query_params.get('tags', '').split(',') if request.query_params.get('tags') else []
                limit = int(request.query_params.get('limit', 50))
                semantic = request.query_params.get('semantic', 'true').lower() == 'true'
                
                # Perform advanced search
                memories = await self.storage.search_memories(
                    query=query,
                    category=category,
                    limit=limit,
                    semantic=semantic
                )
                
                # Apply additional filters
                filtered_memories = []
                for memory in memories:
                    # Filter by machine_id if specified
                    if machine_id and memory.get('machine_id') != machine_id:
                        continue
                    
                    # Filter by date range if specified
                    if date_from or date_to:
                        created_at = memory.get('created_at')
                        if created_at:
                            # Simple date filtering (could be enhanced)
                            if date_from and created_at < date_from:
                                continue
                            if date_to and created_at > date_to:
                                continue
                    
                    # Filter by tags if specified
                    if tags and tags != ['']:
                        memory_tags = memory.get('tags', [])
                        if not any(tag in memory_tags for tag in tags):
                            continue
                    
                    filtered_memories.append(memory)
                
                return JSONResponse({
                    "memories": filtered_memories,
                    "total": len(filtered_memories),
                    "filters_applied": {
                        "query": query,
                        "category": category,
                        "machine_id": machine_id,
                        "date_from": date_from,
                        "date_to": date_to,
                        "tags": tags,
                        "semantic": semantic
                    }
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Bulk memory operations
        @self.mcp.custom_route("/admin/api/memory/bulk-operations", methods=["POST"])
        async def bulk_memory_operations(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                operation = data.get('operation')
                memory_ids = data.get('memory_ids', [])
                parameters = data.get('parameters', {})
                
                if not operation or not memory_ids:
                    return JSONResponse({"error": "Operation and memory_ids required"}, status_code=400)
                
                results = []
                success_count = 0
                
                for memory_id in memory_ids:
                    try:
                        if operation == 'delete':
                            # Store deletion record for audit
                            deletion_record = {
                                "memory_id": memory_id,
                                "deleted_at": datetime.utcnow().isoformat(),
                                "deleted_by": "admin",  # TODO: Get from auth
                                "operation": "bulk_delete"
                            }
                            await self.storage.store_memory(
                                content=json.dumps(deletion_record),
                                category="global",
                                context="Bulk Memory Deletion",
                                tags=["bulk_delete", "audit"]
                            )
                            
                            results.append({"memory_id": memory_id, "status": "deleted"})
                            success_count += 1
                            
                        elif operation == 'categorize':
                            new_category = parameters.get('category', 'global')
                            # This would update the memory category - simplified implementation
                            results.append({"memory_id": memory_id, "status": "categorized", "new_category": new_category})
                            success_count += 1
                            
                        elif operation == 'tag':
                            new_tags = parameters.get('tags', [])
                            # This would add tags to the memory - simplified implementation
                            results.append({"memory_id": memory_id, "status": "tagged", "tags": new_tags})
                            success_count += 1
                            
                        elif operation == 'export':
                            # This would export memories - simplified implementation
                            results.append({"memory_id": memory_id, "status": "exported"})
                            success_count += 1
                            
                        else:
                            results.append({"memory_id": memory_id, "status": "error", "message": "Unknown operation"})
                            
                    except Exception as e:
                        results.append({"memory_id": memory_id, "status": "error", "message": str(e)})
                
                return JSONResponse({
                    "operation": operation,
                    "total_processed": len(memory_ids),
                    "successful": success_count,
                    "failed": len(memory_ids) - success_count,
                    "results": results
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Memory relationships mapping
        @self.mcp.custom_route("/admin/api/memory/relationships", methods=["GET"])
        async def memory_relationships(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                memory_id = request.query_params.get('memory_id')
                if not memory_id:
                    return JSONResponse({"error": "memory_id parameter required"}, status_code=400)
                
                # Get the target memory
                target_memory = await self.storage.retrieve_memory(memory_id)
                if not target_memory:
                    return JSONResponse({"error": "Memory not found"}, status_code=404)
                
                # Find related memories using semantic similarity and tag overlap
                related_memories = await self.storage.search_memories(
                    query=target_memory.get('content', '')[:100],  # Use first 100 chars as query
                    category=target_memory.get('category'),
                    limit=20,
                    semantic=True
                )
                
                relationships = []
                target_tags = set(target_memory.get('tags', []))
                
                for memory in related_memories.get('memories', []):
                    if memory['id'] == memory_id:
                        continue  # Skip self
                    
                    # Calculate relationship strength
                    memory_tags = set(memory.get('tags', []))
                    tag_overlap = len(target_tags.intersection(memory_tags))
                    category_match = memory.get('category') == target_memory.get('category')
                    
                    relationship_strength = memory.get('similarity_score', 0)
                    if tag_overlap > 0:
                        relationship_strength += 0.1 * tag_overlap
                    if category_match:
                        relationship_strength += 0.05
                    
                    relationships.append({
                        "memory_id": memory['id'],
                        "content_preview": memory['content'][:100] + "..." if len(memory['content']) > 100 else memory['content'],
                        "category": memory.get('category'),
                        "relationship_strength": min(relationship_strength, 1.0),
                        "relationship_type": "semantic" if memory.get('similarity_score', 0) > 0.7 else "contextual",
                        "shared_tags": list(target_tags.intersection(memory_tags)),
                        "created_at": memory.get('created_at')
                    })
                
                # Sort by relationship strength
                relationships.sort(key=lambda x: x['relationship_strength'], reverse=True)
                
                return JSONResponse({
                    "memory_id": memory_id,
                    "relationships": relationships[:10],  # Top 10 relationships
                    "total_found": len(relationships)
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Memory analytics
        @self.mcp.custom_route("/admin/api/memory/analytics", methods=["GET"])
        async def memory_analytics(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Get collection info
                collection_stats = self.storage.get_collection_info()
                
                # Calculate analytics
                total_memories = sum(collection.get('count', 0) for collection in collection_stats.values())
                
                category_distribution = {}
                for collection_name, stats in collection_stats.items():
                    category = collection_name.replace('_memories', '')
                    category_distribution[category] = {
                        "count": stats.get('count', 0),
                        "percentage": (stats.get('count', 0) / total_memories * 100) if total_memories > 0 else 0
                    }
                
                # Memory growth analytics (simplified)
                growth_trend = {
                    "daily_growth": 5,  # Simplified - would calculate actual growth
                    "weekly_growth": 35,
                    "monthly_growth": 150,
                    "trend": "increasing"
                }
                
                # Quality metrics (simplified)
                quality_metrics = {
                    "avg_content_length": 250,  # Simplified
                    "tagged_memories_percentage": 75,
                    "memories_with_context": 60,
                    "duplicate_detection_score": 92
                }
                
                return JSONResponse({
                    "total_memories": total_memories,
                    "category_distribution": category_distribution,
                    "growth_analytics": growth_trend,
                    "quality_metrics": quality_metrics,
                    "machine_contributions": {
                        "lance-dev": {"count": total_memories, "percentage": 100}  # Simplified
                    },
                    "recent_activity": {
                        "last_24h": 8,
                        "last_7d": 45,
                        "last_30d": 180
                    },
                    "generated_at": datetime.utcnow().isoformat()
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Duplicate detection
        @self.mcp.custom_route("/admin/api/memory/deduplicate", methods=["POST"])
        async def detect_duplicates(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                threshold = float(data.get('similarity_threshold', 0.9))
                category = data.get('category')
                
                # Get all memories for analysis
                all_memories = await self.storage.search_memories(
                    query="",
                    category=category,
                    limit=500,
                    semantic=True
                )
                
                duplicates = []
                memories_list = all_memories.get('memories', [])
                
                # Simple duplicate detection based on content similarity
                for i, memory1 in enumerate(memories_list):
                    for j, memory2 in enumerate(memories_list[i+1:], i+1):
                        # Calculate similarity (simplified - using string comparison)
                        content1 = memory1.get('content', '').lower().strip()
                        content2 = memory2.get('content', '').lower().strip()
                        
                        if len(content1) > 0 and len(content2) > 0:
                            # Simple similarity calculation
                            similarity = len(set(content1.split()).intersection(set(content2.split()))) / len(set(content1.split()).union(set(content2.split())))
                            
                            if similarity >= threshold:
                                duplicates.append({
                                    "primary_memory": {
                                        "id": memory1['id'],
                                        "content": memory1['content'][:100] + "...",
                                        "created_at": memory1.get('created_at')
                                    },
                                    "duplicate_memory": {
                                        "id": memory2['id'],
                                        "content": memory2['content'][:100] + "...",
                                        "created_at": memory2.get('created_at')
                                    },
                                    "similarity_score": similarity,
                                    "recommended_action": "merge" if similarity > 0.95 else "review"
                                })
                
                return JSONResponse({
                    "duplicates_found": len(duplicates),
                    "threshold_used": threshold,
                    "duplicates": duplicates[:20],  # Limit to first 20
                    "analysis_summary": {
                        "total_memories_analyzed": len(memories_list),
                        "high_confidence_duplicates": len([d for d in duplicates if d['similarity_score'] > 0.95]),
                        "potential_duplicates": len([d for d in duplicates if d['similarity_score'] <= 0.95])
                    }
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Memory recommendations
        @self.mcp.custom_route("/admin/api/memory/recommendations", methods=["GET"])
        async def memory_recommendations(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                recommendation_type = request.query_params.get('type', 'all')
                
                recommendations = []
                
                if recommendation_type in ['all', 'content']:
                    # Content-based recommendations
                    recommendations.append({
                        "type": "content_gap",
                        "title": "Infrastructure Documentation Gap",
                        "description": "Consider adding more memories about backup procedures and disaster recovery",
                        "priority": "medium",
                        "category": "infrastructure",
                        "suggested_actions": [
                            "Document backup restoration procedures",
                            "Add disaster recovery runbooks",
                            "Create infrastructure monitoring alerts"
                        ]
                    })
                
                if recommendation_type in ['all', 'organization']:
                    # Organization recommendations
                    recommendations.append({
                        "type": "organization",
                        "title": "Tag Consistency Improvement",
                        "description": "Some memories lack proper tagging, affecting searchability",
                        "priority": "low",
                        "suggested_actions": [
                            "Review untagged memories",
                            "Establish tagging conventions",
                            "Add tags to improve categorization"
                        ]
                    })
                
                if recommendation_type in ['all', 'quality']:
                    # Quality recommendations
                    recommendations.append({
                        "type": "quality",
                        "title": "Memory Quality Enhancement",
                        "description": "Several memories could benefit from more context and structured formatting",
                        "priority": "medium",
                        "suggested_actions": [
                            "Add context to existing memories",
                            "Use structured formatting for procedures",
                            "Include relevant links and references"
                        ]
                    })
                
                return JSONResponse({
                    "recommendations": recommendations,
                    "total_recommendations": len(recommendations),
                    "generated_at": datetime.utcnow().isoformat()
                })
                
            except Exception as e:
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Memory-specific stats endpoint
        @self.mcp.custom_route("/admin/api/memory/stats", methods=["GET"])
        async def memory_stats(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Get detailed memory statistics
                memory_stats = self.storage.get_collection_info()
                
                # Calculate additional metrics
                total_memories = sum(collection.get('count', 0) for collection in memory_stats.values())
                
                detailed_stats = {
                    "total_memories": total_memories,
                    "collections": memory_stats,
                    "categories": {
                        "project": memory_stats.get('project_memories', {}).get('count', 0),
                        "conversation": memory_stats.get('conversation_memories', {}).get('count', 0),
                        "agent": memory_stats.get('agent_memories', {}).get('count', 0),
                        "global": memory_stats.get('global_memories', {}).get('count', 0),
                        "infrastructure": memory_stats.get('infrastructure_memories', {}).get('count', 0),
                        "security": memory_stats.get('security_memories', {}).get('count', 0),
                    },
                    "storage_health": "healthy",
                    "last_updated": time.time()
                }
                
                return JSONResponse(detailed_stats)
            except Exception as e:
                logger.error(f"Error getting memory stats: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # ===== MCP SERVER MANAGEMENT APIs =====
        
        # List all registered MCP servers
        @self.mcp.custom_route("/admin/api/mcp/servers", methods=["GET"])
        async def list_mcp_servers(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Get servers from registry
                servers = []
                for server_id, server in self.server_registry.items():
                    server_data = dict(server)
                    # Update tools count for memory server
                    if server_id == "memory-server":
                        server_data["tools_count"] = len(await self._get_available_tools())
                        server_data["last_seen"] = time.time()
                    servers.append(server_data)
                
                return JSONResponse({
                    "servers": servers,
                    "total": len(servers),
                    "online": len([s for s in servers if s["status"] == "online"]),
                    "offline": len([s for s in servers if s["status"] == "offline"]),
                    "error": len([s for s in servers if s["status"] == "error"])
                })
            except Exception as e:
                logger.error(f"Error listing MCP servers: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Register new MCP server
        @self.mcp.custom_route("/admin/api/mcp/servers", methods=["POST"])
        async def register_mcp_server(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                required_fields = ["name", "endpoint", "transport"]
                
                if not all(field in data for field in required_fields):
                    return JSONResponse({"error": "Missing required fields"}, status_code=400)
                
                # Store in server registry
                server_id = f"server_{int(time.time())}"
                
                server = {
                    "id": server_id,
                    "name": data["name"],
                    "endpoint": data["endpoint"],
                    "transport": data["transport"],
                    "status": "registered",
                    "health": "unknown",
                    "tools_count": 0,
                    "last_seen": time.time(),
                    "created_at": time.time(),
                    "auto_start": data.get("auto_start", False)
                }
                
                # Add to registry
                self.server_registry[server_id] = server
                
                logger.info(f"Registered new MCP server: {server['name']} ({server_id})")
                return JSONResponse(server, status_code=201)
            except Exception as e:
                logger.error(f"Error registering MCP server: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get specific MCP server details
        @self.mcp.custom_route("/admin/api/mcp/servers/{server_id}", methods=["GET"])
        async def get_mcp_server(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                
                if server_id not in self.server_registry:
                    return JSONResponse({"error": "Server not found"}, status_code=404)
                
                server = dict(self.server_registry[server_id])
                
                # Add extra details for memory server
                if server_id == "memory-server":
                    server.update({
                        "tools_count": len(await self._get_available_tools()),
                        "last_seen": time.time(),
                        "uptime": time.time() - self._start_time,
                        "memory_usage": 0,  # TODO: Get actual metrics
                        "cpu_usage": 0,
                        "last_error": None
                    })
                
                return JSONResponse(server)
                    
            except Exception as e:
                logger.error(f"Error getting MCP server: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Update MCP server configuration
        @self.mcp.custom_route("/admin/api/mcp/servers/{server_id}", methods=["PUT"])
        async def update_mcp_server(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                data = await request.json()
                
                # TODO: Update in database
                logger.info(f"Updated MCP server configuration: {server_id}")
                return JSONResponse({"message": "Server updated", "id": server_id})
                
            except Exception as e:
                logger.error(f"Error updating MCP server: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Delete MCP server
        @self.mcp.custom_route("/admin/api/mcp/servers/{server_id}", methods=["DELETE"])
        async def delete_mcp_server(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                
                if server_id not in self.server_registry:
                    return JSONResponse({"error": "Server not found"}, status_code=404)
                
                # Don't allow deleting the main memory server
                if server_id == "memory-server":
                    return JSONResponse({"error": "Cannot delete main memory server"}, status_code=400)
                
                # Remove from registry
                del self.server_registry[server_id]
                
                logger.info(f"Deleted MCP server: {server_id}")
                return JSONResponse({"message": "Server deleted"})
                
            except Exception as e:
                logger.error(f"Error deleting MCP server: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Check server health
        @self.mcp.custom_route("/admin/api/mcp/servers/{server_id}/health", methods=["GET"])
        async def check_server_health(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                
                # TODO: Implement actual health check
                health = {
                    "server_id": server_id,
                    "status": "healthy",
                    "last_check": time.time(),
                    "response_time": 50,  # ms
                    "uptime": time.time() - self._start_time if server_id == "memory-server" else 0,
                    "errors_count": 0
                }
                
                return JSONResponse(health)
                
            except Exception as e:
                logger.error(f"Error checking server health: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Start MCP server
        @self.mcp.custom_route("/admin/api/mcp/servers/{server_id}/start", methods=["POST"])
        async def start_mcp_server(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                
                # TODO: Implement server start functionality
                logger.info(f"Starting MCP server: {server_id}")
                return JSONResponse({"message": f"Server {server_id} started"})
                
            except Exception as e:
                logger.error(f"Error starting server: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Stop MCP server  
        @self.mcp.custom_route("/admin/api/mcp/servers/{server_id}/stop", methods=["POST"])
        async def stop_mcp_server(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                
                # TODO: Implement server stop functionality
                logger.info(f"Stopping MCP server: {server_id}")
                return JSONResponse({"message": f"Server {server_id} stopped"})
                
            except Exception as e:
                logger.error(f"Error stopping server: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # List server tools
        @self.mcp.custom_route("/admin/api/mcp/servers/{server_id}/tools", methods=["GET"])
        async def list_server_tools(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                
                if server_id == "memory-server":
                    tools = await self._get_available_tools()
                    return JSONResponse({
                        "server_id": server_id,
                        "tools": [{"name": tool, "description": f"hAIveMind tool: {tool}"} for tool in tools]
                    })
                else:
                    return JSONResponse({"server_id": server_id, "tools": []})
                    
            except Exception as e:
                logger.error(f"Error listing server tools: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Browse all available tools
        @self.mcp.custom_route("/admin/api/mcp/tools/catalog", methods=["GET"])
        async def browse_tools_catalog(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                all_tools = await self._get_available_tools()
                catalog = [
                    {
                        "name": tool,
                        "server": "memory-server",
                        "description": f"hAIveMind memory tool: {tool}",
                        "category": "memory" if "memory" in tool else "system"
                    }
                    for tool in all_tools
                ]
                
                return JSONResponse({"tools": catalog, "total": len(catalog)})
                
            except Exception as e:
                logger.error(f"Error browsing tools catalog: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Test tool execution
        @self.mcp.custom_route("/admin/api/mcp/tools/invoke", methods=["POST"])
        async def invoke_tool(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                tool_name = data.get("tool")
                args = data.get("arguments", {})
                
                # TODO: Implement actual tool invocation
                result = {
                    "tool": tool_name,
                    "success": True,
                    "result": f"Tool {tool_name} executed successfully (test)",
                    "execution_time": 0.1,
                    "timestamp": time.time()
                }
                
                return JSONResponse(result)
                
            except Exception as e:
                logger.error(f"Error invoking tool: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # ===== END MCP SERVER MANAGEMENT APIs =====
        
        # ===== CONFLUENCE INTEGRATION APIs =====
        
        # Confluence configuration API
        @self.mcp.custom_route("/admin/api/confluence/config", methods=["GET", "POST"])
        async def confluence_config(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                if request.method == "GET":
                    # Get current configuration
                    config = {
                        "server_url": "",
                        "username": "",
                        "api_token": "",
                        "connection_status": "not_configured",
                        "last_tested": None
                    }
                    
                    # Load from config if available
                    confluence_config = self.config.get("confluence", {})
                    if confluence_config:
                        config.update({
                            "server_url": confluence_config.get("server_url", ""),
                            "username": confluence_config.get("username", ""),
                            "api_token": confluence_config.get("api_token", ""),
                            "connection_status": confluence_config.get("connection_status", "not_configured"),
                            "last_tested": confluence_config.get("last_tested")
                        })
                    
                    return JSONResponse(config)
                
                elif request.method == "POST":
                    # Update configuration
                    data = await request.json()
                    
                    # Validate required fields
                    required_fields = ["server_url", "username", "api_token"]
                    if not all(field in data for field in required_fields):
                        return JSONResponse({"error": "Missing required fields"}, status_code=400)
                    
                    # Update configuration
                    if "confluence" not in self.config:
                        self.config["confluence"] = {}
                    
                    self.config["confluence"].update({
                        "server_url": data["server_url"],
                        "username": data["username"],
                        "api_token": data["api_token"],
                        "connection_status": "configured",
                        "last_updated": time.time()
                    })
                    
                    # Save configuration
                    await self._save_config()
                    
                    return JSONResponse({
                        "message": "Configuration updated",
                        "status": "success"
                    })
                    
            except Exception as e:
                logger.error(f"Error managing Confluence config: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Test Confluence connection
        @self.mcp.custom_route("/admin/api/confluence/test", methods=["POST"])
        async def test_confluence_connection(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                confluence_config = self.config.get("confluence", {})
                if not confluence_config.get("server_url"):
                    return JSONResponse({"error": "Confluence not configured"}, status_code=400)
                
                # Test connection using existing integration
                from confluence_integration import ConfluenceIntegration
                
                integration = ConfluenceIntegration(
                    server_url=confluence_config["server_url"],
                    username=confluence_config["username"],
                    api_token=confluence_config["api_token"]
                )
                
                test_result = await integration.test_connection()
                
                # Update configuration with test results
                self.config["confluence"]["connection_status"] = "connected" if test_result["success"] else "error"
                self.config["confluence"]["last_tested"] = time.time()
                await self._save_config()
                
                return JSONResponse(test_result)
                
            except Exception as e:
                logger.error(f"Error testing Confluence connection: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get Confluence spaces
        @self.mcp.custom_route("/admin/api/confluence/spaces", methods=["GET"])
        async def get_confluence_spaces(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                confluence_config = self.config.get("confluence", {})
                if not confluence_config.get("server_url"):
                    return JSONResponse({"error": "Confluence not configured"}, status_code=400)
                
                from confluence_integration import ConfluenceIntegration
                
                integration = ConfluenceIntegration(
                    server_url=confluence_config["server_url"],
                    username=confluence_config["username"],
                    api_token=confluence_config["api_token"]
                )
                
                spaces = await integration.get_spaces()
                
                return JSONResponse({
                    "spaces": spaces,
                    "total": len(spaces)
                })
                
            except Exception as e:
                logger.error(f"Error getting Confluence spaces: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get pages from a specific space
        @self.mcp.custom_route("/admin/api/confluence/spaces/{space_key}/pages", methods=["GET"])
        async def get_confluence_pages(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                space_key = request.path_params["space_key"]
                confluence_config = self.config.get("confluence", {})
                
                if not confluence_config.get("server_url"):
                    return JSONResponse({"error": "Confluence not configured"}, status_code=400)
                
                from confluence_integration import ConfluenceIntegration
                
                integration = ConfluenceIntegration(
                    server_url=confluence_config["server_url"],
                    username=confluence_config["username"],
                    api_token=confluence_config["api_token"]
                )
                
                pages = await integration.get_pages(space_key)
                
                return JSONResponse({
                    "space_key": space_key,
                    "pages": pages,
                    "total": len(pages)
                })
                
            except Exception as e:
                logger.error(f"Error getting Confluence pages: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Import pages into hAIveMind
        @self.mcp.custom_route("/admin/api/confluence/import", methods=["POST"])
        async def import_confluence_pages(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                page_ids = data.get("page_ids", [])
                
                if not page_ids:
                    return JSONResponse({"error": "No pages selected for import"}, status_code=400)
                
                confluence_config = self.config.get("confluence", {})
                if not confluence_config.get("server_url"):
                    return JSONResponse({"error": "Confluence not configured"}, status_code=400)
                
                from confluence_integration import ConfluenceIntegration
                
                integration = ConfluenceIntegration(
                    server_url=confluence_config["server_url"],
                    username=confluence_config["username"],
                    api_token=confluence_config["api_token"]
                )
                
                results = []
                for page_id in page_ids:
                    try:
                        result = await integration.import_page(page_id, self.storage)
                        results.append(result)
                    except Exception as e:
                        results.append({
                            "page_id": page_id,
                            "success": False,
                            "error": str(e)
                        })
                
                return JSONResponse({
                    "results": results,
                    "total_processed": len(results),
                    "successful": len([r for r in results if r.get("success", False)]),
                    "failed": len([r for r in results if not r.get("success", False)])
                })
                
            except Exception as e:
                logger.error(f"Error importing Confluence pages: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get sync status and statistics
        @self.mcp.custom_route("/admin/api/confluence/status", methods=["GET"])
        async def get_confluence_status(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                confluence_config = self.config.get("confluence", {})
                
                # Count imported pages from hAIveMind memory
                try:
                    confluence_memories = await self.storage.search_memories(
                        query="", 
                        category="confluence",
                        limit=1000
                    )
                    synced_pages_count = len(confluence_memories) if isinstance(confluence_memories, list) else len(confluence_memories.get("memories", []))
                except Exception as e:
                    logger.warning(f"Error counting Confluence memories: {e}")
                    synced_pages_count = 0
                
                status = {
                    "connection_status": confluence_config.get("connection_status", "not_configured"),
                    "last_tested": confluence_config.get("last_tested"),
                    "last_sync": confluence_config.get("last_sync"),
                    "monitored_spaces": len(confluence_config.get("monitored_spaces", [])),
                    "synced_pages": synced_pages_count,
                    "total_pages": confluence_config.get("total_pages", 0),
                    "sync_enabled": confluence_config.get("sync_enabled", False)
                }
                
                return JSONResponse(status)
                
            except Exception as e:
                logger.error(f"Error getting Confluence status: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Trigger manual sync
        @self.mcp.custom_route("/admin/api/confluence/sync", methods=["POST"])
        async def trigger_confluence_sync(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                confluence_config = self.config.get("confluence", {})
                if not confluence_config.get("server_url"):
                    return JSONResponse({"error": "Confluence not configured"}, status_code=400)
                
                # Trigger manual sync (for now, just update last_sync timestamp)
                self.config["confluence"]["last_sync"] = time.time()
                await self._save_config()
                
                return JSONResponse({
                    "message": "Manual sync triggered",
                    "timestamp": time.time(),
                    "status": "in_progress"
                })
                
            except Exception as e:
                logger.error(f"Error triggering Confluence sync: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # ===== END CONFLUENCE INTEGRATION APIs =====
        
        # ===== PLAYBOOK MANAGEMENT APIs =====
        
        # List all playbooks
        @self.mcp.custom_route("/admin/api/playbooks", methods=["GET"])
        async def list_playbooks(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Get all playbooks from memory with category "playbook"
                playbooks = []
                try:
                    search_results = await self.storage.search_memories("", category="playbook", limit=100)
                    for result in search_results:
                        playbook_data = json.loads(result.get('content', '{}'))
                        playbooks.append({
                            "id": result['id'],
                            "name": playbook_data.get('name', 'Untitled Playbook'),
                            "description": playbook_data.get('description', ''),
                            "type": playbook_data.get('type', 'ansible'),
                            "tags": playbook_data.get('tags', []),
                            "created_at": result.get('created_at'),
                            "updated_at": result.get('updated_at'),
                            "status": playbook_data.get('status', 'draft'),
                            "execution_count": playbook_data.get('execution_count', 0)
                        })
                except Exception as e:
                    logger.warning(f"Error loading playbooks from memory: {e}")
                
                return JSONResponse({
                    "playbooks": playbooks,
                    "total": len(playbooks),
                    "categories": ["ansible", "terraform", "kubernetes", "shell", "python"]
                })
                
            except Exception as e:
                logger.error(f"Error listing playbooks: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Create new playbook
        @self.mcp.custom_route("/admin/api/playbooks", methods=["POST"])
        async def create_playbook(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                data = await request.json()
                
                # Validate required fields
                if not data.get("name"):
                    return JSONResponse({"error": "Playbook name is required"}, status_code=400)
                
                playbook_data = {
                    "name": data.get("name"),
                    "description": data.get("description", ""),
                    "type": data.get("type", "ansible"),
                    "content": data.get("content", ""),
                    "variables": data.get("variables", {}),
                    "tags": data.get("tags", []),
                    "targets": data.get("targets", []),
                    "status": "draft",
                    "execution_count": 0,
                    "created_by": "admin",
                    "version": "1.0"
                }
                
                # Store playbook in memory
                memory_id = await self.storage.store_memory(
                    content=json.dumps(playbook_data),
                    category="playbook",
                    tags=["playbook", data.get("type", "ansible")] + data.get("tags", []),
                    metadata={
                        "name": data.get("name"),
                        "type": data.get("type", "ansible"),
                        "status": "draft"
                    }
                )
                
                return JSONResponse({
                    "id": memory_id,
                    "message": "Playbook created successfully",
                    "playbook": {
                        "id": memory_id,
                        **playbook_data
                    }
                })
                
            except Exception as e:
                logger.error(f"Error creating playbook: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get available playbook templates
        @self.mcp.custom_route("/admin/api/playbooks/templates", methods=["GET"])
        async def get_playbook_templates(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                templates = [
                    {
                        "id": "ansible_basic",
                        "name": "Basic Ansible Playbook",
                        "type": "ansible",
                        "description": "Simple Ansible playbook template with common tasks",
                        "content": """---
- name: Basic Ansible Playbook
  hosts: all
  become: yes
  vars:
    package_name: nginx
    
  tasks:
    - name: Update package cache
      apt:
        update_cache: yes
        cache_valid_time: 3600
    
    - name: Install package
      apt:
        name: "{{ package_name }}"
        state: present
    
    - name: Start and enable service
      service:
        name: "{{ package_name }}"
        state: started
        enabled: yes""",
                        "variables": {
                            "package_name": {"type": "string", "default": "nginx", "description": "Package to install"}
                        }
                    },
                    {
                        "id": "terraform_basic",
                        "name": "Basic Terraform Configuration",
                        "type": "terraform",
                        "description": "Simple Terraform configuration for AWS resources",
                        "content": """# Basic Terraform Configuration
terraform {
  required_version = ">= 0.14"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-west-2"
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t3.micro"
}

resource "aws_instance" "example" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = var.instance_type
  
  tags = {
    Name = "terraform-example"
  }
}

data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"] # Canonical
  
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-20.04-amd64-server-*"]
  }
}""",
                        "variables": {
                            "aws_region": {"type": "string", "default": "us-west-2", "description": "AWS region"},
                            "instance_type": {"type": "string", "default": "t3.micro", "description": "EC2 instance type"}
                        }
                    },
                    {
                        "id": "kubernetes_deployment",
                        "name": "Kubernetes Deployment",
                        "type": "kubernetes",
                        "description": "Basic Kubernetes deployment and service",
                        "content": """apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ app_name }}
  labels:
    app: {{ app_name }}
spec:
  replicas: {{ replicas }}
  selector:
    matchLabels:
      app: {{ app_name }}
  template:
    metadata:
      labels:
        app: {{ app_name }}
    spec:
      containers:
      - name: {{ app_name }}
        image: {{ image }}
        ports:
        - containerPort: {{ port }}
        env:
        - name: ENV
          value: "{{ environment }}"
---
apiVersion: v1
kind: Service
metadata:
  name: {{ app_name }}-service
spec:
  selector:
    app: {{ app_name }}
  ports:
  - protocol: TCP
    port: 80
    targetPort: {{ port }}
  type: LoadBalancer""",
                        "variables": {
                            "app_name": {"type": "string", "default": "myapp", "description": "Application name"},
                            "image": {"type": "string", "default": "nginx:latest", "description": "Container image"},
                            "replicas": {"type": "number", "default": 3, "description": "Number of replicas"},
                            "port": {"type": "number", "default": 80, "description": "Container port"},
                            "environment": {"type": "string", "default": "production", "description": "Environment"}
                        }
                    },
                    {
                        "id": "shell_script",
                        "name": "Shell Script Template",
                        "type": "shell",
                        "description": "Basic shell script with error handling",
                        "content": """#!/bin/bash

# Shell Script Template
# Description: {{ description }}

set -euo pipefail

# Variables
LOG_FILE="/var/log/{{ script_name }}.log"
TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')

# Functions
log() {
    echo "[$TIMESTAMP] $1" | tee -a "$LOG_FILE"
}

error_exit() {
    log "ERROR: $1"
    exit 1
}

# Main script
main() {
    log "Starting {{ script_name }}"
    
    # Add your commands here
    log "Executing main tasks..."
    
    # Example task
    if command -v {{ command }} &> /dev/null; then
        log "{{ command }} is installed"
        {{ command }} --version
    else
        error_exit "{{ command }} is not installed"
    fi
    
    log "Script completed successfully"
}

# Run main function
main "$@" """,
                        "variables": {
                            "script_name": {"type": "string", "default": "my_script", "description": "Script name"},
                            "description": {"type": "string", "default": "My shell script", "description": "Script description"},
                            "command": {"type": "string", "default": "docker", "description": "Command to check"}
                        }
                    }
                ]
                
                return JSONResponse({
                    "templates": templates,
                    "total": len(templates),
                    "categories": ["ansible", "terraform", "kubernetes", "shell"]
                })
                
            except Exception as e:
                logger.error(f"Error getting templates: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get specific playbook
        @self.mcp.custom_route("/admin/api/playbooks/{playbook_id}", methods=["GET"])
        async def get_playbook(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                playbook_id = request.path_params["playbook_id"]
                
                # Retrieve playbook from memory
                memory = await self.storage.retrieve_memory(playbook_id)
                if not memory:
                    return JSONResponse({"error": "Playbook not found"}, status_code=404)
                
                playbook_data = json.loads(memory.get('content', '{}'))
                
                return JSONResponse({
                    "id": playbook_id,
                    **playbook_data,
                    "created_at": memory.get('created_at'),
                    "updated_at": memory.get('updated_at')
                })
                
            except Exception as e:
                logger.error(f"Error retrieving playbook: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Update playbook
        @self.mcp.custom_route("/admin/api/playbooks/{playbook_id}", methods=["PUT"])
        async def update_playbook(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                playbook_id = request.path_params["playbook_id"]
                data = await request.json()
                
                # Retrieve existing playbook
                existing = await self.storage.retrieve_memory(playbook_id)
                if not existing:
                    return JSONResponse({"error": "Playbook not found"}, status_code=404)
                
                playbook_data = json.loads(existing.get('content', '{}'))
                
                # Update fields
                playbook_data.update({
                    "name": data.get("name", playbook_data.get("name")),
                    "description": data.get("description", playbook_data.get("description")),
                    "content": data.get("content", playbook_data.get("content")),
                    "variables": data.get("variables", playbook_data.get("variables", {})),
                    "tags": data.get("tags", playbook_data.get("tags", [])),
                    "targets": data.get("targets", playbook_data.get("targets", [])),
                    "status": data.get("status", playbook_data.get("status", "draft")),
                    "modified_by": "admin",
                    "version": str(float(playbook_data.get("version", "1.0")) + 0.1)
                })
                
                # Update in memory
                await self.storage.update_memory(
                    playbook_id,
                    content=json.dumps(playbook_data),
                    tags=["playbook", playbook_data.get("type", "ansible")] + playbook_data.get("tags", []),
                    metadata={
                        "name": playbook_data.get("name"),
                        "type": playbook_data.get("type", "ansible"),
                        "status": playbook_data.get("status", "draft")
                    }
                )
                
                return JSONResponse({
                    "id": playbook_id,
                    "message": "Playbook updated successfully",
                    "playbook": playbook_data
                })
                
            except Exception as e:
                logger.error(f"Error updating playbook: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Delete playbook
        @self.mcp.custom_route("/admin/api/playbooks/{playbook_id}", methods=["DELETE"])
        async def delete_playbook(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                playbook_id = request.path_params["playbook_id"]
                
                # Check if playbook exists
                existing = await self.storage.retrieve_memory(playbook_id)
                if not existing:
                    return JSONResponse({"error": "Playbook not found"}, status_code=404)
                
                # Delete from memory
                await self.storage.delete_memory(playbook_id)
                
                return JSONResponse({
                    "message": "Playbook deleted successfully",
                    "id": playbook_id
                })
                
            except Exception as e:
                logger.error(f"Error deleting playbook: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Execute playbook
        @self.mcp.custom_route("/admin/api/playbooks/{playbook_id}/execute", methods=["POST"])
        async def execute_playbook(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                playbook_id = request.path_params["playbook_id"]
                data = await request.json()
                
                # Retrieve playbook
                playbook_memory = await self.storage.retrieve_memory(playbook_id)
                if not playbook_memory:
                    return JSONResponse({"error": "Playbook not found"}, status_code=404)
                
                playbook_data = json.loads(playbook_memory.get('content', '{}'))
                
                # Create execution record
                execution_id = f"exec_{int(time.time())}_{playbook_id[:8]}"
                execution_data = {
                    "execution_id": execution_id,
                    "playbook_id": playbook_id,
                    "playbook_name": playbook_data.get("name"),
                    "status": "running",
                    "started_at": time.time(),
                    "variables": data.get("variables", {}),
                    "targets": data.get("targets", playbook_data.get("targets", [])),
                    "logs": [],
                    "progress": 0,
                    "steps_total": 1,
                    "steps_completed": 0,
                    "executed_by": "admin"
                }
                
                # Store execution record
                await self.storage.store_memory(
                    content=json.dumps(execution_data),
                    category="execution",
                    tags=["execution", "playbook", execution_id],
                    metadata={
                        "execution_id": execution_id,
                        "playbook_id": playbook_id,
                        "status": "running"
                    }
                )
                
                # Update playbook execution count
                playbook_data["execution_count"] = playbook_data.get("execution_count", 0) + 1
                await self.storage.update_memory(
                    playbook_id,
                    content=json.dumps(playbook_data)
                )
                
                # For now, simulate execution completion
                execution_data.update({
                    "status": "completed",
                    "completed_at": time.time(),
                    "progress": 100,
                    "steps_completed": 1,
                    "logs": ["Execution started", "Playbook executed successfully", "Execution completed"]
                })
                
                return JSONResponse({
                    "execution_id": execution_id,
                    "status": "completed",
                    "message": "Playbook execution completed",
                    "execution": execution_data
                })
                
            except Exception as e:
                logger.error(f"Error executing playbook: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get playbook execution history
        @self.mcp.custom_route("/admin/api/playbooks/{playbook_id}/executions", methods=["GET"])
        async def get_playbook_executions(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                playbook_id = request.path_params["playbook_id"]
                
                # Search for executions of this playbook
                executions = []
                try:
                    search_results = await self.storage.search_memories(
                        f"playbook_id:{playbook_id}",
                        category="execution",
                        limit=50
                    )
                    
                    for result in search_results:
                        exec_data = json.loads(result.get('content', '{}'))
                        executions.append({
                            "execution_id": exec_data.get("execution_id"),
                            "status": exec_data.get("status"),
                            "started_at": exec_data.get("started_at"),
                            "completed_at": exec_data.get("completed_at"),
                            "duration": exec_data.get("completed_at", 0) - exec_data.get("started_at", 0) if exec_data.get("completed_at") else None,
                            "executed_by": exec_data.get("executed_by"),
                            "progress": exec_data.get("progress", 0)
                        })
                except Exception as e:
                    logger.warning(f"Error loading executions: {e}")
                
                return JSONResponse({
                    "playbook_id": playbook_id,
                    "executions": executions,
                    "total": len(executions)
                })
                
            except Exception as e:
                logger.error(f"Error getting executions: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        
        # ===== END PLAYBOOK MANAGEMENT APIs =====
        
        # ===== RULES & GOVERNANCE APIs =====
        
        # List all rules with filtering
        @self.mcp.custom_route("/admin/api/rules", methods=["GET"])
        async def list_rules(request):
            """List all rules with optional filtering"""
            try:
                # Get query parameters
                rule_type = request.query_params.get("type")
                scope = request.query_params.get("scope")
                status = request.query_params.get("status")
                limit = int(request.query_params.get("limit", 50))
                
                # Search for rules in memory
                search_query = "rule"
                search_results = await self.storage.search_memories(
                    search_query, 
                    category="rules", 
                    limit=limit
                )
                
                rules = []
                for memory in search_results.get("memories", []):
                    try:
                        rule_data = json.loads(memory["content"])
                        
                        # Apply filters
                        if rule_type and rule_data.get("rule_type") != rule_type:
                            continue
                        if scope and rule_data.get("scope") != scope:
                            continue
                        if status and rule_data.get("status") != status:
                            continue
                        
                        rules.append({
                            "id": memory["id"],
                            "name": rule_data.get("name", "Unnamed Rule"),
                            "description": rule_data.get("description", ""),
                            "rule_type": rule_data.get("rule_type", "custom"),
                            "scope": rule_data.get("scope", "global"),
                            "priority": rule_data.get("priority", 500),
                            "status": rule_data.get("status", "active"),
                            "created_at": memory.get("created_at"),
                            "updated_at": memory.get("updated_at"),
                            "conditions": rule_data.get("conditions", []),
                            "actions": rule_data.get("actions", []),
                            "tags": rule_data.get("tags", [])
                        })
                    except (json.JSONDecodeError, KeyError) as e:
                        logger.warning(f"Error parsing rule memory: {e}")
                        continue
                
                return JSONResponse({
                    "rules": rules,
                    "total": len(rules),
                    "filters": {
                        "type": rule_type,
                        "scope": scope,
                        "status": status
                    }
                })
                
            except Exception as e:
                logger.error(f"Error listing rules: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Create new rule
        @self.mcp.custom_route("/admin/api/rules", methods=["POST"])
        async def create_rule(request):
            """Create a new rule"""
            try:
                rule_data = await request.json()
                
                # Validate required fields
                if not rule_data.get("name"):
                    return JSONResponse({"error": "Rule name is required"}, status_code=400)
                
                # Generate rule ID
                rule_id = str(uuid.uuid4())
                
                # Set default values
                rule = {
                    "id": rule_id,
                    "name": rule_data["name"],
                    "description": rule_data.get("description", ""),
                    "rule_type": rule_data.get("rule_type", "custom"),
                    "scope": rule_data.get("scope", "global"),
                    "priority": rule_data.get("priority", 500),
                    "status": rule_data.get("status", "active"),
                    "conditions": rule_data.get("conditions", []),
                    "actions": rule_data.get("actions", []),
                    "tags": rule_data.get("tags", []),
                    "created_at": datetime.utcnow().isoformat(),
                    "updated_at": datetime.utcnow().isoformat(),
                    "created_by": "admin"  # TODO: Get from auth
                }
                
                # Store rule in memory
                await self.storage.store_memory(
                    content=json.dumps(rule),
                    category="rules",
                    context=f"Rule: {rule['name']}",
                    tags=["rule", rule["rule_type"]] + rule["tags"]
                )
                
                return JSONResponse({
                    "success": True,
                    "rule": rule,
                    "message": f"Rule '{rule['name']}' created successfully"
                })
                
            except Exception as e:
                logger.error(f"Error creating rule: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get rule templates
        @self.mcp.custom_route("/admin/api/rules/templates", methods=["GET"])
        async def get_rule_templates(request):
            """Get available rule templates"""
            try:
                templates = [
                    {
                        "id": "security_policy",
                        "name": "Security Policy Rule",
                        "description": "Template for security and access control policies",
                        "rule_type": "security",
                        "conditions": [
                            {"type": "user_role", "operator": "equals", "value": "admin"}
                        ],
                        "actions": [
                            {"type": "allow_access", "target": "admin_panel"}
                        ]
                    },
                    {
                        "id": "infrastructure_governance",
                        "name": "Infrastructure Governance",
                        "description": "Template for infrastructure compliance and governance",
                        "rule_type": "infrastructure",
                        "conditions": [
                            {"type": "resource_type", "operator": "equals", "value": "server"}
                        ],
                        "actions": [
                            {"type": "apply_policy", "policy": "compliance_standard"}
                        ]
                    },
                    {
                        "id": "workflow_automation",
                        "name": "Workflow Automation",
                        "description": "Template for automated workflow triggers",
                        "rule_type": "automation",
                        "conditions": [
                            {"type": "event", "operator": "equals", "value": "deployment_complete"}
                        ],
                        "actions": [
                            {"type": "trigger_playbook", "playbook_id": "post_deployment"}
                        ]
                    },
                    {
                        "id": "monitoring_alert",
                        "name": "Monitoring & Alerting",
                        "description": "Template for monitoring rules and alerting",
                        "rule_type": "monitoring",
                        "conditions": [
                            {"type": "metric", "operator": "greater_than", "value": "80", "metric": "cpu_usage"}
                        ],
                        "actions": [
                            {"type": "send_alert", "severity": "warning"}
                        ]
                    },
                    {
                        "id": "compliance_audit",
                        "name": "Compliance Audit",
                        "description": "Template for compliance monitoring and audit trails",
                        "rule_type": "compliance",
                        "conditions": [
                            {"type": "audit_event", "operator": "contains", "value": "sensitive_data_access"}
                        ],
                        "actions": [
                            {"type": "log_audit", "level": "high_priority"}
                        ]
                    }
                ]
                
                return JSONResponse({
                    "templates": templates,
                    "total": len(templates)
                })
                
            except Exception as e:
                logger.error(f"Error getting rule templates: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get specific rule
        @self.mcp.custom_route("/admin/api/rules/{rule_id}", methods=["GET"])
        async def get_rule(request):
            """Get a specific rule by ID"""
            try:
                rule_id = request.path_params["rule_id"]
                
                # Search for the specific rule
                search_results = await self.storage.search_memories(
                    rule_id, 
                    category="rules", 
                    limit=1
                )
                
                for memory in search_results.get("memories", []):
                    if memory["id"] == rule_id:
                        try:
                            rule_data = json.loads(memory["content"])
                            rule_data["id"] = memory["id"]
                            rule_data["created_at"] = memory.get("created_at")
                            rule_data["updated_at"] = memory.get("updated_at")
                            
                            return JSONResponse({
                                "rule": rule_data,
                                "success": True
                            })
                        except json.JSONDecodeError as e:
                            return JSONResponse({"error": f"Invalid rule data: {e}"}, status_code=500)
                
                return JSONResponse({"error": "Rule not found"}, status_code=404)
                
            except Exception as e:
                logger.error(f"Error getting rule: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Update rule
        @self.mcp.custom_route("/admin/api/rules/{rule_id}", methods=["PUT"])
        async def update_rule(request):
            """Update an existing rule"""
            try:
                rule_id = request.path_params["rule_id"]
                update_data = await request.json()
                
                # Get existing rule
                search_results = await self.storage.search_memories(
                    rule_id, 
                    category="rules", 
                    limit=1
                )
                
                existing_rule = None
                for memory in search_results.get("memories", []):
                    if memory["id"] == rule_id:
                        existing_rule = json.loads(memory["content"])
                        break
                
                if not existing_rule:
                    return JSONResponse({"error": "Rule not found"}, status_code=404)
                
                # Update rule data
                existing_rule.update({
                    k: v for k, v in update_data.items() 
                    if k not in ["id", "created_at", "created_by"]
                })
                existing_rule["updated_at"] = datetime.utcnow().isoformat()
                
                # Store updated rule
                await self.storage.store_memory(
                    content=json.dumps(existing_rule),
                    category="rules",
                    context=f"Rule: {existing_rule['name']} (Updated)",
                    tags=["rule", existing_rule.get("rule_type", "custom")] + existing_rule.get("tags", [])
                )
                
                return JSONResponse({
                    "success": True,
                    "rule": existing_rule,
                    "message": f"Rule '{existing_rule['name']}' updated successfully"
                })
                
            except Exception as e:
                logger.error(f"Error updating rule: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Delete rule
        @self.mcp.custom_route("/admin/api/rules/{rule_id}", methods=["DELETE"])
        async def delete_rule(request):
            """Delete a rule"""
            try:
                rule_id = request.path_params["rule_id"]
                
                # Get rule name for response
                search_results = await self.storage.search_memories(
                    rule_id, 
                    category="rules", 
                    limit=1
                )
                
                rule_name = "Unknown Rule"
                for memory in search_results.get("memories", []):
                    if memory["id"] == rule_id:
                        try:
                            rule_data = json.loads(memory["content"])
                            rule_name = rule_data.get("name", "Unknown Rule")
                        except:
                            pass
                        break
                
                # Mark rule as deleted by storing deletion record
                deletion_record = {
                    "rule_id": rule_id,
                    "rule_name": rule_name,
                    "deleted_at": datetime.utcnow().isoformat(),
                    "deleted_by": "admin",  # TODO: Get from auth
                    "action": "deleted"
                }
                
                await self.storage.store_memory(
                    content=json.dumps(deletion_record),
                    category="rules",
                    context=f"Rule Deleted: {rule_name}",
                    tags=["rule", "deleted"]
                )
                
                return JSONResponse({
                    "success": True,
                    "message": f"Rule '{rule_name}' deleted successfully"
                })
                
            except Exception as e:
                logger.error(f"Error deleting rule: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Test rule execution
        @self.mcp.custom_route("/admin/api/rules/{rule_id}/test", methods=["POST"])
        async def test_rule(request):
            """Test rule execution with sample data"""
            try:
                rule_id = request.path_params["rule_id"]
                test_data = await request.json()
                
                # Get the rule
                search_results = await self.storage.search_memories(
                    rule_id, 
                    category="rules", 
                    limit=1
                )
                
                rule = None
                for memory in search_results.get("memories", []):
                    if memory["id"] == rule_id:
                        rule = json.loads(memory["content"])
                        break
                
                if not rule:
                    return JSONResponse({"error": "Rule not found"}, status_code=404)
                
                # Simulate rule evaluation
                conditions_met = 0
                total_conditions = len(rule.get("conditions", []))
                
                evaluation_results = []
                for condition in rule.get("conditions", []):
                    # Simple condition evaluation simulation
                    result = {
                        "condition": condition,
                        "met": True,  # Simplified - always pass for demo
                        "message": f"Condition '{condition.get('type', 'unknown')}' evaluated successfully"
                    }
                    evaluation_results.append(result)
                    conditions_met += 1
                
                # Actions that would be executed
                actions_to_execute = rule.get("actions", [])
                
                test_result = {
                    "rule_id": rule_id,
                    "rule_name": rule.get("name", "Unknown"),
                    "test_status": "passed" if conditions_met == total_conditions else "failed",
                    "conditions_met": conditions_met,
                    "total_conditions": total_conditions,
                    "evaluation_results": evaluation_results,
                    "actions_to_execute": actions_to_execute,
                    "test_data": test_data,
                    "tested_at": datetime.utcnow().isoformat()
                }
                
                # Store test result
                await self.storage.store_memory(
                    content=json.dumps(test_result),
                    category="rules",
                    context=f"Rule Test: {rule.get('name')}",
                    tags=["rule", "test", rule.get("rule_type", "custom")]
                )
                
                return JSONResponse({
                    "success": True,
                    "test_result": test_result
                })
                
            except Exception as e:
                logger.error(f"Error testing rule: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # ===== END RULES & GOVERNANCE APIs =====
        
        # Execution monitoring dashboard endpoint (must come before catch-all admin route)
        @self.mcp.custom_route("/admin/mcp_bridge.html", methods=["GET"])
        @self.mcp.custom_route("/admin/mcp-bridge", methods=["GET"])
        async def mcp_bridge_dashboard(request):
            """Serve the MCP Bridge management dashboard interface"""
            try:
                bridge_dashboard_path = Path(__file__).parent.parent / "admin" / "mcp_bridge.html"
                if bridge_dashboard_path.exists():
                    return FileResponse(str(bridge_dashboard_path), media_type="text/html")
                else:
                    return JSONResponse({"error": "MCP Bridge dashboard not found"}, status_code=404)
            except Exception as e:
                logger.error(f"Error serving MCP Bridge dashboard: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        @self.mcp.custom_route("/admin/executions", methods=["GET"])
        async def executions_dashboard(request):
            """Serve the execution monitoring dashboard interface"""
            try:
                return HTMLResponse("""
                <!DOCTYPE html>
                <html>
                <head>
                    <title>Execution Monitoring - hAIveMind</title>
                    <link rel="stylesheet" href="/admin/static/admin.css">
                    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
                </head>
                <body class="dashboard">
                    <nav class="nav-header">
                        <div class="nav-brand">
                            <img src="/assets/logo.png" alt="hAIveMind" class="nav-logo">
                            <h1>hAIveMind Execution Monitoring</h1>
                        </div>
                        <div class="nav-links">
                            <a href="/admin/dashboard.html">Dashboard</a>
                            <a href="/admin/memory.html">Memory Browser</a>
                            <a href="/admin/mcp_servers.html">MCP Servers</a>
                            <a href="/admin/vault">Vault Management</a>
                            <a href="/admin/rules-dashboard">Rules & Governance</a>
                            <a href="/admin/playbooks">Playbook Management</a>
                            <a href="/admin/executions" class="active">Execution Monitoring</a>
                            <a href="/admin/confluence">Confluence Integration</a>
                            <a href="/admin/help-dashboard">Help System</a>
                        </div>
                        <button class="logout-btn" onclick="logout()">Logout</button>
                    </nav>
                    <main class="main-content">
                        <div class="card">
                            <h2><i class="fas fa-tasks"></i> Execution Monitoring</h2>
                            <p>Real-time monitoring of playbook executions with detailed logging and analytics.</p>
                            
                            <div class="execution-overview">
                                <div class="status-grid">
                                    <div class="status-item">
                                        <span class="status-label">Active Executions</span>
                                        <span class="status-value">0</span>
                                    </div>
                                    <div class="status-item">
                                        <span class="status-label">Queued Tasks</span>
                                        <span class="status-value">0</span>
                                    </div>
                                    <div class="status-item">
                                        <span class="status-label">Failed Today</span>
                                        <span class="status-value">0</span>
                                    </div>
                                    <div class="status-item">
                                        <span class="status-label">System Load</span>
                                        <span class="status-value status-good">Low</span>
                                    </div>
                                </div>
                            </div>

                            <div class="execution-controls">
                                <h3>Execution Controls</h3>
                                <button class="btn btn-primary" onclick="viewActiveExecutions()">
                                    <i class="fas fa-eye"></i> Active Executions
                                </button>
                                <button class="btn btn-secondary" onclick="viewExecutionHistory()">
                                    <i class="fas fa-history"></i> Execution History
                                </button>
                                <button class="btn btn-info" onclick="viewExecutionLogs()">
                                    <i class="fas fa-file-text"></i> Execution Logs
                                </button>
                                <button class="btn btn-warning" onclick="viewFailedExecutions()">
                                    <i class="fas fa-exclamation-triangle"></i> Failed Executions
                                </button>
                            </div>
                        </div>
                    </main>
                    <script src="/admin/static/admin.js"></script>
                    <script>
                        function viewActiveExecutions() { alert('Active executions viewer coming soon!'); }
                        function viewExecutionHistory() { alert('Execution history coming soon!'); }
                        function viewExecutionLogs() { alert('Execution logs coming soon!'); }
                        function viewFailedExecutions() { alert('Failed executions viewer coming soon!'); }
                    </script>
                </body>
                </html>
                """)
            except Exception as e:
                logger.error(f"Error serving execution monitoring dashboard: {e}")
                return JSONResponse({"error": "Execution monitoring dashboard unavailable"}, status_code=500)
        
        # Login redirect endpoint (for compatibility)
        @self.mcp.custom_route("/login", methods=["GET"])
        async def login_redirect(request):
            """Redirect /login to proper admin login page"""
            from starlette.responses import RedirectResponse
            return RedirectResponse(url="/admin/login.html", status_code=302)
        
# Device management endpoints
        @self.mcp.custom_route("/admin/api/devices", methods=["GET", "POST"])
        async def manage_devices(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                if request.method == "GET":
                    devices = self.dashboard_db.list_devices()
                    return JSONResponse([{
                        "id": d.id,
                        "device_id": d.device_id,
                        "machine_id": d.machine_id,
                        "hostname": d.hostname,
                        "status": d.status.value,
                        "created_at": d.created_at.isoformat(),
                        "last_seen": d.last_seen.isoformat() if d.last_seen else None
                    } for d in devices])
                
                elif request.method == "POST":
                    data = await request.json()
                    device_id = self.dashboard_db.register_device(
                        data['device_id'],
                        data['machine_id'], 
                        data['hostname'],
                        1,  # Default user ID
                        data.get('metadata', {}),
                        request.client.host
                    )
                    return JSONResponse({"device_id": device_id, "status": "registered"})
                    
            except Exception as e:
                logger.error(f"Error managing devices: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Configuration generation endpoints
        @self.mcp.custom_route("/admin/api/config/generate", methods=["POST"])
        async def generate_config(request):
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                if not self._config_generator:
                    return JSONResponse({"error": "Configuration generator not available"}, status_code=500)
                
                data = await request.json()
                device_id = data.get('device_id')
                format_str = data.get('format', 'claude_desktop')
                
                # Map format string to enum
                format_map = {
                    "claude_desktop": ConfigFormat.CLAUDE_DESKTOP,
                    "claude_code": ConfigFormat.CLAUDE_CODE,
                    "custom": ConfigFormat.CUSTOM_JSON,
                    "mcp_json": ConfigFormat.MCP_JSON,
                    "yaml": ConfigFormat.YAML,
                    "shell_script": ConfigFormat.SHELL_SCRIPT,
                    "docker_compose": ConfigFormat.DOCKER_COMPOSE
                }
                
                config_format = format_map.get(format_str, ConfigFormat.CLAUDE_DESKTOP)
                
                # Generate configuration
                client_config = self._config_generator.generate_client_config(
                    user_id="admin",
                    device_id=device_id,
                    format=config_format,
                    include_auth=data.get('include_auth', True)
                )
                
                config_string = self._config_generator.template_engine.generate(client_config)
                
                return JSONResponse({
                    "config": config_string,
                    "format": format_str,
                    "device_id": device_id,
                    "generated_at": datetime.utcnow().isoformat()
                })
                
            except Exception as e:
                logger.error(f"Error generating config: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        logger.info("ðŸ“Š Enhanced dashboard routes registered")
        
        # ===== MCP BRIDGE API ENDPOINTS =====
        
        # Discover local MCP servers
        @self.mcp.custom_route("/admin/api/mcp-bridge/discover", methods=["GET"])
        async def discover_local_servers(request):
            """Discover local MCP servers from configuration files"""
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                discovered = await self.bridge_manager.discover_local_servers()
                return JSONResponse({
                    "servers": discovered,
                    "total": len(discovered),
                    "status": "success"
                })
            except Exception as e:
                logger.error(f"Error discovering local servers: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Register MCP bridge
        @self.mcp.custom_route("/admin/api/mcp-bridge/register", methods=["POST"])
        async def register_bridge(request):
            """Register a new MCP server bridge"""
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                config = await request.json()
                server_id = await self.bridge_manager.register_bridge(config)
                
                return JSONResponse({
                    "server_id": server_id,
                    "endpoint": f"/mcp-bridge/{server_id}",
                    "status": "registered",
                    "success": True
                })
            except Exception as e:
                logger.error(f"Error registering bridge: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # List bridges
        @self.mcp.custom_route("/admin/api/mcp-bridge/servers", methods=["GET"])
        async def list_bridges(request):
            """List all registered MCP bridges"""
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                bridges = await self.bridge_manager.list_bridges()
                return JSONResponse({
                    "bridges": bridges,
                    "total": len(bridges),
                    "status": "success"
                })
            except Exception as e:
                logger.error(f"Error listing bridges: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Get bridge status
        @self.mcp.custom_route("/admin/api/mcp-bridge/{server_id}/status", methods=["GET"])
        async def get_bridge_status(request):
            """Get status of a specific bridge"""
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                status = await self.bridge_manager.get_bridge_status(server_id)
                
                if not status:
                    return JSONResponse({"error": "Bridge not found"}, status_code=404)
                
                return JSONResponse({
                    "status": status,
                    "success": True
                })
            except Exception as e:
                logger.error(f"Error getting bridge status: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Proxy requests to bridge
        @self.mcp.custom_route("/admin/api/mcp-bridge/{server_id}/proxy", methods=["POST"])
        async def proxy_bridge_request(request):
            """Proxy a request to a bridged MCP server"""
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                request_data = await request.json()
                
                method = request_data.get("method")
                params = request_data.get("params", {})
                
                if not method:
                    return JSONResponse({"error": "Method required"}, status_code=400)
                
                response = await self.bridge_manager.proxy_request(server_id, method, params)
                return JSONResponse(response)
                
            except ValueError as e:
                return JSONResponse({"error": str(e)}, status_code=404)
            except Exception as e:
                logger.error(f"Error proxying request: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Remove bridge
        @self.mcp.custom_route("/admin/api/mcp-bridge/{server_id}", methods=["DELETE"])
        async def remove_bridge(request):
            """Remove a registered bridge"""
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                server_id = request.path_params["server_id"]
                removed = await self.bridge_manager.remove_bridge(server_id)
                
                if not removed:
                    return JSONResponse({"error": "Bridge not found"}, status_code=404)
                
                return JSONResponse({
                    "success": True,
                    "message": f"Bridge {server_id} removed"
                })
            except Exception as e:
                logger.error(f"Error removing bridge: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Bridge health check
        @self.mcp.custom_route("/admin/api/mcp-bridge/health", methods=["GET"])
        async def bridge_health_check(request):
            """Get health status of all bridges"""
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                health = await self.bridge_manager.health_check()
                return JSONResponse(health)
            except Exception as e:
                logger.error(f"Error getting bridge health: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        # Bridge auto-register for known servers
        @self.mcp.custom_route("/admin/api/mcp-bridge/auto-register", methods=["POST"])
        async def auto_register_bridges(request):
            """Auto-register known MCP servers like vibe-kanban"""
            if not await self._check_admin_auth(request):
                return JSONResponse({"error": "Unauthorized"}, status_code=401)
            
            try:
                # Auto-register predefined bridges from configuration
                registered_bridges = []
                
                # Load bridge configuration
                try:
                    from pathlib import Path
                    bridge_config_path = Path(__file__).parent.parent / "config" / "bridge_config.json"
                    if bridge_config_path.exists():
                        with open(bridge_config_path, 'r') as f:
                            bridge_config = json.load(f)
                        
                        predefined_bridges = bridge_config.get("predefined_bridges", {})
                        
                        for bridge_name, bridge_config in predefined_bridges.items():
                            try:
                                server_id = await self.bridge_manager.register_bridge(bridge_config)
                                registered_bridges.append({
                                    "name": bridge_config.get("name", bridge_name),
                                    "server_id": server_id,
                                    "endpoint": f"/mcp-bridge/{server_id}"
                                })
                            except Exception as e:
                                logger.warning(f"Could not auto-register {bridge_name}: {e}")
                    else:
                        # Fallback to hardcoded vibe-kanban config if bridge_config.json not found
                        vibe_config = {
                            "name": "Vibe Kanban",
                            "type": "stdio",
                            "command": "npx",
                            "args": ["vibe-kanban", "--mcp"],
                            "working_directory": "/home/lj/haivemind-mcp-server",
                            "description": "Task and project management MCP server",
                            "tags": ["kanban", "project-management", "tasks"]
                        }
                        
                        try:
                            server_id = await self.bridge_manager.register_bridge(vibe_config)
                            registered_bridges.append({
                                "name": "Vibe Kanban",
                                "server_id": server_id,
                                "endpoint": f"/mcp-bridge/{server_id}"
                            })
                        except Exception as e:
                            logger.warning(f"Could not auto-register vibe-kanban: {e}")
                            
                except Exception as e:
                    logger.error(f"Error loading bridge configuration: {e}")
                    # Fall back to hardcoded config if needed
                
                return JSONResponse({
                    "registered": registered_bridges,
                    "total": len(registered_bridges),
                    "status": "completed"
                })
                
            except Exception as e:
                logger.error(f"Error in auto-registration: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)
        
        logger.info("ðŸŒ‰ MCP Bridge API endpoints registered")
    
    async def _check_admin_auth(self, request):
        """Check if request has valid admin authentication"""
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            return False
        
        token = auth_header.split(' ')[1]
        valid, payload = self.auth.validate_jwt_token(token)
        return valid and payload.get('role') == 'admin'
    
    async def _get_available_tools(self):
        """Get list of available MCP tools"""
        try:
            # Get tools from the MCP server
            if hasattr(self.mcp, '_tools'):
                return list(self.mcp._tools.keys())
            else:
                # Fallback to common hAIveMind tools
                return [
                    "store_memory", "retrieve_memory", "search_memories", 
                    "get_recent_memories", "get_memory_stats", "get_project_memories",
                    "register_agent", "get_agent_roster", "delegate_task",
                    "broadcast_discovery", "sync_infrastructure_config"
                ]
        except Exception as e:
            logger.warning(f"Could not get available tools: {e}")
            return []
    
    async def _save_config(self):
        """Save configuration to file"""
        try:
            import json
            config_path = Path(__file__).parent.parent / "config" / "config.json"
            config_path.parent.mkdir(exist_ok=True)
            
            with open(config_path, 'w') as f:
                json.dump(self.config, f, indent=2)
                
        except Exception as e:
            logger.error(f"Error saving configuration: {e}")
    
    def run(self):
        """Run the remote MCP server"""
        try:
            # Check SSL configuration
            ssl_config = self.config.get('security', {}).get('tls', {})
            ssl_enabled = ssl_config.get('enabled', False)
            
            # Determine protocol
            protocol = "https" if ssl_enabled else "http"
            
            logger.info("ðŸš€ Activating hAIveMind network portal - preparing for remote drone connections...")
            logger.info(f"ðŸ“¶ Hive broadcast channel: {protocol}://{self.host}:{self.port}/sse")
            logger.info(f"ðŸŒŠ Collective consciousness stream: {protocol}://{self.host}:{self.port}/mcp")
            logger.info(f"ðŸ©¸ Hive vitals monitor: {protocol}://{self.host}:{self.port}/health")
            
            if ssl_enabled:
                logger.info(f"ðŸ”’ SSL/TLS enabled - secure hive communications active")
                logger.info(f"ðŸ“œ Certificate: {ssl_config.get('cert_file')}")
            
            # Add health endpoint
            @self.mcp.custom_route("/health", methods=["GET"])
            async def health(request):
                from starlette.responses import JSONResponse
                return JSONResponse({
                    "status": "healthy",
                    "server": "remote-memory-mcp",
                    "version": "1.0.0",
                    "machine_id": self.storage.machine_id,
                    "endpoints": {
                        "sse": f"{protocol}://{self.host}:{self.port}/sse",
                        "streamable_http": f"{protocol}://{self.host}:{self.port}/mcp"
                    },
                    "ssl_enabled": ssl_enabled
                })
            
            # Suppress MCP framework warnings about early requests  
            logging.getLogger("root").setLevel(logging.ERROR)
            
            # Verify SSL files exist if SSL is enabled
            if ssl_enabled:
                cert_file = ssl_config.get('cert_file')
                key_file = ssl_config.get('key_file')
                
                from pathlib import Path
                if not Path(cert_file).exists():
                    raise FileNotFoundError(f"SSL certificate not found: {cert_file}")
                if not Path(key_file).exists():
                    raise FileNotFoundError(f"SSL private key not found: {key_file}")
                
                logger.info(f"ðŸ” Starting server with SSL - cert: {cert_file}, key: {key_file}")
            
            # Run the server (SSL must be configured during FastMCP initialization)
            self.mcp.run(transport="sse")
            
        except Exception as e:
            logger.error(f"ðŸ’¥ Network portal activation failed: {e} - remote hive access unavailable")
            raise
    
    def _get_vibe_kanban_tools(self):
        """Get Vibe Kanban tools for ticket operations"""
        # Import vibe kanban tools dynamically
        try:
            import importlib.util
            spec = importlib.util.find_spec("mcp__vibe_kanban__create_task")
            if spec:
                # Use the actual MCP tools
                class VibeKanbanWrapper:
                    def __init__(self):
                        pass
                    
                    async def create_task(self, project_id: str, title: str, description: str = ""):
                        # This would call the actual vibe kanban MCP tool
                        # For now, simulate the response
                        return {
                            'success': True,
                            'task': {
                                'id': str(uuid.uuid4()),
                                'title': title,
                                'description': description,
                                'status': 'todo',
                                'created_at': datetime.now().isoformat() + 'Z',
                                'updated_at': datetime.now().isoformat() + 'Z'
                            }
                        }
                    
                    async def get_task(self, project_id: str, task_id: str):
                        return {
                            'success': True,
                            'task': {
                                'id': task_id,
                                'title': 'Sample Task',
                                'description': 'Sample Description',
                                'status': 'todo',
                                'created_at': '2024-01-01T00:00:00Z',
                                'updated_at': '2024-01-01T00:00:00Z'
                            }
                        }
                    
                    async def update_task(self, project_id: str, task_id: str, **kwargs):
                        return {
                            'success': True,
                            'old_status': 'todo'
                        }
                    
                    async def list_tasks(self, project_id: str, status: str = None, limit: int = 50):
                        return {
                            'success': True,
                            'tasks': [],
                            'count': 0
                        }
                
                return VibeKanbanWrapper()
            else:
                # Fallback wrapper
                class FallbackWrapper:
                    async def create_task(self, **kwargs):
                        return {'success': False, 'error': 'Vibe Kanban not available'}
                    async def get_task(self, **kwargs):
                        return {'success': False, 'error': 'Vibe Kanban not available'}
                    async def update_task(self, **kwargs):
                        return {'success': False, 'error': 'Vibe Kanban not available'}
                    async def list_tasks(self, **kwargs):
                        return {'success': False, 'error': 'Vibe Kanban not available'}
                
                return FallbackWrapper()
        except Exception as e:
            logger.warning(f"Could not initialize Vibe Kanban wrapper: {e}")
            class ErrorWrapper:
                async def create_task(self, **kwargs):
                    return {'success': False, 'error': str(e)}
                async def get_task(self, **kwargs):
                    return {'success': False, 'error': str(e)}
                async def update_task(self, **kwargs):
                    return {'success': False, 'error': str(e)}
                async def list_tasks(self, **kwargs):
                    return {'success': False, 'error': str(e)}
            
            return ErrorWrapper()
    
    def _get_enhanced_ticket_system(self):
        """Lazy initialization of enhanced ticket system"""
        if self.enhanced_tickets is None:
            vibe_tools = self._get_vibe_kanban_tools()
            self.enhanced_tickets = EnhancedTicketSystem(vibe_tools, self.storage, self.config)
        return self.enhanced_tickets
    
    def _register_enhanced_ticket_tools(self):
        """Register enhanced ticket management MCP tools"""
        
        # # @self.mcp.tool()
        async def create_ticket(
            project_id: str,
            title: str,
            description: str = "",
            ticket_type: str = "task",
            priority: str = "medium",
            assignee: Optional[str] = None,
            labels: Optional[List[str]] = None,
            due_date: Optional[str] = None,
            time_estimate: Optional[int] = None,
            parent_ticket: Optional[str] = None,
            reporter: str = "system"
        ) -> str:
            """
            Create enhanced ticket with comprehensive metadata and hAIveMind integration
            
            Args:
                project_id: Project ID from Vibe Kanban
                title: Ticket title
                description: Detailed description
                ticket_type: Type (bug/feature/task/epic/story/incident/request)
                priority: Priority level (low/medium/high/critical/emergency)
                assignee: Assigned agent ID
                labels: List of labels for categorization
                due_date: Due date in ISO format
                time_estimate: Estimated hours
                parent_ticket: Parent ticket ID for hierarchies
                reporter: Reporter/creator ID
                
            Returns:
                Ticket creation status and details
            """
            try:
                ticket_system = self._get_enhanced_ticket_system()
                result = await ticket_system.create_ticket(
                    project_id=project_id,
                    title=title,
                    description=description,
                    ticket_type=ticket_type,
                    priority=priority,
                    assignee=assignee,
                    labels=labels or [],
                    due_date=due_date,
                    time_estimate=time_estimate,
                    parent_ticket=parent_ticket,
                    reporter=reporter
                )
                
                if result.get('success'):
                    # Add timestamp information to response
                    timestamp_system = self.enhanced_ticket_system.timestamp_system
                    timestamp_info = timestamp_system.format_message_timestamp()
                    
                    return f"""ðŸŽ« Ticket Created Successfully

Ticket #: {result.get('ticket_number')}
ID: {result.get('ticket_id')}
Title: {title}
Type: {ticket_type}
Priority: {priority}
Status: New

The ticket has been created in Vibe Kanban and indexed in hAIveMind memory for enhanced tracking and search.

{timestamp_info}"""
                else:
                    return f"âŒ Failed to create ticket: {result.get('error')}"
                    
            except Exception as e:
                return f"âŒ Error creating ticket: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_ticket(project_id: str, ticket_id: str) -> str:
            """
            Get comprehensive ticket details with hAIveMind context
            
            Args:
                project_id: Project ID from Vibe Kanban
                ticket_id: Ticket ID to retrieve
                
            Returns:
                Full ticket details with metadata and memory context
            """
            try:
                ticket_system = self._get_enhanced_ticket_system()
                result = await ticket_system.get_ticket(project_id, ticket_id)
                
                if result.get('success'):
                    ticket = result['ticket']
                    metadata = ticket['metadata']
                    timestamp_info = ticket.get('timestamp_info', {})
                    
                    # Format timestamp display
                    created_display = "Unknown"
                    updated_display = "Unknown"
                    age_display = "Unknown"
                    
                    if timestamp_info.get('created'):
                        created_display = f"{timestamp_info['created']['absolute']} ({timestamp_info['created']['relative']})"
                    if timestamp_info.get('last_updated'):
                        updated_display = f"{timestamp_info['last_updated']['absolute']} ({timestamp_info['last_updated']['relative']})"
                    if timestamp_info.get('age'):
                        age_display = timestamp_info['age']
                    
                    # Format due date display
                    due_display = metadata.get('due_date', 'Not set')
                    if timestamp_info.get('due_date'):
                        due_info = timestamp_info['due_date']
                        if due_info['is_overdue']:
                            due_display = f"âš ï¸ OVERDUE by {due_info['overdue_by']} (was due {due_info['absolute']})"
                        else:
                            due_display = f"{due_info['absolute']} ({due_info['time_remaining']} remaining)"
                    
                    # Add current timestamp to message
                    ticket_system = self._get_enhanced_ticket_system()
                    current_time = ticket_system.timestamp_system.format_message_timestamp()
                    
                    details = f"""ðŸŽ« Ticket #{ticket.get('ticket_number', 'Unknown')}

ðŸ“‹ Basic Info:
Title: {ticket['title']}
Status: {ticket['status']}
Type: {metadata.get('ticket_type', 'task')}
Priority: {metadata.get('priority', 'medium')}

ðŸ‘¤ Assignment:
Assignee: {metadata.get('assignee', 'Unassigned')}
Reporter: {metadata.get('reporter', 'Unknown')}

ðŸ“ Description:
{ticket['description']}

ðŸ·ï¸ Labels: {', '.join(metadata.get('labels', []))}
â° Due Date: {due_display}
â±ï¸ Time Estimate: {metadata.get('time_estimate', 'Not estimated')}h

ðŸ“… Timeline:
Created: {created_display}
Last Updated: {updated_display}
Age: {age_display}

ðŸ§  Memory Context: {len(ticket.get('memory_context', []))} related memories found

{current_time}"""
                    return details
                else:
                    return f"âŒ Failed to retrieve ticket: {result.get('error')}"
                    
            except Exception as e:
                return f"âŒ Error retrieving ticket: {str(e)}"
        
        # # @self.mcp.tool()
        async def list_tickets(
            project_id: str,
            status: Optional[str] = None,
            priority: Optional[str] = None,
            assignee: Optional[str] = None,
            ticket_type: Optional[str] = None,
            limit: int = 20
        ) -> str:
            """
            List tickets with enhanced filtering and status breakdown
            
            Args:
                project_id: Project ID from Vibe Kanban
                status: Filter by status (new/in_progress/review/done/etc)
                priority: Filter by priority (low/medium/high/critical/emergency)
                assignee: Filter by assignee
                ticket_type: Filter by type (bug/feature/task/etc)
                limit: Maximum tickets to return
                
            Returns:
                Formatted list of tickets with key details
            """
            try:
                ticket_system = self._get_enhanced_ticket_system()
                result = await ticket_system.list_tickets(
                    project_id=project_id,
                    status=status,
                    priority=priority,
                    assignee=assignee,
                    ticket_type=ticket_type,
                    limit=limit
                )
                
                if result.get('success'):
                    tickets = result['tickets']
                    filters = result.get('applied_filters', {})
                    
                    if not tickets:
                        return f"ðŸ“‹ No tickets found matching the criteria.\n\nApplied filters: {filters}"
                    
                    # Group by status for overview
                    by_status = {}
                    for ticket in tickets:
                        status = ticket['status']
                        if status not in by_status:
                            by_status[status] = []
                        by_status[status].append(ticket)
                    
                    output = [f"ðŸŽ« Found {len(tickets)} tickets"]
                    output.append(f"Filters: {filters}")
                    output.append("")
                    
                    # Show breakdown by status
                    output.append("ðŸ“Š Status Breakdown:")
                    for status, status_tickets in by_status.items():
                        output.append(f"  {status}: {len(status_tickets)}")
                    output.append("")
                    
                    # Show ticket list
                    output.append("ðŸ“‹ Ticket List:")
                    for ticket in tickets:
                        metadata = ticket['metadata']
                        priority_icon = {"low": "ðŸŸ¢", "medium": "ðŸŸ¡", "high": "ðŸŸ ", "critical": "ðŸ”´", "emergency": "ðŸš¨"}.get(metadata.get('priority', 'medium'), 'ðŸŸ¡')
                        
                        output.append(f"  #{ticket.get('ticket_number', '?')} {priority_icon} {ticket['title']}")
                        output.append(f"    Status: {ticket['status']} | Type: {metadata.get('ticket_type', 'task')} | Assignee: {metadata.get('assignee', 'Unassigned')}")
                    
                    # Add timestamp information
                    timestamp_system = self.enhanced_ticket_system.timestamp_system
                    timestamp_info = timestamp_system.format_message_timestamp()
                    output.append("")
                    output.append(timestamp_info)
                    
                    return '\n'.join(output)
                else:
                    return f"âŒ Failed to list tickets: {result.get('error')}"
                    
            except Exception as e:
                return f"âŒ Error listing tickets: {str(e)}"
        
        # # @self.mcp.tool()
        async def update_ticket_status(
            project_id: str,
            ticket_id: str,
            new_status: str,
            updated_by: str = "system",
            comment: str = ""
        ) -> str:
            """
            Update ticket status with workflow validation and memory tracking
            
            Args:
                project_id: Project ID from Vibe Kanban
                ticket_id: Ticket ID to update
                new_status: New status (new/in_progress/review/done/blocked/cancelled)
                updated_by: Agent making the change
                comment: Optional comment about the status change
                
            Returns:
                Status update confirmation
            """
            try:
                ticket_system = self._get_enhanced_ticket_system()
                result = await ticket_system.update_ticket_status(
                    project_id=project_id,
                    ticket_id=ticket_id,
                    new_status=new_status,
                    updated_by=updated_by,
                    comment=comment
                )
                
                if result.get('success'):
                    return f"""âœ… Ticket Status Updated

Ticket ID: {ticket_id}
New Status: {new_status}
Updated By: {updated_by}
Comment: {comment or 'No comment provided'}

Status change has been recorded in both Vibe Kanban and hAIveMind memory for full audit trail.

{self.enhanced_ticket_system.timestamp_system.format_message_timestamp()}"""
                else:
                    return f"âŒ Failed to update status: {result.get('error')}"
                    
            except Exception as e:
                return f"âŒ Error updating status: {str(e)}"
        
        # # @self.mcp.tool()
        async def search_tickets(
            project_id: str,
            query: str,
            limit: int = 10
        ) -> str:
            """
            Search tickets using hAIveMind semantic search and text matching
            
            Args:
                project_id: Project ID from Vibe Kanban
                query: Search query (supports keywords, IDs, etc)
                limit: Maximum results to return
                
            Returns:
                Relevant tickets with relevance scores
            """
            try:
                ticket_system = self._get_enhanced_ticket_system()
                result = await ticket_system.search_tickets(
                    project_id=project_id,
                    query=query,
                    limit=limit
                )
                
                if result.get('success'):
                    tickets = result['tickets']
                    
                    if not tickets:
                        return f"ðŸ” No tickets found matching '{query}'"
                    
                    output = [f"ðŸ” Found {len(tickets)} tickets matching '{query}'"]
                    output.append("")
                    
                    for ticket in tickets:
                        relevance = ticket.get('relevance_score', 0.0)
                        metadata = ticket['metadata']
                        
                        output.append(f"#{ticket.get('ticket_number', '?')} - {ticket['title']} (Relevance: {relevance:.2f})")
                        output.append(f"  Status: {ticket['status']} | Priority: {metadata.get('priority', 'medium')}")
                        output.append(f"  ID: {ticket['id']}")
                        output.append("")
                    
                    # Add timestamp information
                    timestamp_system = self.enhanced_ticket_system.timestamp_system
                    timestamp_info = timestamp_system.format_message_timestamp()
                    output.append(timestamp_info)
                    
                    return '\n'.join(output)
                else:
                    return f"âŒ Search failed: {result.get('error')}"
                    
            except Exception as e:
                return f"âŒ Search error: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_my_tickets(
            project_id: str,
            assignee: str,
            status: Optional[str] = None
        ) -> str:
            """
            Get tickets assigned to specific user with workload summary
            
            Args:
                project_id: Project ID from Vibe Kanban
                assignee: Agent ID to get tickets for
                status: Optional status filter
                
            Returns:
                Personal ticket dashboard
            """
            try:
                ticket_system = self._get_enhanced_ticket_system()
                result = await ticket_system.get_my_tickets(
                    project_id=project_id,
                    assignee=assignee,
                    status=status
                )
                
                if result.get('success'):
                    tickets = result['tickets']
                    
                    if not tickets:
                        return f"ðŸ‘¤ No tickets assigned to {assignee}"
                    
                    # Calculate workload metrics
                    total_estimated = sum(t['metadata'].get('time_estimate', 0) for t in tickets if t['metadata'].get('time_estimate'))
                    in_progress = [t for t in tickets if t['status'] == 'in_progress']
                    high_priority = [t for t in tickets if t['metadata'].get('priority') in ['high', 'critical', 'emergency']]
                    
                    output = [f"ðŸ‘¤ {assignee}'s Tickets ({len(tickets)} total)"]
                    output.append(f"â±ï¸ Total Estimated Work: {total_estimated}h")
                    output.append(f"ðŸƒ In Progress: {len(in_progress)}")
                    output.append(f"ðŸš¨ High Priority: {len(high_priority)}")
                    output.append("")
                    
                    # Group by status
                    by_status = {}
                    for ticket in tickets:
                        status = ticket['status']
                        if status not in by_status:
                            by_status[status] = []
                        by_status[status].append(ticket)
                    
                    for status, status_tickets in by_status.items():
                        output.append(f"ðŸ“‹ {status.upper()} ({len(status_tickets)})")
                        for ticket in status_tickets:
                            metadata = ticket['metadata']
                            priority_icon = {"low": "ðŸŸ¢", "medium": "ðŸŸ¡", "high": "ðŸŸ ", "critical": "ðŸ”´", "emergency": "ðŸš¨"}.get(metadata.get('priority', 'medium'), 'ðŸŸ¡')
                            estimate = f" ({metadata.get('time_estimate', 0)}h)" if metadata.get('time_estimate') else ""
                            
                            output.append(f"  #{ticket.get('ticket_number', '?')} {priority_icon} {ticket['title']}{estimate}")
                        output.append("")
                    
                    # Add timestamp information
                    timestamp_system = self.enhanced_ticket_system.timestamp_system
                    timestamp_info = timestamp_system.format_message_timestamp()
                    output.append(timestamp_info)
                    
                    return '\n'.join(output)
                else:
                    return f"âŒ Failed to get tickets: {result.get('error')}"
                    
            except Exception as e:
                return f"âŒ Error getting tickets: {str(e)}"
        
        # # @self.mcp.tool()
        async def add_ticket_comment(
            project_id: str,
            ticket_id: str,
            comment: str,
            author: str = "system"
        ) -> str:
            """
            Add comment to ticket with hAIveMind memory integration
            
            Args:
                project_id: Project ID from Vibe Kanban
                ticket_id: Ticket ID to comment on
                comment: Comment text
                author: Comment author
                
            Returns:
                Comment addition confirmation
            """
            try:
                ticket_system = self._get_enhanced_ticket_system()
                result = await ticket_system.add_ticket_comment(
                    project_id=project_id,
                    ticket_id=ticket_id,
                    comment=comment,
                    author=author
                )
                
                if result.get('success'):
                    memory_id = result.get('memory_id', 'N/A')
                    comment_id = result.get('comment_id', 'N/A')
                    return f"""âœ… Comment Added Successfully

ðŸŽ« Ticket ID: {ticket_id}
ðŸ’¬ Comment ID: {comment_id}
ðŸ§  Memory ID: {memory_id}
ðŸ‘¤ Author: {author}
ðŸ“ Comment: {comment}

ðŸ“Š Memory Context:
â€¢ Stored in hAIveMind collective memory
â€¢ Searchable via memory ID: {memory_id}
â€¢ Linked to ticket workflow: {ticket_id}
â€¢ Available to all network agents

{self.enhanced_ticket_system.timestamp_system.format_message_timestamp()}"""
                else:
                    return f"âŒ Failed to add comment: {result.get('error')}"
                    
            except Exception as e:
                return f"âŒ Error adding comment: {str(e)}"

        # # @self.mcp.tool()
        async def get_ticket_comments(
            ticket_id: str
        ) -> str:
            """
            Get all comments for a ticket with full memory context
            
            Args:
                ticket_id: Ticket ID to get comments for
                
            Returns:
                Complete comment history with memory IDs and full context
            """
            try:
                ticket_system = self._get_enhanced_ticket_system()
                result = await ticket_system.get_ticket_comments(ticket_id)
                
                if result.get('success'):
                    comments = result.get('comments', [])
                    total_comments = result.get('total_comments', 0)
                    
                    if not comments:
                        return f"ðŸ“­ No comments found for ticket {ticket_id}"
                    
                    output = [
                        f"ðŸ’¬ Ticket Comments: {ticket_id}",
                        f"ðŸ“Š Total Comments: {total_comments}",
                        "=" * 60,
                        ""
                    ]
                    
                    for i, comment in enumerate(comments, 1):
                        output.extend([
                            f"Comment #{i}:",
                            f"  ðŸ’¬ ID: {comment.get('comment_id', 'N/A')}",
                            f"  ðŸ§  Memory ID: {comment.get('memory_id', 'N/A')}",
                            f"  ðŸ‘¤ Author: {comment.get('author', 'Unknown')}",
                            f"  ðŸ“… Created: {comment.get('created_at', 'Unknown')}",
                            f"  ðŸ“ Content: {comment.get('comment', 'No content')}",
                            f"  ðŸ”— Memory Content: {comment.get('memory_content', 'N/A')}",
                            ""
                        ])
                    
                    output.append(f"ðŸ” Browser Integration: All comments accessible via memory IDs for enhanced context understanding")
                    
                    return "\n".join(output)
                else:
                    return f"âŒ Failed to get comments: {result.get('error')}"
                    
            except Exception as e:
                return f"âŒ Error getting comments: {str(e)}"
        
        # # @self.mcp.tool()
        async def get_ticket_metrics(
            project_id: str,
            days: int = 30
        ) -> str:
            """
            Get comprehensive ticket metrics and analytics
            
            Args:
                project_id: Project ID from Vibe Kanban
                days: Time period for metrics calculation
                
            Returns:
                Detailed metrics report
            """
            try:
                ticket_system = self._get_enhanced_ticket_system()
                result = await ticket_system.get_ticket_metrics(
                    project_id=project_id,
                    days=days
                )
                
                if result.get('success'):
                    metrics = result['metrics']
                    
                    output = [f"ðŸ“Š Ticket Metrics ({days} days)"]
                    output.append(f"Generated: {result.get('generated_at', 'Unknown')}")
                    output.append("")
                    
                    # Overall stats
                    output.append(f"ðŸ“ˆ Overview:")
                    output.append(f"  Total Tickets: {metrics['total_tickets']}")
                    output.append(f"  Created in Period: {metrics['created_in_period']}")
                    output.append(f"  Closed in Period: {metrics['closed_in_period']}")
                    output.append(f"  Average Resolution: {metrics['average_resolution_time']:.1f}h")
                    output.append("")
                    
                    # Status breakdown
                    output.append("ðŸ“Š By Status:")
                    for status, count in metrics['by_status'].items():
                        output.append(f"  {status}: {count}")
                    output.append("")
                    
                    # Priority breakdown
                    output.append("ðŸš¨ By Priority:")
                    for priority, count in metrics['by_priority'].items():
                        output.append(f"  {priority}: {count}")
                    output.append("")
                    
                    # Type breakdown
                    output.append("ðŸŽ¯ By Type:")
                    for ticket_type, count in metrics['by_type'].items():
                        output.append(f"  {ticket_type}: {count}")
                    output.append("")
                    
                    # Alerts
                    output.append("âš ï¸ Alerts:")
                    output.append(f"  Critical Tickets: {metrics['critical_tickets']}")
                    output.append(f"  Overdue Tickets: {metrics['overdue_tickets']}")
                    
                    # Add timestamp information
                    timestamp_system = self.enhanced_ticket_system.timestamp_system
                    timestamp_info = timestamp_system.format_message_timestamp()
                    output.append("")
                    output.append(timestamp_info)
                    
                    return '\n'.join(output)
                else:
                    return f"âŒ Failed to get metrics: {result.get('error')}"
                    
            except Exception as e:
                return f"âŒ Error getting metrics: {str(e)}"
        
        logger.info("ðŸŽ« Enhanced ticket management tools registered - comprehensive ticket system enabled")

def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Remote Memory MCP Server")
    parser.add_argument(
        "--config", 
        type=str, 
        default=None,
        help="Path to configuration file"
    )
    parser.add_argument(
        "--host",
        type=str,
        default=None,
        help="Host to bind to (overrides config)"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=None,
        help="Port to bind to (overrides config)"
    )
    
    args = parser.parse_args()
    
    # Print hAIveMind startup art
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                             â”‚
â”‚    â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â”‚
â”‚    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â”‚
â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â”‚
â”‚    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â”‚
â”‚    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â”‚
â”‚    â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â•  â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â• â”‚
â”‚                                                             â”‚
â”‚        ðŸŒðŸ¤– Remote MCP Server - Network Hub Starting ðŸ¤–ðŸŒ        â”‚
â”‚                                                             â”‚
â”‚             ðŸ”„ Initializing Remote Access Portal...             â”‚
â”‚                                                             â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    """)
    
    try:
        print("ðŸ§  Initializing hAIveMind remote memory server...")
        # Initialize server
        server = RemoteMemoryMCPServer(config_path=args.config)
        
        # Override host/port if provided
        if args.host:
            server.host = args.host
        if args.port:
            server.port = args.port
        
        # Run server
        server.run()
        
    except KeyboardInterrupt:
        logger.info("ðŸ›‘ Network portal deactivated by user - remote hive access terminated")
    except Exception as e:
        logger.error(f"ðŸ’¥ Critical portal malfunction: {e} - remote hive connection lost")
        sys.exit(1)

if __name__ == "__main__":
    main()
